{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# https://www.kaggle.com/demetrypascal/fork-of-2heads-looper-super-puper-plate/notebook\n",
    "\n",
    "kernel_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032594,
     "end_time": "2020-11-11T07:45:11.484416",
     "exception": false,
     "start_time": "2020-11-11T07:45:11.451822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032435,
     "end_time": "2020-11-11T07:45:11.549966",
     "exception": false,
     "start_time": "2020-11-11T07:45:11.517531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Letâ€™s load the packages and provide some constants for our script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T07:45:11.624887Z",
     "iopub.status.busy": "2020-11-11T07:45:11.623973Z",
     "iopub.status.idle": "2020-11-11T07:45:18.923587Z",
     "shell.execute_reply": "2020-11-11T07:45:18.92233Z"
    },
    "papermill": {
     "duration": 7.340999,
     "end_time": "2020-11-11T07:45:18.923723",
     "exception": false,
     "start_time": "2020-11-11T07:45:11.582724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import layers, regularizers, Sequential, Model, backend, callbacks, optimizers, metrics, losses\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2020-11-11T07:45:19.006735Z",
     "iopub.status.busy": "2020-11-11T07:45:19.004625Z",
     "iopub.status.idle": "2020-11-11T07:45:19.007534Z",
     "shell.execute_reply": "2020-11-11T07:45:19.008095Z"
    },
    "papermill": {
     "duration": 0.050102,
     "end_time": "2020-11-11T07:45:19.008235",
     "exception": false,
     "start_time": "2020-11-11T07:45:18.958133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = \"../input/lish-moa\" if kernel_mode else \"/workspace/Kaggle/MoA\"\n",
    "model_output_folder = \".\" if kernel_mode else f\"{PATH}/improving-mark-s-2-heads-model\"\n",
    "os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "# SEEDS = [23]\n",
    "SEEDS = [23, 228, 1488, 1998, 2208, 2077, 404]\n",
    "KFOLDS = 10\n",
    "\n",
    "label_smoothing_alpha = 0.0005\n",
    "\n",
    "P_MIN = label_smoothing_alpha\n",
    "P_MAX = 1 - P_MIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T07:45:19.086097Z",
     "iopub.status.busy": "2020-11-11T07:45:19.084976Z",
     "iopub.status.idle": "2020-11-11T07:45:25.522055Z",
     "shell.execute_reply": "2020-11-11T07:45:25.518386Z"
    },
    "papermill": {
     "duration": 6.480782,
     "end_time": "2020-11-11T07:45:25.522253",
     "exception": false,
     "start_time": "2020-11-11T07:45:19.041471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import train data, drop sig_id, cp_type\n",
    "train_features = pd.read_csv(f'{PATH}/train_features.csv')\n",
    "\n",
    "non_ctl_idx = train_features.loc[\n",
    "    train_features['cp_type'] != 'ctl_vehicle'].index.to_list()\n",
    "\n",
    "# Drop training data with ctl vehicle\n",
    "tr = train_features.iloc[non_ctl_idx, :].reset_index(drop=True)\n",
    "\n",
    "test_features = pd.read_csv(f'{PATH}/test_features.csv')\n",
    "te = test_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets_scored = pd.read_csv(f'{PATH}/train_targets_scored.csv')\n",
    "Y = train_targets_scored.drop('sig_id', axis=1)\n",
    "Y = Y.iloc[non_ctl_idx, :].copy().reset_index(drop=True).values\n",
    "\n",
    "train_targets_nonscored = pd.read_csv(f'{PATH}/train_targets_nonscored.csv')\n",
    "Y0 = train_targets_nonscored.drop('sig_id', axis=1)\n",
    "Y0 = Y0.iloc[non_ctl_idx, :].copy().reset_index(drop=True).values\n",
    "\n",
    "sub = pd.read_csv(f'{PATH}/sample_submission.csv')\n",
    "sub.iloc[:, 1:] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03583,
     "end_time": "2020-11-11T07:45:28.303012",
     "exception": false,
     "start_time": "2020-11-11T07:45:28.267182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Features from t.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036472,
     "end_time": "2020-11-11T07:45:28.375902",
     "exception": false,
     "start_time": "2020-11-11T07:45:28.33943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here I am getting most important predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T07:45:28.455017Z",
     "iopub.status.busy": "2020-11-11T07:45:28.454184Z",
     "iopub.status.idle": "2020-11-11T07:45:28.461483Z",
     "shell.execute_reply": "2020-11-11T07:45:28.460844Z"
    },
    "papermill": {
     "duration": 0.049565,
     "end_time": "2020-11-11T07:45:28.461601",
     "exception": false,
     "start_time": "2020-11-11T07:45:28.412036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import predictors from public kernel\n",
    "json_file_path = '../input/t-test-pca-rfe-logistic-regression/main_predictors.json' if kernel_mode \\\n",
    "    else \"/workspace/Kaggle/MoA/t-test-pca-rfe-logistic-regression/main_predictors.json\"\n",
    "\n",
    "with open(json_file_path, 'r') as j:\n",
    "    predictors = json.loads(j.read())\n",
    "    predictors = predictors['start_predictors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T07:45:28.573031Z",
     "iopub.status.busy": "2020-11-11T07:45:28.565161Z",
     "iopub.status.idle": "2020-11-11T07:45:28.576452Z",
     "shell.execute_reply": "2020-11-11T07:45:28.57703Z"
    },
    "papermill": {
     "duration": 0.0786,
     "end_time": "2020-11-11T07:45:28.577177",
     "exception": false,
     "start_time": "2020-11-11T07:45:28.498577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21948, 447)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_Xtrain = tr[predictors].copy().values\n",
    "\n",
    "second_Xtest = te[predictors].copy().values\n",
    "second_Xtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036297,
     "end_time": "2020-11-11T07:45:28.650577",
     "exception": false,
     "start_time": "2020-11-11T07:45:28.61428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036666,
     "end_time": "2020-11-11T07:45:28.724069",
     "exception": false,
     "start_time": "2020-11-11T07:45:28.687403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I got idea of **label smoothing** from this notebook: https://www.kaggle.com/kailex/moa-transfer-recipe-with-smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T07:45:28.804562Z",
     "iopub.status.busy": "2020-11-11T07:45:28.803696Z",
     "iopub.status.idle": "2020-11-11T07:45:28.8082Z",
     "shell.execute_reply": "2020-11-11T07:45:28.807569Z"
    },
    "papermill": {
     "duration": 0.047482,
     "end_time": "2020-11-11T07:45:28.808311",
     "exception": false,
     "start_time": "2020-11-11T07:45:28.760829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logloss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, P_MIN, P_MAX)\n",
    "    return -backend.mean(y_true * backend.log(y_pred) +\n",
    "                         (1 - y_true) * backend.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036797,
     "end_time": "2020-11-11T07:45:28.956193",
     "exception": false,
     "start_time": "2020-11-11T07:45:28.919396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(772, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features = [c for c in train_features.columns if c != \"sig_id\"]\n",
    "gene_experssion_features = [c for c in numeric_features if c.startswith(\"g-\")]\n",
    "cell_viability_features = [c for c in numeric_features if c.startswith(\"c-\")]\n",
    "len(gene_experssion_features), len(cell_viability_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.get_dummies(tr, columns=[\"cp_time\", \"cp_dose\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = tr.drop(['sig_id', 'cp_type'], axis=1)\n",
    "te = test_features.drop(['sig_id', 'cp_type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = pd.get_dummies(te, columns=[\"cp_time\", \"cp_dose\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>g-9</th>\n",
       "      <th>...</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "      <th>cp_time_24</th>\n",
       "      <th>cp_time_48</th>\n",
       "      <th>cp_time_72</th>\n",
       "      <th>cp_dose_D1</th>\n",
       "      <th>cp_dose_D2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.5458</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>-0.5135</td>\n",
       "      <td>0.4408</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>-0.1644</td>\n",
       "      <td>-0.2140</td>\n",
       "      <td>0.2221</td>\n",
       "      <td>-0.3260</td>\n",
       "      <td>1.9390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1193</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>-0.0502</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>-0.7750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.1829</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>1.2080</td>\n",
       "      <td>-0.4522</td>\n",
       "      <td>-0.3652</td>\n",
       "      <td>-0.3319</td>\n",
       "      <td>-1.8820</td>\n",
       "      <td>0.4022</td>\n",
       "      <td>-0.3528</td>\n",
       "      <td>0.1271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5382</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>-0.4764</td>\n",
       "      <td>-1.3810</td>\n",
       "      <td>-0.7300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1852</td>\n",
       "      <td>-0.1404</td>\n",
       "      <td>-0.3911</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>-1.4380</td>\n",
       "      <td>0.2455</td>\n",
       "      <td>-0.3390</td>\n",
       "      <td>-0.3206</td>\n",
       "      <td>0.6944</td>\n",
       "      <td>0.5837</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0140</td>\n",
       "      <td>0.8662</td>\n",
       "      <td>1.0160</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>-0.1942</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.1955</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.4244</td>\n",
       "      <td>-0.5855</td>\n",
       "      <td>-1.2020</td>\n",
       "      <td>0.5998</td>\n",
       "      <td>-0.1799</td>\n",
       "      <td>0.9365</td>\n",
       "      <td>0.2942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9005</td>\n",
       "      <td>0.8131</td>\n",
       "      <td>-0.1305</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>-0.5809</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.3979</td>\n",
       "      <td>-1.2680</td>\n",
       "      <td>1.9130</td>\n",
       "      <td>0.2057</td>\n",
       "      <td>-0.5864</td>\n",
       "      <td>-0.0166</td>\n",
       "      <td>0.5128</td>\n",
       "      <td>0.6365</td>\n",
       "      <td>0.2611</td>\n",
       "      <td>-1.1120</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0900</td>\n",
       "      <td>-0.2962</td>\n",
       "      <td>-0.5313</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>1.8380</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>0.4571</td>\n",
       "      <td>-0.5743</td>\n",
       "      <td>3.3930</td>\n",
       "      <td>-0.6202</td>\n",
       "      <td>0.8557</td>\n",
       "      <td>1.6240</td>\n",
       "      <td>0.0640</td>\n",
       "      <td>-0.6316</td>\n",
       "      <td>-1.1990</td>\n",
       "      <td>0.7312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4791</td>\n",
       "      <td>-1.2680</td>\n",
       "      <td>-1.1280</td>\n",
       "      <td>-0.4167</td>\n",
       "      <td>-0.6600</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>-0.5885</td>\n",
       "      <td>-0.2548</td>\n",
       "      <td>2.5850</td>\n",
       "      <td>0.3456</td>\n",
       "      <td>0.4401</td>\n",
       "      <td>0.3107</td>\n",
       "      <td>-0.7437</td>\n",
       "      <td>-0.0143</td>\n",
       "      <td>0.2615</td>\n",
       "      <td>1.9980</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2730</td>\n",
       "      <td>0.2614</td>\n",
       "      <td>-0.2790</td>\n",
       "      <td>-0.0131</td>\n",
       "      <td>-0.0934</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>-0.3985</td>\n",
       "      <td>-0.1554</td>\n",
       "      <td>0.2677</td>\n",
       "      <td>-0.6813</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.4791</td>\n",
       "      <td>-0.0166</td>\n",
       "      <td>0.7501</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>1.3910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4666</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>-0.4205</td>\n",
       "      <td>-0.1504</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>-1.0960</td>\n",
       "      <td>-1.7750</td>\n",
       "      <td>-0.3977</td>\n",
       "      <td>1.0160</td>\n",
       "      <td>-1.3350</td>\n",
       "      <td>-0.2207</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-1.3020</td>\n",
       "      <td>1.0150</td>\n",
       "      <td>0.6747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1286</td>\n",
       "      <td>-0.2618</td>\n",
       "      <td>0.5074</td>\n",
       "      <td>0.7430</td>\n",
       "      <td>-0.0484</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>-0.5174</td>\n",
       "      <td>0.2953</td>\n",
       "      <td>0.3286</td>\n",
       "      <td>-0.0428</td>\n",
       "      <td>-0.0800</td>\n",
       "      <td>0.8702</td>\n",
       "      <td>-0.8724</td>\n",
       "      <td>0.3883</td>\n",
       "      <td>-0.0491</td>\n",
       "      <td>0.3557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4041</td>\n",
       "      <td>-0.4948</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>-0.1356</td>\n",
       "      <td>0.5280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3982 rows Ã— 877 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         g-0     g-1     g-2     g-3     g-4     g-5     g-6     g-7     g-8  \\\n",
       "0    -0.5458  0.1306 -0.5135  0.4408  1.5500 -0.1644 -0.2140  0.2221 -0.3260   \n",
       "1    -0.1829  0.2320  1.2080 -0.4522 -0.3652 -0.3319 -1.8820  0.4022 -0.3528   \n",
       "2     0.1852 -0.1404 -0.3911  0.1310 -1.4380  0.2455 -0.3390 -0.3206  0.6944   \n",
       "3     0.4828  0.1955  0.3825  0.4244 -0.5855 -1.2020  0.5998 -0.1799  0.9365   \n",
       "4    -0.3979 -1.2680  1.9130  0.2057 -0.5864 -0.0166  0.5128  0.6365  0.2611   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "3977  0.4571 -0.5743  3.3930 -0.6202  0.8557  1.6240  0.0640 -0.6316 -1.1990   \n",
       "3978 -0.5885 -0.2548  2.5850  0.3456  0.4401  0.3107 -0.7437 -0.0143  0.2615   \n",
       "3979 -0.3985 -0.1554  0.2677 -0.6813  0.0152  0.4791 -0.0166  0.7501  0.0346   \n",
       "3980 -1.0960 -1.7750 -0.3977  1.0160 -1.3350 -0.2207 -0.3611 -1.3020  1.0150   \n",
       "3981 -0.5174  0.2953  0.3286 -0.0428 -0.0800  0.8702 -0.8724  0.3883 -0.0491   \n",
       "\n",
       "         g-9  ...    c-95    c-96    c-97    c-98    c-99  cp_time_24  \\\n",
       "0     1.9390  ... -0.1193  0.0210 -0.0502  0.1510 -0.7750           1   \n",
       "1     0.1271  ... -0.5382  0.0359 -0.4764 -1.3810 -0.7300           0   \n",
       "2     0.5837  ... -1.0140  0.8662  1.0160  0.4924 -0.1942           1   \n",
       "3     0.2942  ... -0.9005  0.8131 -0.1305  0.5645 -0.5809           1   \n",
       "4    -1.1120  ...  1.0900 -0.2962 -0.5313  0.9931  1.8380           0   \n",
       "...      ...  ...     ...     ...     ...     ...     ...         ...   \n",
       "3977  0.7312  ... -0.4791 -1.2680 -1.1280 -0.4167 -0.6600           1   \n",
       "3978  1.9980  ...  1.2730  0.2614 -0.2790 -0.0131 -0.0934           1   \n",
       "3979  1.3910  ...  0.4666  0.0461  0.5888 -0.4205 -0.1504           0   \n",
       "3980  0.6747  ...  0.1286 -0.2618  0.5074  0.7430 -0.0484           0   \n",
       "3981  0.3557  ... -0.4041 -0.4948  0.0757 -0.1356  0.5280           0   \n",
       "\n",
       "      cp_time_48  cp_time_72  cp_dose_D1  cp_dose_D2  \n",
       "0              0           0           1           0  \n",
       "1              0           1           1           0  \n",
       "2              0           0           1           0  \n",
       "3              0           0           0           1  \n",
       "4              1           0           1           0  \n",
       "...          ...         ...         ...         ...  \n",
       "3977           0           0           1           0  \n",
       "3978           0           0           1           0  \n",
       "3979           0           1           1           0  \n",
       "3980           1           0           0           1  \n",
       "3981           0           1           1           0  \n",
       "\n",
       "[3982 rows x 877 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T07:45:29.048267Z",
     "iopub.status.busy": "2020-11-11T07:45:29.046389Z",
     "iopub.status.idle": "2020-11-11T07:45:29.049041Z",
     "shell.execute_reply": "2020-11-11T07:45:29.049614Z"
    },
    "papermill": {
     "duration": 0.056399,
     "end_time": "2020-11-11T07:45:29.049751",
     "exception": false,
     "start_time": "2020-11-11T07:45:28.993352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessor_1(train, valid, test, seed):\n",
    "    n_gs = 50\n",
    "    n_cs = 50\n",
    "\n",
    "    # g-mean, c-mean\n",
    "    train_g_mean = train[gene_experssion_features].mean(axis=1)\n",
    "    valid_g_mean = valid[gene_experssion_features].mean(axis=1)\n",
    "    test_g_mean = test[gene_experssion_features].mean(axis=1)\n",
    "\n",
    "    train_c_mean = train[cell_viability_features].mean(axis=1)\n",
    "    valid_c_mean = valid[cell_viability_features].mean(axis=1)\n",
    "    test_c_mean = test[cell_viability_features].mean(axis=1)\n",
    "\n",
    "    train_columns = train.columns.tolist()\n",
    "    test_columns = test.columns.tolist()\n",
    "\n",
    "    train = np.concatenate(\n",
    "        (train, train_g_mean[:, np.newaxis], train_c_mean[:, np.newaxis]),\n",
    "        axis=1)\n",
    "    valid = np.concatenate(\n",
    "        (valid, valid_g_mean[:, np.newaxis], valid_c_mean[:, np.newaxis]),\n",
    "        axis=1)\n",
    "    test = np.concatenate(\n",
    "        (test, test_g_mean[:, np.newaxis], test_c_mean[:, np.newaxis]), axis=1)\n",
    "\n",
    "    # Standard Scaler for Numerical Values\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    train = pd.DataFrame(data=scaler.fit_transform(train),\n",
    "                         columns=train_columns + [\"g_mean\", \"c_mean\"])\n",
    "    valid = pd.DataFrame(data=scaler.transform(valid),\n",
    "                         columns=train_columns + [\"g_mean\", \"c_mean\"])\n",
    "    test = pd.DataFrame(data=scaler.transform(test),\n",
    "                        columns=test_columns + [\"g_mean\", \"c_mean\"])\n",
    "\n",
    "    pca_gs = PCA(n_components=n_gs, random_state=seed)\n",
    "    train_pca_gs = pca_gs.fit_transform(train[gene_experssion_features].values)\n",
    "    valid_pca_gs = pca_gs.transform(valid[gene_experssion_features].values)\n",
    "    test_pca_gs = pca_gs.transform(test[gene_experssion_features].values)\n",
    "\n",
    "    pca_cs = PCA(n_components=n_cs, random_state=seed)\n",
    "    train_pca_cs = pca_cs.fit_transform(train[cell_viability_features].values)\n",
    "    valid_pca_cs = pca_cs.transform(valid[cell_viability_features].values)\n",
    "    test_pca_cs = pca_cs.transform(test[cell_viability_features].values)\n",
    "\n",
    "    # Append Features\n",
    "    train = np.concatenate((train, train_pca_gs, train_pca_cs), axis=1)\n",
    "    valid = np.concatenate((valid, valid_pca_gs, valid_pca_cs), axis=1)\n",
    "    test = np.concatenate((test, test_pca_gs, test_pca_cs), axis=1)\n",
    "\n",
    "    return train, valid, test, scaler, pca_gs, pca_cs\n",
    "\n",
    "\n",
    "def preprocessor_2(train, valid, test):\n",
    "    # Standard Scaler for Numerical Values\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    train = scaler.fit_transform(train)\n",
    "    valid = scaler.transform(valid)\n",
    "    test = scaler.transform(test)\n",
    "\n",
    "    return train, valid, test, scaler\n",
    "\n",
    "\n",
    "def save_pickle(obj, model_output_folder, name):\n",
    "    dump(obj, open(f\"{model_output_folder}/{name}.pkl\", 'wb'),\n",
    "         pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_pickle(model_output_folder, name):\n",
    "    return load(open(f\"{model_output_folder}/{name}.pkl\", 'rb'))\n",
    "\n",
    "\n",
    "def mean_logloss(y_pred, y_true):\n",
    "    logloss = (1 - y_true) * np.log(1 - y_pred +\n",
    "                                    1e-15) + y_true * np.log(y_pred + 1e-15)\n",
    "    return np.mean(-logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21948, 877), (3982, 877))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape, te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T07:45:30.295979Z",
     "iopub.status.busy": "2020-11-11T07:45:30.27493Z",
     "iopub.status.idle": "2020-11-11T08:45:30.221121Z",
     "shell.execute_reply": "2020-11-11T08:45:30.220025Z"
    },
    "papermill": {
     "duration": 3600.006513,
     "end_time": "2020-11-11T08:45:30.221259",
     "exception": false,
     "start_time": "2020-11-11T07:45:30.214746",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After non-scored training: validation_loss = 0.006313337944447994\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "After scored training: validation_loss = 0.017869964241981506\n",
      "Before loop: validation_loss = 0.017869964241981506\n",
      "No Improvement, stopped\n",
      "1 loop ---> After Frozen-step best valid = 0.017869964241981506 after 3 epochs \n",
      "\n",
      "Improved: 0.017847945913672447 from 0.017869964241981506\n",
      "Improved: 0.017834585160017014 from 0.017847945913672447\n",
      "Improved: 0.017831293866038322 from 0.017834585160017014\n",
      "No Improvement, stopped\n",
      "1 loop ---> After Non-frozen-step best valid = 0.017831293866038322 after 3 epochs \n",
      "\n",
      "Total Non-frozen-step improved: 3\n",
      "No Improvement, stopped\n",
      "2 loop ---> After Frozen-step best valid = 0.017831293866038322 after 3 epochs \n",
      "\n",
      "No Improvement, stopped\n",
      "2 loop ---> After Non-frozen-step best valid = 0.017831293866038322 after 3 epochs \n",
      "\n",
      "Total Non-frozen-step improved: 0\n",
      "\n",
      "Seed: 23 Fold: 0 score: 0.016001806361360837\n",
      "[[4.5883721e-03 1.3314264e-03 1.1061698e-03 ... 6.3399756e-03\n",
      "  5.3564168e-04 1.7224540e-03]\n",
      " [3.7353305e-04 3.6415621e-03 1.9101978e-04 ... 1.3001213e-03\n",
      "  8.7594334e-03 1.7212309e-02]\n",
      " [1.8056507e-04 1.1970058e-04 2.0283621e-03 ... 3.5543072e-03\n",
      "  1.8054353e-04 3.2924335e-03]\n",
      " [3.7527722e-04 4.8549910e-04 2.3946988e-03 ... 8.1798388e-04\n",
      "  4.1353624e-04 1.8117927e-03]\n",
      " [4.0923762e-03 1.7249689e-03 3.7018731e-04 ... 1.1705356e-03\n",
      "  7.9950805e-05 5.2157068e-04]]\n",
      "\n",
      "\n",
      "After non-scored training: validation_loss = 0.006215711589902639\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "After scored training: validation_loss = 0.01846202090382576\n",
      "Before loop: validation_loss = 0.01846202090382576\n",
      "No Improvement, stopped\n",
      "1 loop ---> After Frozen-step best valid = 0.01846202090382576 after 3 epochs \n",
      "\n",
      "Improved: 0.018416542559862137 from 0.01846202090382576\n",
      "Improved: 0.01839749701321125 from 0.018416542559862137\n",
      "Improved: 0.01838759519159794 from 0.01839749701321125\n",
      "No Improvement, stopped\n",
      "1 loop ---> After Non-frozen-step best valid = 0.01838759519159794 after 3 epochs \n",
      "\n",
      "Total Non-frozen-step improved: 3\n",
      "No Improvement, stopped\n",
      "2 loop ---> After Frozen-step best valid = 0.01838759519159794 after 3 epochs \n",
      "\n",
      "Improved: 0.01838412880897522 from 0.01838759519159794\n",
      "Improved: 0.018382931128144264 from 0.01838412880897522\n",
      "No Improvement, stopped\n",
      "2 loop ---> After Non-frozen-step best valid = 0.018382931128144264 after 3 epochs \n",
      "\n",
      "Total Non-frozen-step improved: 2\n",
      "No Improvement, stopped\n",
      "3 loop ---> After Frozen-step best valid = 0.018382931128144264 after 3 epochs \n",
      "\n",
      "No Improvement, stopped\n",
      "3 loop ---> After Non-frozen-step best valid = 0.018382931128144264 after 3 epochs \n",
      "\n",
      "Total Non-frozen-step improved: 0\n",
      "\n",
      "Seed: 23 Fold: 1 score: 0.016604786728897154\n",
      "[[9.2098507e-04 1.2421452e-03 2.0154044e-03 ... 3.8339151e-03\n",
      "  6.5343347e-03 1.4083593e-03]\n",
      " [5.9024649e-05 1.2696219e-03 2.0346539e-04 ... 1.2936720e-04\n",
      "  4.3208790e-03 7.9425267e-04]\n",
      " [4.3342402e-04 4.0080122e-05 4.5065745e-03 ... 2.6601900e-03\n",
      "  1.5936649e-03 6.7622331e-03]\n",
      " [2.5037942e-03 3.1341467e-04 6.5668835e-03 ... 9.7421458e-04\n",
      "  6.9633918e-04 2.9842672e-03]\n",
      " [6.5295899e-04 3.6221795e-04 2.7858326e-03 ... 1.8581403e-03\n",
      "  3.1294383e-04 1.0978647e-03]]\n",
      "\n",
      "\n",
      "After non-scored training: validation_loss = 0.005887834820896387\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "After scored training: validation_loss = 0.018097946420311928\n",
      "Before loop: validation_loss = 0.018097946420311928\n",
      "No Improvement, stopped\n",
      "1 loop ---> After Frozen-step best valid = 0.018097946420311928 after 3 epochs \n",
      "\n",
      "Improved: 0.018067223951220512 from 0.018097946420311928\n",
      "Improved: 0.018052315339446068 from 0.018067223951220512\n",
      "Improved: 0.018045680597424507 from 0.018052315339446068\n",
      "Improved: 0.018042681738734245 from 0.018045680597424507\n",
      "Improved: 0.018040701746940613 from 0.018042681738734245\n",
      "Improved: 0.018038935959339142 from 0.018040701746940613\n",
      "Improved: 0.018037334084510803 from 0.018038935959339142\n",
      "No Improvement, stopped\n",
      "1 loop ---> After Non-frozen-step best valid = 0.018037334084510803 after 7 epochs \n",
      "\n",
      "Total Non-frozen-step improved: 7\n",
      "No Improvement, stopped\n",
      "2 loop ---> After Frozen-step best valid = 0.018037334084510803 after 3 epochs \n",
      "\n",
      "No Improvement, stopped\n",
      "2 loop ---> After Non-frozen-step best valid = 0.018037334084510803 after 3 epochs \n",
      "\n",
      "Total Non-frozen-step improved: 0\n",
      "\n",
      "Seed: 23 Fold: 2 score: 0.016289219290121768\n",
      "[[6.1298342e-05 3.2791708e-04 1.1236169e-03 ... 4.3898206e-03\n",
      "  2.9831012e-03 1.0600307e-03]\n",
      " [4.6486396e-04 2.0949435e-03 4.4554891e-03 ... 1.4126847e-03\n",
      "  5.9396108e-03 3.0632358e-02]\n",
      " [5.1377597e-04 1.0597213e-04 1.5591565e-03 ... 1.6998637e-03\n",
      "  3.6567316e-04 1.9341819e-03]\n",
      " [1.9783992e-03 5.1200722e-04 4.4274721e-03 ... 7.4213010e-04\n",
      "  8.7316666e-04 6.3560796e-03]\n",
      " [1.7384844e-03 7.7759934e-04 5.8152992e-04 ... 1.7522648e-03\n",
      "  6.8117792e-05 2.3392831e-04]]\n",
      "\n",
      "\n",
      "After non-scored training: validation_loss = 0.006430142559111118\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "After scored training: validation_loss = 0.01837753690779209\n",
      "Before loop: validation_loss = 0.01837753690779209\n",
      "No Improvement, stopped\n",
      "1 loop ---> After Frozen-step best valid = 0.01837753690779209 after 3 epochs \n",
      "\n",
      "Improved: 0.018347758799791336 from 0.01837753690779209\n",
      "Improved: 0.01833970472216606 from 0.018347758799791336\n",
      "No Improvement, stopped\n",
      "1 loop ---> After Non-frozen-step best valid = 0.01833970472216606 after 3 epochs \n",
      "\n",
      "Total Non-frozen-step improved: 2\n",
      "No Improvement, stopped\n",
      "2 loop ---> After Frozen-step best valid = 0.01833970472216606 after 3 epochs \n",
      "\n",
      "No Improvement, stopped\n",
      "2 loop ---> After Non-frozen-step best valid = 0.01833970472216606 after 3 epochs \n",
      "\n",
      "Total Non-frozen-step improved: 0\n",
      "\n",
      "Seed: 23 Fold: 3 score: 0.016564825356055305\n",
      "[[1.5011369e-04 8.9761474e-05 1.0060483e-03 ... 5.0898932e-04\n",
      "  7.8802975e-04 2.1138750e-03]\n",
      " [3.7971002e-04 1.0536861e-03 1.2891846e-03 ... 6.5217284e-04\n",
      "  2.6173224e-03 2.3541814e-03]\n",
      " [5.2144891e-04 9.7646203e-05 2.1546183e-03 ... 2.2457691e-03\n",
      "  2.9331341e-03 4.2268448e-03]\n",
      " [4.8771114e-04 2.2896643e-04 4.2975019e-03 ... 1.7361622e-03\n",
      "  4.7829631e-03 7.7758706e-03]\n",
      " [7.0792763e-04 3.2072759e-04 1.4492659e-03 ... 9.6513826e-04\n",
      "  4.5232143e-04 1.7028011e-03]]\n",
      "\n",
      "\n",
      "After non-scored training: validation_loss = 0.005927253048866987\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2f4ae70296b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    161\u001b[0m                            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                            verbose=0)\n\u001b[0m\u001b[1;32m    164\u001b[0m         m_nn = tf.keras.models.load_model(\n\u001b[1;32m    165\u001b[0m             \u001b[0;34mf\"{model_output_folder}/{file_name}.h5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof_predictions = np.zeros((tr.shape[0], Y.shape[1]))\n",
    "\n",
    "y_pred = np.zeros((te.shape[0], 206))\n",
    "for s in SEEDS:\n",
    "\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    tf.random.set_seed(s)\n",
    "\n",
    "    k = 0\n",
    "    kf = KFold(n_splits=KFOLDS, shuffle=True, random_state=s)\n",
    "    for train_index, valid_index in kf.split(tr):\n",
    "        file_name = f\"seed{s}_fold{k}\"\n",
    "\n",
    "        X_train_1, X_valid_1, X_test_1, scaler_1, pca_gs, pca_cs = preprocessor_1(\n",
    "            tr.iloc[train_index, :], tr.iloc[valid_index, :], te, s)\n",
    "        save_pickle(scaler_1, model_output_folder, f\"{file_name}_scaler_1\")\n",
    "        save_pickle(pca_gs, model_output_folder, f\"{file_name}_pca_gs\")\n",
    "        save_pickle(pca_cs, model_output_folder, f\"{file_name}_pca_cs\")\n",
    "\n",
    "        X_train_2, X_valid_2, X_test_2, scaler_2 = preprocessor_2(\n",
    "            second_Xtrain[train_index, :], second_Xtrain[valid_index, :],\n",
    "            second_Xtest)\n",
    "        save_pickle(scaler_2, model_output_folder, f\"{file_name}_scaler_2\")\n",
    "\n",
    "        y_train_1, y_valid_1 = Y[train_index, :], Y[valid_index, :]\n",
    "        y_train_2, y_valid_2 = Y0[train_index, :], Y0[valid_index, :]\n",
    "\n",
    "        n_features = X_train_1.shape[1]\n",
    "        n_features_2 = X_train_2.shape[1]\n",
    "\n",
    "        early_stopping = callbacks.EarlyStopping(min_delta=1e-5,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 patience=10,\n",
    "                                                 verbose=0,\n",
    "                                                 mode='min',\n",
    "                                                 restore_best_weights=True)\n",
    "        check_point = callbacks.ModelCheckpoint(\n",
    "            f\"{model_output_folder}/{file_name}_nonscore.h5\",\n",
    "            save_best_only=True,\n",
    "            verbose=0,\n",
    "            mode=\"min\")\n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(factor=0.5,\n",
    "                                                patience=4,\n",
    "                                                verbose=0,\n",
    "                                                mode=\"auto\")\n",
    "\n",
    "        # Model Definition #\n",
    "\n",
    "        input1_ = layers.Input(shape=(n_features, ))\n",
    "        input2_ = layers.Input(shape=(n_features_2, ))\n",
    "\n",
    "        output1 = Sequential([\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(512, activation=\"elu\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(256, activation=\"elu\")\n",
    "        ])(input1_)\n",
    "\n",
    "        answer1 = Sequential([\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(512, \"relu\")\n",
    "        ])(layers.Concatenate()([output1, input2_]))\n",
    "\n",
    "        answer2 = Sequential([\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(512, \"elu\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(256, \"relu\")\n",
    "        ])(layers.Concatenate()([output1, input2_, answer1]))\n",
    "\n",
    "        answer3 = Sequential(\n",
    "            [layers.BatchNormalization(),\n",
    "             layers.Dense(256,\n",
    "                          \"elu\")])(layers.Concatenate()([answer1, answer2]))\n",
    "\n",
    "        answer3_ = Sequential([\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(256, \"relu\")\n",
    "        ])(layers.Concatenate()([answer1, answer2, answer3]))\n",
    "\n",
    "        answer4 = Sequential([\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(\n",
    "                256,\n",
    "                kernel_initializer=tf.keras.initializers.lecun_normal(seed=s),\n",
    "                activation='selu',\n",
    "                name='last_frozen'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(\n",
    "                206,\n",
    "                kernel_initializer=tf.keras.initializers.lecun_normal(seed=s),\n",
    "                activation='selu')\n",
    "        ])(layers.Concatenate()([output1, answer2, answer3, answer3_]))\n",
    "\n",
    "        # Non-scored Training #\n",
    "\n",
    "        answer5 = Sequential([\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(Y0.shape[1], \"sigmoid\")\n",
    "        ])(answer4)\n",
    "\n",
    "        m_nn = tf.keras.Model([input1_, input2_], answer5)\n",
    "\n",
    "        m_nn.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                     loss=losses.BinaryCrossentropy(\n",
    "                         label_smoothing=label_smoothing_alpha),\n",
    "                     metrics=logloss)\n",
    "\n",
    "        history = m_nn.fit([X_train_1, X_train_2],\n",
    "                           y_train_2,\n",
    "                           epochs=50,\n",
    "                           batch_size=64,\n",
    "                           validation_data=([X_valid_1, X_valid_2], y_valid_2),\n",
    "                           callbacks=[check_point, early_stopping, reduce_lr],\n",
    "                           verbose=0)\n",
    "        m_nn = tf.keras.models.load_model(\n",
    "            f\"{model_output_folder}/{file_name}_nonscore.h5\",\n",
    "            custom_objects={'logloss': logloss})\n",
    "\n",
    "        valid_metric_old = m_nn.evaluate([X_valid_1, X_valid_2],\n",
    "                                         y_valid_2,\n",
    "                                         verbose=0)[0]  # loss\n",
    "        print('After non-scored training: validation_loss =', valid_metric_old)\n",
    "\n",
    "        # Scored Training #\n",
    "\n",
    "        answer5 = Sequential(\n",
    "            [layers.BatchNormalization(),\n",
    "             layers.Dense(Y.shape[1], \"sigmoid\")])(answer4)\n",
    "\n",
    "        m_nn = tf.keras.Model([input1_, input2_], answer5)\n",
    "\n",
    "        m_nn.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                     loss=losses.BinaryCrossentropy(\n",
    "                         label_smoothing=label_smoothing_alpha),\n",
    "                     metrics=logloss)\n",
    "\n",
    "        early_stopping = callbacks.EarlyStopping(min_delta=1e-5,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 patience=10,\n",
    "                                                 verbose=1,\n",
    "                                                 mode='min',\n",
    "                                                 restore_best_weights=True)\n",
    "        check_point = callbacks.ModelCheckpoint(\n",
    "            f\"{model_output_folder}/{file_name}.h5\",\n",
    "            save_best_only=True,\n",
    "            verbose=0,\n",
    "            mode=\"min\")\n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(factor=0.5,\n",
    "                                                patience=4,\n",
    "                                                verbose=0,\n",
    "                                                mode=\"auto\")\n",
    "\n",
    "        history = m_nn.fit([X_train_1, X_train_2],\n",
    "                           y_train_1,\n",
    "                           epochs=50,\n",
    "                           batch_size=64,\n",
    "                           validation_data=([X_valid_1, X_valid_2], y_valid_1),\n",
    "                           callbacks=[check_point, early_stopping, reduce_lr],\n",
    "                           verbose=0)\n",
    "        m_nn = tf.keras.models.load_model(\n",
    "            f\"{model_output_folder}/{file_name}.h5\",\n",
    "            custom_objects={'logloss': logloss})\n",
    "\n",
    "        # val_old = m_nn.predict([X_valid_1, X_valid_2])\n",
    "        # valid_metric_old = mean_logloss(val_old, y_valid_1)\n",
    "        valid_metric_old = m_nn.evaluate([X_valid_1, X_valid_2],\n",
    "                                         y_valid_1,\n",
    "                                         verbose=0)[0]  # loss\n",
    "        print('After scored training: validation_loss =', valid_metric_old)\n",
    "\n",
    "        m_nn.save(f'{model_output_folder}/tmp.h5')\n",
    "\n",
    "        print('Before loop: validation_loss =', valid_metric_old)\n",
    "\n",
    "        # big loop\n",
    "        loop = 1\n",
    "        while True:\n",
    "\n",
    "            # Freeze_weights(m_nn, to = 'last_frozen')\n",
    "            for i, layer in enumerate(m_nn.layers):\n",
    "                if layer.name == \"last_frozen\":\n",
    "                    layer.trainable = True\n",
    "                    break\n",
    "                else:\n",
    "                    layer.trainable = False\n",
    "\n",
    "            m_nn.compile(optimizer=tf.keras.optimizers.Adadelta(lr=0.001 / 3),\n",
    "                         loss=tf.losses.BinaryCrossentropy(\n",
    "                             label_smoothing=label_smoothing_alpha),\n",
    "                         metrics=logloss)\n",
    "\n",
    "            # Frozen Mode #\n",
    "\n",
    "            reps = 0\n",
    "            improved = 0\n",
    "            patience = 3\n",
    "            while True:\n",
    "                history = m_nn.fit([X_valid_1, X_valid_2],\n",
    "                                   y_valid_1,\n",
    "                                   epochs=1,\n",
    "                                   batch_size=64,\n",
    "                                   verbose=0)\n",
    "\n",
    "                # val_preds = m_nn.predict([X_valid_1, X_valid_2])\n",
    "                # valid_metric = mean_logloss(val_preds, y_valid_1)\n",
    "                valid_metric = m_nn.evaluate([X_valid_1, X_valid_2],\n",
    "                                             y_valid_1,\n",
    "                                             verbose=0)[0]\n",
    "\n",
    "                if valid_metric_old - valid_metric >= 1e-6:\n",
    "                    print('Improved:', valid_metric, 'from', valid_metric_old)\n",
    "                    reps += 1\n",
    "                    improved += 1\n",
    "                    valid_metric_old = valid_metric\n",
    "                    m_nn.save(f'{model_output_folder}/tmp.h5')\n",
    "                elif reps < patience:\n",
    "                    reps += 1\n",
    "                    pass\n",
    "                else:\n",
    "                    print('No Improvement, stopped')\n",
    "                    m_nn = tf.keras.models.load_model(\n",
    "                        f'{model_output_folder}/tmp.h5',\n",
    "                        custom_objects={'logloss': logloss})\n",
    "                    print(loop, 'loop ---> After Frozen-step best valid =',\n",
    "                          valid_metric_old, 'after', reps, 'epochs \\n')\n",
    "\n",
    "                    break\n",
    "\n",
    "            # Should continue training instead?\n",
    "            # if (improved == 0):  # no progress? STOP!\n",
    "            #     break\n",
    "\n",
    "            # Unfrozen Mode #\n",
    "\n",
    "            # Unfreeze all layers\n",
    "            for i, layer in enumerate(m_nn.layers):\n",
    "                layer.trainable = True\n",
    "\n",
    "            m_nn.compile(optimizer=tf.keras.optimizers.Adadelta(lr=0.001 / 5),\n",
    "                         loss=tf.losses.BinaryCrossentropy(\n",
    "                             label_smoothing=label_smoothing_alpha),\n",
    "                         metrics=logloss)\n",
    "\n",
    "            reps = 0\n",
    "            improved = 0\n",
    "            patience = 3\n",
    "            while True:\n",
    "                history = m_nn.fit([X_valid_1, X_valid_2],\n",
    "                                   y_valid_1,\n",
    "                                   epochs=1,\n",
    "                                   batch_size=64,\n",
    "                                   verbose=0)\n",
    "\n",
    "                # val_preds = m_nn.predict([X_valid_1, X_valid_2])\n",
    "                # valid_metric = mean_logloss(val_preds, y_valid_1)\n",
    "                valid_metric = m_nn.evaluate([X_valid_1, X_valid_2],\n",
    "                                             y_valid_1,\n",
    "                                             verbose=0)[0]\n",
    "\n",
    "                if valid_metric_old - valid_metric >= 1e-6:\n",
    "                    print('Improved:', valid_metric, 'from', valid_metric_old)\n",
    "                    reps += 1\n",
    "                    improved += 1\n",
    "                    valid_metric_old = valid_metric\n",
    "                    m_nn.save(f'{model_output_folder}/tmp.h5')\n",
    "                elif reps < patience:\n",
    "                    reps += 1\n",
    "                    pass\n",
    "                else:\n",
    "                    print('No Improvement, stopped')\n",
    "                    m_nn = tf.keras.models.load_model(\n",
    "                        f'{model_output_folder}/tmp.h5',\n",
    "                        custom_objects={'logloss': logloss})\n",
    "                    print(loop, 'loop ---> After Non-frozen-step best valid =',\n",
    "                          valid_metric_old, 'after', reps, 'epochs \\n')\n",
    "\n",
    "                    break\n",
    "\n",
    "            print(\"Total Non-frozen-step improved:\", improved)\n",
    "            if (improved == 0):\n",
    "                break\n",
    "\n",
    "            loop += 1\n",
    "\n",
    "        # Save Final Model\n",
    "        m_nn.save(f'{model_output_folder}/{file_name}_final.h5')\n",
    "\n",
    "        # OOF Predictions and Score #\n",
    "\n",
    "        val_preds = m_nn.predict([X_valid_1, X_valid_2])\n",
    "        fold_valid_score = mean_logloss(val_preds, y_valid_1)\n",
    "\n",
    "        oof_predictions[valid_index, :] += val_preds / len(SEEDS)\n",
    "        print('\\nSeed:', s, 'Fold:', k, 'score:', fold_valid_score)\n",
    "\n",
    "        # Generate Submission Prediction #\n",
    "        fold_submit_preds = m_nn.predict([X_test_1, X_test_2])\n",
    "        y_pred += fold_submit_preds / (KFOLDS * len(SEEDS))\n",
    "        print(fold_submit_preds[:5, :])\n",
    "\n",
    "        k += 1\n",
    "\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T08:46:27.991646Z",
     "iopub.status.busy": "2020-11-11T08:46:27.990901Z",
     "iopub.status.idle": "2020-11-11T08:46:28.000205Z",
     "shell.execute_reply": "2020-11-11T08:46:28.002353Z"
    },
    "papermill": {
     "duration": 14.092036,
     "end_time": "2020-11-11T08:46:28.002567",
     "exception": false,
     "start_time": "2020-11-11T08:46:13.910531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof_loss = mean_logloss(oof_predictions, Y)\n",
    "print(f\"OOF Validation Loss: {oof_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{model_output_folder}/oof_{oof_loss}.npy', 'wb') as f:\n",
    "    np.save(f, oof_predictions)\n",
    "\n",
    "with open(f'{model_output_folder}/oof_{oof_loss}.npy', 'rb') as f:\n",
    "    tmp = np.load(f)\n",
    "    print(tmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 13.933663,
     "end_time": "2020-11-11T08:46:56.74572",
     "exception": false,
     "start_time": "2020-11-11T08:46:42.812057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T08:48:54.311631Z",
     "iopub.status.busy": "2020-11-11T08:48:54.310535Z",
     "iopub.status.idle": "2020-11-11T08:48:54.916705Z",
     "shell.execute_reply": "2020-11-11T08:48:54.916073Z"
    },
    "papermill": {
     "duration": 14.67484,
     "end_time": "2020-11-11T08:48:54.916824",
     "exception": false,
     "start_time": "2020-11-11T08:48:40.241984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.iloc[:, 1:] = y_pred\n",
    "# sub.iloc[:, 1:] = np.clip(y_pred, P_MIN, P_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T08:49:24.916192Z",
     "iopub.status.busy": "2020-11-11T08:49:24.915133Z",
     "iopub.status.idle": "2020-11-11T08:49:24.955202Z",
     "shell.execute_reply": "2020-11-11T08:49:24.955799Z"
    },
    "papermill": {
     "duration": 14.848436,
     "end_time": "2020-11-11T08:49:24.95597",
     "exception": false,
     "start_time": "2020-11-11T08:49:10.107534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T08:50:23.604717Z",
     "iopub.status.busy": "2020-11-11T08:50:23.601559Z",
     "iopub.status.idle": "2020-11-11T08:50:26.002857Z",
     "shell.execute_reply": "2020-11-11T08:50:26.001634Z"
    },
    "papermill": {
     "duration": 16.925683,
     "end_time": "2020-11-11T08:50:26.003039",
     "exception": false,
     "start_time": "2020-11-11T08:50:09.077356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set ctl_vehicle to 0\n",
    "sub.iloc[test_features['cp_type'] == 'ctl_vehicle', 1:] = 0\n",
    "\n",
    "# Save Submission\n",
    "sub.to_csv('submission_improving-mark-s-2-heads-model.csv', index=False)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T08:50:54.60422Z",
     "iopub.status.busy": "2020-11-11T08:50:54.602918Z",
     "iopub.status.idle": "2020-11-11T08:50:54.644669Z",
     "shell.execute_reply": "2020-11-11T08:50:54.645356Z"
    },
    "papermill": {
     "duration": 14.756009,
     "end_time": "2020-11-11T08:50:54.645529",
     "exception": false,
     "start_time": "2020-11-11T08:50:39.88952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

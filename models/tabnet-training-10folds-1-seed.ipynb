{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":43,"outputs":[{"output_type":"stream","text":"/kaggle/input/tabnetdevelop/tabnet-develop/.dockerignore\n/kaggle/input/tabnetdevelop/tabnet-develop/forest_example.ipynb\n/kaggle/input/tabnetdevelop/tabnet-develop/.gitatttributes\n/kaggle/input/tabnetdevelop/tabnet-develop/.gitignore\n/kaggle/input/tabnetdevelop/tabnet-develop/CHANGELOG.md\n/kaggle/input/tabnetdevelop/tabnet-develop/renovate.json\n/kaggle/input/tabnetdevelop/tabnet-develop/census_example.ipynb\n/kaggle/input/tabnetdevelop/tabnet-develop/Dockerfile\n/kaggle/input/tabnetdevelop/tabnet-develop/pyproject.toml\n/kaggle/input/tabnetdevelop/tabnet-develop/LICENSE\n/kaggle/input/tabnetdevelop/tabnet-develop/Makefile\n/kaggle/input/tabnetdevelop/tabnet-develop/regression_example.ipynb\n/kaggle/input/tabnetdevelop/tabnet-develop/poetry.lock\n/kaggle/input/tabnetdevelop/tabnet-develop/.editorconfig\n/kaggle/input/tabnetdevelop/tabnet-develop/.flake8\n/kaggle/input/tabnetdevelop/tabnet-develop/Dockerfile_gpu\n/kaggle/input/tabnetdevelop/tabnet-develop/multi_regression_example.ipynb\n/kaggle/input/tabnetdevelop/tabnet-develop/README.md\n/kaggle/input/tabnetdevelop/tabnet-develop/release-script/Dockerfile_changelog\n/kaggle/input/tabnetdevelop/tabnet-develop/release-script/do-release.sh\n/kaggle/input/tabnetdevelop/tabnet-develop/release-script/prepare-release.sh\n/kaggle/input/tabnetdevelop/tabnet-develop/.github/PULL_REQUEST_TEMPLATE.md\n/kaggle/input/tabnetdevelop/tabnet-develop/.github/ISSUE_TEMPLATE/bug_report.md\n/kaggle/input/tabnetdevelop/tabnet-develop/.github/ISSUE_TEMPLATE/feature_request.md\n/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/tab_network.py\n/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/sparsemax.py\n/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/multiclass_utils.py\n/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/tab_model.py\n/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/utils.py\n/kaggle/input/tabnetdevelop/tabnet-develop/.circleci/config.yml\n/kaggle/input/lish-moa/train_features.csv\n/kaggle/input/lish-moa/test_features.csv\n/kaggle/input/lish-moa/train_targets_nonscored.csv\n/kaggle/input/lish-moa/sample_submission.csv\n/kaggle/input/lish-moa/train_targets_scored.csv\n/kaggle/input/iterative-stratification/iterative_stratification-0.1.6-py3-none-any.whl\n/kaggle/input/pytorch16gpu/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Install package"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"! pip install ../input/pytorch16gpu/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl","execution_count":44,"outputs":[{"output_type":"stream","text":"Processing /kaggle/input/pytorch16gpu/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==1.6.0cu101) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.6.0cu101) (1.18.5)\nInstalling collected packages: torch\n  Attempting uninstall: torch\n    Found existing installation: torch 1.6.0+cu101\n    Uninstalling torch-1.6.0+cu101:\n      Successfully uninstalled torch-1.6.0+cu101\n\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n\nkornia 0.3.2 requires torch<1.6.0,>=1.5.0, but you'll have torch 1.6.0+cu101 which is incompatible.\nallennlp 1.0.0 requires torch<1.6.0,>=1.5.0, but you'll have torch 1.6.0+cu101 which is incompatible.\u001b[0m\nSuccessfully installed torch-1.6.0+cu101\n","name":"stdout"}]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install ../input/iterative-stratification/iterative_stratification-0.1.6-py3-none-any.whl\n","execution_count":45,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: iterative-stratification==0.1.6 from file:///kaggle/input/iterative-stratification/iterative_stratification-0.1.6-py3-none-any.whl in /opt/conda/lib/python3.7/site-packages (0.1.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.4.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.18.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (0.23.2)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (0.14.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (2.1.0)\n","name":"stdout"}]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/tabnetdevelop/tabnet-develop\")","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA,FactorAnalysis\nimport numpy as np\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","execution_count":48,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":50,"outputs":[{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"(23814, 876)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RankGauss\nfrom sklearn.preprocessing import QuantileTransformer\n\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES\n\nn_comp = 90\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (FactorAnalysis(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n#CELLS\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (FactorAnalysis(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features ,test_features=fe_cluster(train_features,test_features)\n","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\ntrain_features,test_features=fe_stats(train_features,test_features)","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"(23814, 1071)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_vehicle = True\n\nif remove_vehicle:\n    train_features = train_features.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n    train_targets_nonscored = train_targets_nonscored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\nelse:\n    train_features = train_features","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"(21948, 1071)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ratio for each label\n\ndef get_ratio_labels(df):\n    columns = list(df.columns)\n    columns.pop(0)\n    ratios = []\n    toremove = []\n    for c in columns:\n        counts = df[c].value_counts()\n        if len(counts) != 1:\n            ratios.append(counts[0]/counts[1])\n        else:\n            toremove.append(c)\n    print(f\"remove {len(toremove)} columns\")\n    \n    for t in toremove:\n        columns.remove(t)\n    return columns, np.array(ratios).astype(np.int32)\n\ncolumns, ratios = get_ratio_labels(train_targets_scored)\ncolumns_nonscored, ratios_nonscored = get_ratio_labels(train_targets_nonscored)","execution_count":59,"outputs":[{"output_type":"stream","text":"remove 0 columns\nremove 71 columns\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_data(train, test, col, normalize=True, removed_vehicle=False):\n    \"\"\"\n        the first 3 columns represents categories, the others numericals features\n    \"\"\"\n    mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n               \"cp_time\":{48:0, 72:1, 24:2},\n               \"cp_dose\":{\"D1\":0, \"D2\":1}}\n    \n    if removed_vehicle:\n        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n    else:\n        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n    \n    max_ = 10.\n    min_ = -10.\n   \n    if removed_vehicle:\n        numerical_tr = train[col[3:]].values\n        numerical_test = test[col[3:]].values\n    else:\n        numerical_tr = train[col[3:]].values\n        numerical_test = test[col[3:]].values\n    if normalize:\n        numerical_tr = (numerical_tr-min_)/(max_ - min_)\n        numerical_test = (numerical_test-min_)/(max_ - min_)\n    return categories_tr, categories_test, numerical_tr, numerical_test\n\ncol_features = list(train_features.columns)[1:]\ncat_tr, cat_test, numerical_tr, numerical_test = transform_data(train_features, test_features, col_features, normalize=False, removed_vehicle=remove_vehicle)\ntargets_tr = train_targets_scored[columns].values.astype(np.float32)\ntargets2_tr = train_targets_nonscored[columns_nonscored].values.astype(np.float32)","execution_count":60,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"           \ndef evals(model, X, y, verbose=True):\n    with torch.no_grad():\n        y_preds = model.predict(X)\n        y_preds = torch.clamp(y_preds, 0.0,1.0).detach().numpy()\n    score = log_loss_multi(y, y_preds)\n    #print(\"Logloss = \", score)\n    return y_preds, score\n\n\ndef inference_fn(model, X ,verbose=True):\n    with torch.no_grad():\n        y_preds = model.predict( X )\n        y_preds = torch.sigmoid(torch.as_tensor(y_preds)).numpy()\n    return y_preds","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_score(actual, predicted,  eps=1e-15):\n\n        \"\"\"\n        :param predicted:   The predicted probabilities as floats between 0-1\n        :param actual:      The binary labels. Either 0 or 1.\n        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n        \"\"\"\n\n        \n        p1 = actual * np.log(predicted+eps)\n        p0 = (1-actual) * np.log(1-predicted+eps)\n        loss = p0 + p1\n\n        return -loss.mean()","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n    return results.mean()\n        ","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_targets(targets):\n    ### check if targets are all binary in training set\n    \n    for i in range(targets.shape[1]):\n        if len(np.unique(targets[:,i])) != 2:\n            return False\n    return True","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def auc_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        try:\n            results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n        except:\n            pass\n    return results.mean()","execution_count":65,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"## TABNET\n\nimport torch\nimport numpy as np\nfrom scipy.sparse import csc_matrix\nimport time\nfrom abc import abstractmethod\nfrom pytorch_tabnet import tab_network\nfrom pytorch_tabnet.multiclass_utils import unique_labels\nfrom sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\nfrom torch.nn.utils import clip_grad_norm_\nfrom pytorch_tabnet.utils import (PredictDataset,\n                                  create_dataloaders,\n                                  create_explain_matrix)\nfrom sklearn.base import BaseEstimator\nfrom torch.utils.data import DataLoader\nfrom copy import deepcopy\nimport io\nimport json\nfrom pathlib import Path\nimport shutil\nimport zipfile\n\nclass TabModel(BaseEstimator):\n    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n                 lambda_sparse=1e-3, seed=0,\n                 clip_value=1, verbose=1,\n                 optimizer_fn=torch.optim.Adam,\n                 optimizer_params=dict(lr=2e-2),\n                 scheduler_params=None, scheduler_fn=None,\n                 mask_type=\"sparsemax\",\n                 input_dim=None, output_dim=None,\n                 device_name='auto'):\n        \"\"\" Class for TabNet model\n        Parameters\n        ----------\n            device_name: str\n                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n        \"\"\"\n\n        self.n_d = n_d\n        self.n_a = n_a\n        self.n_steps = n_steps\n        self.gamma = gamma\n        self.cat_idxs = cat_idxs\n        self.cat_dims = cat_dims\n        self.cat_emb_dim = cat_emb_dim\n        self.n_independent = n_independent\n        self.n_shared = n_shared\n        self.epsilon = epsilon\n        self.momentum = momentum\n        self.lambda_sparse = lambda_sparse\n        self.clip_value = clip_value\n        self.verbose = verbose\n        self.optimizer_fn = optimizer_fn\n        self.optimizer_params = optimizer_params\n        self.device_name = device_name\n        self.scheduler_params = scheduler_params\n        self.scheduler_fn = scheduler_fn\n        self.mask_type = mask_type\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.batch_size = 1024\n\n        self.seed = seed\n        torch.manual_seed(self.seed)\n        # Defining device\n        if device_name == 'auto':\n            if torch.cuda.is_available():\n                device_name = 'cuda'\n            else:\n                device_name = 'cpu'\n        self.device = torch.device(device_name)\n        print(f\"Device used : {self.device}\")\n\n    @abstractmethod\n    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n                          weights, batch_size, num_workers, drop_last):\n        \"\"\"\n        Returns\n        -------\n        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n            Training and validation dataloaders\n        -------\n        \"\"\"\n        raise NotImplementedError('users must define construct_loaders to use this base class')\n\n    def init_network(\n                     self,\n                     input_dim,\n                     output_dim,\n                     n_d,\n                     n_a,\n                     n_steps,\n                     gamma,\n                     cat_idxs,\n                     cat_dims,\n                     cat_emb_dim,\n                     n_independent,\n                     n_shared,\n                     epsilon,\n                     virtual_batch_size,\n                     momentum,\n                     device_name,\n                     mask_type,\n                     ):\n        self.network = tab_network.TabNet(\n            input_dim,\n            output_dim,\n            n_d=n_d,\n            n_a=n_a,\n            n_steps=n_steps,\n            gamma=gamma,\n            cat_idxs=cat_idxs,\n            cat_dims=cat_dims,\n            cat_emb_dim=cat_emb_dim,\n            n_independent=n_independent,\n            n_shared=n_shared,\n            epsilon=epsilon,\n            virtual_batch_size=virtual_batch_size,\n            momentum=momentum,\n            device_name=device_name,\n            mask_type=mask_type).to(self.device)\n\n        self.reducing_matrix = create_explain_matrix(\n            self.network.input_dim,\n            self.network.cat_emb_dim,\n            self.network.cat_idxs,\n            self.network.post_embed_dim)\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n            weights=0, max_epochs=100, patience=10, batch_size=1024,\n            virtual_batch_size=128, num_workers=0, drop_last=False):\n        \"\"\"Train a neural network stored in self.network\n        Using train_dataloader for training data and\n        valid_dataloader for validation.\n        Parameters\n        ----------\n            X_train: np.ndarray\n                Train set\n            y_train : np.array\n                Train targets\n            X_train: np.ndarray\n                Train set\n            y_train : np.array\n                Train targets\n            weights : bool or dictionnary\n                0 for no balancing\n                1 for automated balancing\n                dict for custom weights per class\n            max_epochs : int\n                Maximum number of epochs during training\n            patience : int\n                Number of consecutive non improving epoch before early stopping\n            batch_size : int\n                Training batch size\n            virtual_batch_size : int\n                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n            num_workers : int\n                Number of workers used in torch.utils.data.DataLoader\n            drop_last : bool\n                Whether to drop last batch during training\n        \"\"\"\n        # update model name\n\n        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n                               weights, max_epochs, patience, batch_size,\n                               virtual_batch_size, num_workers, drop_last)\n\n        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n                                                                    y_train,\n                                                                    X_valid,\n                                                                    y_valid,\n                                                                    self.updated_weights,\n                                                                    self.batch_size,\n                                                                    self.num_workers,\n                                                                    self.drop_last)\n\n        self.init_network(\n            input_dim=self.input_dim,\n            output_dim=self.output_dim,\n            n_d=self.n_d,\n            n_a=self.n_a,\n            n_steps=self.n_steps,\n            gamma=self.gamma,\n            cat_idxs=self.cat_idxs,\n            cat_dims=self.cat_dims,\n            cat_emb_dim=self.cat_emb_dim,\n            n_independent=self.n_independent,\n            n_shared=self.n_shared,\n            epsilon=self.epsilon,\n            virtual_batch_size=self.virtual_batch_size,\n            momentum=self.momentum,\n            device_name=self.device_name,\n            mask_type=self.mask_type\n        )\n\n        self.optimizer = self.optimizer_fn(self.network.parameters(),\n                                           **self.optimizer_params)\n\n        if self.scheduler_fn:\n            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n        else:\n            self.scheduler = None\n\n        self.losses_train = []\n        self.losses_valid = []\n        self.learning_rates = []\n        self.metrics_train = []\n        self.metrics_valid = []\n\n        if self.verbose > 0:\n            print(\"Will train until validation stopping metric\",\n                  f\"hasn't improved in {self.patience} rounds.\")\n            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n            print('---------------------------------------')\n            print(msg_epoch)\n\n        total_time = 0\n        while (self.epoch < self.max_epochs and\n               self.patience_counter < self.patience):\n            starting_time = time.time()\n            # updates learning rate history\n            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n\n            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n\n            # leaving it here, may be used for callbacks later\n            self.losses_train.append(fit_metrics['train']['loss_avg'])\n            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n\n            stopping_loss = fit_metrics['valid']['stopping_loss']\n            if stopping_loss < self.best_cost:\n                self.best_cost = stopping_loss\n                self.patience_counter = 0\n                # Saving model\n                self.best_network = deepcopy(self.network)\n                has_improved = True\n            else:\n                self.patience_counter += 1\n                has_improved=False\n            self.epoch += 1\n            total_time += time.time() - starting_time\n            if self.verbose > 0:\n                if self.epoch % self.verbose == 0:\n                    separator = \"|\"\n                    msg_epoch = f\"| {self.epoch:<5} | \"\n                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n                    msg_epoch += f' {separator:<2} '\n                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n                    msg_epoch += f' {separator:<2} '\n                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n                    msg_epoch += f\" {has_improved}\"\n                    print(msg_epoch)\n\n        if self.verbose > 0:\n            if self.patience_counter == self.patience:\n                print(f\"Early stopping occured at epoch {self.epoch}\")\n            print(f\"Training done in {total_time:.3f} seconds.\")\n            print('---------------------------------------')\n\n        self.history = {\"train\": {\"loss\": self.losses_train,\n                                  \"metric\": self.metrics_train,\n                                  \"lr\": self.learning_rates},\n                        \"valid\": {\"loss\": self.losses_valid,\n                                  \"metric\": self.metrics_valid}}\n        # load best models post training\n        self.load_best_model()\n\n        # compute feature importance once the best model is defined\n        self._compute_feature_importances(train_dataloader)\n\n    def save_model(self, path):\n        \"\"\"\n        Saving model with two distinct files.\n        \"\"\"\n        saved_params = {}\n        for key, val in self.get_params().items():\n            if isinstance(val, type):\n                # Don't save torch specific params\n                continue\n            else:\n                saved_params[key] = val\n\n        # Create folder\n        Path(path).mkdir(parents=True, exist_ok=True)\n\n        # Save models params\n        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n            json.dump(saved_params, f)\n\n        # Save state_dict\n        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n        shutil.make_archive(path, 'zip', path)\n        shutil.rmtree(path)\n        print(f\"Successfully saved model at {path}.zip\")\n        return f\"{path}.zip\"\n\n    def load_model(self, filepath):\n\n        try:\n            try:\n                with zipfile.ZipFile(filepath) as z:\n                    with z.open(\"model_params.json\") as f:\n                        loaded_params = json.load(f)\n                    with z.open(\"network.pt\") as f:\n                        try:\n                            saved_state_dict = torch.load(f)\n                        except io.UnsupportedOperation:\n                            # In Python <3.7, the returned file object is not seekable (which at least\n                            # some versions of PyTorch require) - so we'll try buffering it in to a\n                            # BytesIO instead:\n                            saved_state_dict = torch.load(io.BytesIO(f.read()))\n                            \n            except:\n                with open(os.path.join(filepath, \"model_params.json\")) as f:\n                        loaded_params = json.load(f)\n\n                saved_state_dict = torch.load(os.path.join(filepath, \"network.pt\"), map_location=\"cpu\")\n \n        except KeyError:\n            raise KeyError(\"Your zip file is missing at least one component\")\n\n        #print(loaded_params)\n        if torch.cuda.is_available():\n            device_name = 'cuda'\n        else:\n            device_name = 'cpu'\n        loaded_params[\"device_name\"] = device_name\n        self.__init__(**loaded_params)\n        \n        \n\n        self.init_network(\n            input_dim=self.input_dim,\n            output_dim=self.output_dim,\n            n_d=self.n_d,\n            n_a=self.n_a,\n            n_steps=self.n_steps,\n            gamma=self.gamma,\n            cat_idxs=self.cat_idxs,\n            cat_dims=self.cat_dims,\n            cat_emb_dim=self.cat_emb_dim,\n            n_independent=self.n_independent,\n            n_shared=self.n_shared,\n            epsilon=self.epsilon,\n            virtual_batch_size=1024,\n            momentum=self.momentum,\n            device_name=self.device_name,\n            mask_type=self.mask_type\n        )\n        self.network.load_state_dict(saved_state_dict)\n        self.network.eval()\n        return\n\n    def fit_epoch(self, train_dataloader, valid_dataloader):\n        \"\"\"\n        Evaluates and updates network for one epoch.\n        Parameters\n        ----------\n            train_dataloader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with valid set\n        \"\"\"\n        train_metrics = self.train_epoch(train_dataloader)\n        valid_metrics = self.predict_epoch(valid_dataloader)\n\n        fit_metrics = {'train': train_metrics,\n                       'valid': valid_metrics}\n\n        return fit_metrics\n\n    @abstractmethod\n    def train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n            train_loader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n        \"\"\"\n        raise NotImplementedError('users must define train_epoch to use this base class')\n\n    @abstractmethod\n    def train_batch(self, data, targets):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n            data: a :tensor: `torch.tensor`\n                Input data\n            target: a :tensor: `torch.tensor`\n                Target data\n        \"\"\"\n        raise NotImplementedError('users must define train_batch to use this base class')\n\n    @abstractmethod\n    def predict_epoch(self, loader):\n        \"\"\"\n        Validates one epoch of the network in self.network\n        Parameters\n        ----------\n            loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with validation set\n        \"\"\"\n        raise NotImplementedError('users must define predict_epoch to use this base class')\n\n    @abstractmethod\n    def predict_batch(self, data, targets):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            batch_outs: dict\n        \"\"\"\n        raise NotImplementedError('users must define predict_batch to use this base class')\n\n    def load_best_model(self):\n        if self.best_network is not None:\n            self.network = self.best_network\n\n    @abstractmethod\n    def predict(self, X):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            predictions: np.array\n                Predictions of the regression problem or the last class\n        \"\"\"\n        raise NotImplementedError('users must define predict to use this base class')\n\n    def explain(self, X):\n        \"\"\"\n        Return local explanation\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            M_explain: matrix\n                Importance per sample, per columns.\n            masks: matrix\n                Sparse matrix showing attention masks used by network.\n        \"\"\"\n        self.network.eval()\n\n        dataloader = DataLoader(PredictDataset(X),\n                                batch_size=self.batch_size, shuffle=False)\n\n        for batch_nb, data in enumerate(dataloader):\n            data = data.to(self.device).float()\n\n            M_explain, masks = self.network.forward_masks(data)\n            for key, value in masks.items():\n                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n                                            self.reducing_matrix)\n\n            if batch_nb == 0:\n                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                             self.reducing_matrix)\n                res_masks = masks\n            else:\n                res_explain = np.vstack([res_explain,\n                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                                        self.reducing_matrix)])\n                for key, value in masks.items():\n                    res_masks[key] = np.vstack([res_masks[key], value])\n        return res_explain, res_masks\n\n    def _compute_feature_importances(self, loader):\n        self.network.eval()\n        feature_importances_ = np.zeros((self.network.post_embed_dim))\n        for data, targets in loader:\n            data = data.to(self.device).float()\n            M_explain, masks = self.network.forward_masks(data)\n            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n\n        feature_importances_ = csc_matrix.dot(feature_importances_,\n                                              self.reducing_matrix)\n        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n        \n        \nclass TabNetRegressor(TabModel):\n\n    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n                          batch_size, num_workers, drop_last):\n        \"\"\"\n        Returns\n        -------\n        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n            Training and validation dataloaders\n        -------\n        \"\"\"\n        if isinstance(weights, int):\n            if weights == 1:\n                raise ValueError(\"Please provide a list of weights for regression.\")\n        if isinstance(weights, dict):\n            raise ValueError(\"Please provide a list of weights for regression.\")\n\n        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n                                                                y_train,\n                                                                X_valid,\n                                                                y_valid,\n                                                                weights,\n                                                                batch_size,\n                                                                num_workers,\n                                                                drop_last)\n        return train_dataloader, valid_dataloader\n\n    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n                          weights, max_epochs, patience,\n                          batch_size, virtual_batch_size, num_workers, drop_last):\n\n        if loss_fn is None:\n            self.loss_fn = torch.nn.functional.mse_loss\n        else:\n            self.loss_fn = loss_fn\n\n        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n        self.input_dim = X_train.shape[1]\n\n        if len(y_train.shape) == 1:\n            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n                                if doing single regression.\"\"\")\n        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n        self.output_dim = y_train.shape[1]\n\n        self.updated_weights = weights\n\n        self.max_epochs = max_epochs\n        self.patience = patience\n        self.batch_size = batch_size\n        self.virtual_batch_size = virtual_batch_size\n        # Initialize counters and histories.\n        self.patience_counter = 0\n        self.epoch = 0\n        self.best_cost = np.inf\n        self.num_workers = num_workers\n        self.drop_last = drop_last\n\n    def train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n            train_loader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n        \"\"\"\n\n        self.network.train()\n        y_preds = []\n        ys = []\n        total_loss = 0\n\n        for data, targets in train_loader:\n            batch_outs = self.train_batch(data, targets)\n            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n            total_loss += batch_outs[\"loss\"]\n\n        y_preds = np.vstack(y_preds)\n        ys = np.vstack(ys)\n\n        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n        total_loss = total_loss / len(train_loader)\n        epoch_metrics = {'loss_avg': total_loss,\n                         'stopping_loss': stopping_loss,#total_loss,\n                         }\n\n        if self.scheduler is not None:\n            self.scheduler.step()\n        return epoch_metrics\n\n    def train_batch(self, data, targets):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n            data: a :tensor: `torch.tensor`\n                Input data\n            target: a :tensor: `torch.tensor`\n                Target data\n        \"\"\"\n        self.network.train()\n        data = data.to(self.device).float()\n\n        targets = targets.to(self.device).float()\n        self.optimizer.zero_grad()\n\n        output, M_loss = self.network(data)\n\n        loss = self.loss_fn(output, targets)\n        \n        loss -= self.lambda_sparse*M_loss\n\n        loss.backward()\n        if self.clip_value:\n            clip_grad_norm_(self.network.parameters(), self.clip_value)\n        self.optimizer.step()\n\n        loss_value = loss.item()\n        batch_outs = {'loss': loss_value,\n                      'y_preds': output,\n                      'y': targets}\n        return batch_outs\n\n    def predict_epoch(self, loader):\n        \"\"\"\n        Validates one epoch of the network in self.network\n        Parameters\n        ----------\n            loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with validation set\n        \"\"\"\n        y_preds = []\n        ys = []\n        self.network.eval()\n        total_loss = 0\n\n        for data, targets in loader:\n            batch_outs = self.predict_batch(data, targets)\n            total_loss += batch_outs[\"loss\"]\n            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n\n        y_preds = np.vstack(y_preds)\n        ys = np.vstack(ys)\n\n        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n\n        total_loss = total_loss / len(loader)\n        epoch_metrics = {'total_loss': total_loss,\n                         'stopping_loss': stopping_loss}\n\n        return epoch_metrics\n\n    def predict_batch(self, data, targets):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            batch_outs: dict\n        \"\"\"\n        self.network.eval()\n        data = data.to(self.device).float()\n        targets = targets.to(self.device).float()\n\n        output, M_loss = self.network(data)\n       \n        loss = self.loss_fn(output, targets)\n        #print(self.loss_fn, loss)\n        loss -= self.lambda_sparse*M_loss\n        #print(loss)\n        loss_value = loss.item()\n        batch_outs = {'loss': loss_value,\n                      'y_preds': output,\n                      'y': targets}\n        return batch_outs\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            predictions: np.array\n                Predictions of the regression problem\n        \"\"\"\n        self.network.eval()\n        dataloader = DataLoader(PredictDataset(X),\n                                batch_size=self.batch_size, shuffle=False)\n\n        results = []\n        for batch_nb, data in enumerate(dataloader):\n            data = data.to(self.device).float()\n\n            output, M_loss = self.network(data)\n            predictions = output.cpu().detach().numpy()\n            results.append(predictions)\n        res = np.vstack(results)\n        return res","execution_count":66,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# script"},{"metadata":{"trusted":true},"cell_type":"code","source":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass LabelSmoothing(nn.Module):\n    \"\"\"\n    NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.0, n_cls=2):\n        \"\"\"\n        Constructor for the LabelSmoothing module.\n        :param smoothing: label smoothing factor\n        \"\"\"\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing + smoothing / n_cls\n        self.smoothing = smoothing / n_cls\n\n    def forward(self, x, target):\n        probs = torch.nn.functional.sigmoid(x,)\n        # ylogy + (1-y)log(1-y)\n        #with torch.no_grad():\n        target1 = self.confidence * target + (1-target) * self.smoothing\n        #print(target1.cpu())\n        loss = -(torch.log(probs+1e-15) * target1 + (1-target1) * torch.log(1-probs+1e-15))\n        #print(loss.cpu())\n        #nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        #nll_loss = nll_loss.squeeze(1)\n        #smooth_loss = -logprobs.mean(dim=-1)\n        #loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config(object):\n    def __init__(self):\n        self.num_class = targets_tr.shape[1]\n        self.verbose=False\n        #\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.SPLITS = 10\n        self.EPOCHS = 200\n        self.num_ensembling = 1\n        self.seed = 0\n        # Parameters model\n        self.cat_emb_dim=[1] * cat_tr.shape[1] #to choose\n        self.cats_idx = list(range(cat_tr.shape[1]))\n        self.cat_dims = [len(np.unique(cat_tr[:, i])) for i in range(cat_tr.shape[1])]\n        self.num_numericals= numerical_tr.shape[1]\n        # save\n        self.save_name = \"tabnet_raw_step1\"\n        \n        self.strategy = \"KFOLD\" # \ncfg = Config()\n","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.concatenate([cat_test, numerical_test ], axis=1)","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if cfg.strategy == \"KFOLD\":    \n    oof_preds_all = []\n    oof_targets_all = []\n    scores_all =  []\n    scores_auc_all= []\n    preds_test = []\n    for seed in range(cfg.num_ensembling):\n        print(\"## SEED : \", seed)\n        mskf = MultilabelStratifiedKFold(n_splits=cfg.SPLITS, random_state=cfg.seed+seed, shuffle=True)\n        oof_preds = []\n        oof_targets = []\n        scores = []\n        scores_auc = []\n        p = []\n        for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n            print(\"FOLDS : \", j)\n\n            ## model\n            X_train, y_train = torch.as_tensor(np.concatenate([cat_tr[train_idx], numerical_tr[train_idx] ], axis=1)), torch.as_tensor(targets_tr[train_idx])\n            X_val, y_val = torch.as_tensor(np.concatenate([cat_tr[val_idx], numerical_tr[val_idx] ], axis=1)), torch.as_tensor(targets_tr[val_idx])\n            model = TabNetRegressor(n_d=32, n_a=32, n_steps=1, lambda_sparse=0, \n                                    cat_dims=cfg.cat_dims, cat_emb_dim=cfg.cat_emb_dim, cat_idxs=cfg.cats_idx, \n                                    optimizer_fn=torch.optim.Adam,\n                                   optimizer_params=dict(lr=2e-2, weight_decay=1e-5), \n                                    mask_type='entmax', device_name=cfg.device, \n                                    scheduler_params=dict(milestones=[100, 150], gamma=0.9),#)\n                                    scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n            #'sparsemax'\n            model.fit(X_train=X_train, y_train=y_train, X_valid=X_val, y_valid=y_val,max_epochs=cfg.EPOCHS, patience=50, \n                      batch_size=1024, virtual_batch_size=128,\nnum_workers=0, drop_last=False, loss_fn=LabelSmoothing(0.001)\n            #loss_fn=torch.nn.functional.binary_cross_entropy_with_logits\n                     )\n            model.load_best_model()\n\n            name = cfg.save_name + f\"_fold{j}_{seed}\"\n            model.save_model(name)\n            #model.load_model(name)\n            # preds on val\n            preds = model.predict(X_val)\n            preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n            score = log_loss_multi(y_val, preds)\n            \n            # preds on test\n            temp = model.predict(X_test)\n            p.append(torch.sigmoid(torch.as_tensor(temp)).detach().cpu().numpy())\n            ## save oof to compute the CV later\n            oof_preds.append(preds)\n            oof_targets.append(y_val)\n            scores.append(score)\n            scores_auc.append(auc_multi(y_val,preds))\n            print(f\"validation fold {j} : {score}\")\n        p = np.stack(p)\n        preds_test.append(p)\n        oof_preds_all.append(np.concatenate(oof_preds))\n        oof_targets_all.append(np.concatenate(oof_targets))\n        scores_all.append(np.array(scores))\n        scores_auc_all.append(np.array(scores_auc))\n    preds_test = np.stack(preds_test)","execution_count":85,"outputs":[{"output_type":"stream","text":"## SEED :  0\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"FOLDS :  0\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","name":"stderr"},{"output_type":"stream","text":"| 1     |  0.36799 |   0.03680 |   1.8        True\n| 2     |  0.02781 |   0.02499 |   3.8        True\n| 3     |  0.02227 |   0.02122 |   5.8        True\n| 4     |  0.02116 |   0.02069 |   7.6        True\n| 5     |  0.02062 |   0.02043 |   9.6        True\n| 6     |  0.02032 |   0.02033 |   11.4       True\n| 7     |  0.01996 |   0.01989 |   13.2       True\n| 8     |  0.01953 |   0.01959 |   15.1       True\n| 9     |  0.01914 |   0.01922 |   17.7       True\n| 10    |  0.01885 |   0.01904 |   19.5       True\n| 11    |  0.01857 |   0.01897 |   21.3       True\n| 12    |  0.01831 |   0.01983 |   23.3       False\n| 13    |  0.01813 |   0.01912 |   25.1       False\n| 14    |  0.01799 |   0.01945 |   27.2       False\n| 15    |  0.01792 |   0.01966 |   29.1       False\n| 16    |  0.01766 |   0.01958 |   31.0       False\n| 17    |  0.01758 |   0.02107 |   33.3       False\n| 18    |  0.01766 |   0.02028 |   35.2       False\n| 19    |  0.01739 |   0.01825 |   37.3       True\n| 20    |  0.01722 |   0.01997 |   39.5       False\n| 21    |  0.01723 |   0.01781 |   41.3       True\n| 22    |  0.01708 |   0.01955 |   43.2       False\n| 23    |  0.01700 |   0.01794 |   45.1       False\n| 24    |  0.01696 |   0.01883 |   46.9       False\n| 25    |  0.01695 |   0.02021 |   49.5       False\n| 26    |  0.01681 |   0.01888 |   51.5       False\n| 27    |  0.01674 |   0.01827 |   53.3       False\n| 28    |  0.01666 |   0.01946 |   55.2       False\n| 29    |  0.01696 |   0.01922 |   57.1       False\n| 30    |  0.01678 |   0.01950 |   59.1       False\n| 31    |  0.01657 |   0.01917 |   61.2       False\n| 32    |  0.01650 |   0.01969 |   63.1       False\n| 33    |  0.01650 |   0.01727 |   65.0       True\n| 34    |  0.01648 |   0.01874 |   66.9       False\n| 35    |  0.01661 |   0.01912 |   68.7       False\n| 36    |  0.01667 |   0.01719 |   70.8       True\n| 37    |  0.01645 |   0.01729 |   72.8       False\n| 38    |  0.01623 |   0.01886 |   74.6       False\n| 39    |  0.01643 |   0.01723 |   76.5       False\n| 40    |  0.01638 |   0.01703 |   78.4       True\n| 41    |  0.01620 |   0.01744 |   80.3       False\n| 42    |  0.01617 |   0.01732 |   82.8       False\n| 43    |  0.01620 |   0.01692 |   84.8       True\n| 44    |  0.01618 |   0.01728 |   86.7       False\n| 45    |  0.01620 |   0.01695 |   88.5       False\n| 46    |  0.01608 |   0.01690 |   90.5       True\n| 47    |  0.01611 |   0.01755 |   93.3       False\n| 48    |  0.01619 |   0.01710 |   95.1       False\n| 49    |  0.01627 |   0.01702 |   97.1       False\n| 50    |  0.01615 |   0.01717 |   99.0       False\n| 51    |  0.01619 |   0.01701 |   100.9      False\n| 52    |  0.01612 |   0.01706 |   102.9      False\n| 53    |  0.01613 |   0.01710 |   105.0      False\n| 54    |  0.01607 |   0.01684 |   106.9      True\n| 55    |  0.01606 |   0.01704 |   108.8      False\n| 56    |  0.01615 |   0.01743 |   110.7      False\n| 57    |  0.01619 |   0.01693 |   112.5      False\n| 58    |  0.01609 |   0.01704 |   115.1      False\n| 59    |  0.01602 |   0.01695 |   116.9      False\n| 60    |  0.01600 |   0.01724 |   118.9      False\n| 61    |  0.01610 |   0.01688 |   120.8      False\n| 62    |  0.01601 |   0.01694 |   122.7      False\n| 63    |  0.01599 |   0.01717 |   124.5      False\n| 64    |  0.01606 |   0.01698 |   126.6      False\n| 65    |  0.01597 |   0.01703 |   128.5      False\n| 66    |  0.01606 |   0.01715 |   130.3      False\n| 67    |  0.01607 |   0.01703 |   132.2      False\n| 68    |  0.01611 |   0.01709 |   134.1      False\n| 69    |  0.01600 |   0.01697 |   135.9      False\n| 70    |  0.01612 |   0.01701 |   138.0      False\n| 71    |  0.01608 |   0.01723 |   139.9      False\n| 72    |  0.01599 |   0.01691 |   141.7      False\n| 73    |  0.01597 |   0.01703 |   143.7      False\n| 74    |  0.01599 |   0.01693 |   145.7      False\n| 75    |  0.01604 |   0.01706 |   147.9      False\n| 76    |  0.01600 |   0.01697 |   149.9      False\n| 77    |  0.01590 |   0.01785 |   152.1      False\n| 78    |  0.01604 |   0.01715 |   154.0      False\n| 79    |  0.01609 |   0.01716 |   155.9      False\n| 80    |  0.01602 |   0.01694 |   158.1      False\n| 81    |  0.01598 |   0.01701 |   160.3      False\n| 82    |  0.01601 |   0.01681 |   162.3      True\n| 83    |  0.01607 |   0.01681 |   164.2      False\n| 84    |  0.01602 |   0.01698 |   166.0      False\n| 85    |  0.01593 |   0.01684 |   168.0      False\n| 86    |  0.01593 |   0.01714 |   170.0      False\n| 87    |  0.01593 |   0.01671 |   172.0      True\n| 88    |  0.01591 |   0.01692 |   173.8      False\n| 89    |  0.01598 |   0.01715 |   175.7      False\n| 90    |  0.01595 |   0.01735 |   178.0      False\n| 91    |  0.01596 |   0.01681 |   179.9      False\n| 92    |  0.01593 |   0.01696 |   182.0      False\n| 93    |  0.01593 |   0.01694 |   183.9      False\n| 94    |  0.01595 |   0.01705 |   185.8      False\n| 95    |  0.01606 |   0.01698 |   187.7      False\n| 96    |  0.01589 |   0.01693 |   189.6      False\n| 97    |  0.01590 |   0.01715 |   191.7      False\n| 98    |  0.01599 |   0.01727 |   193.6      False\n| 99    |  0.01601 |   0.01696 |   195.5      False\n| 100   |  0.01603 |   0.01728 |   197.4      False\n| 101   |  0.01592 |   0.01680 |   199.3      False\n| 102   |  0.01587 |   0.01707 |   201.2      False\n| 103   |  0.01580 |   0.01704 |   203.3      False\n| 104   |  0.01572 |   0.01682 |   205.3      False\n| 105   |  0.01574 |   0.01690 |   207.3      False\n| 106   |  0.01580 |   0.01680 |   209.2      False\n| 107   |  0.01569 |   0.01716 |   211.5      False\n| 108   |  0.01574 |   0.01698 |   214.0      False\n| 109   |  0.01571 |   0.01697 |   215.9      False\n| 110   |  0.01574 |   0.01716 |   217.9      False\n| 111   |  0.01579 |   0.01700 |   219.9      False\n| 112   |  0.01568 |   0.01688 |   221.8      False\n| 113   |  0.01569 |   0.01685 |   223.6      False\n| 114   |  0.01583 |   0.01702 |   225.8      False\n| 115   |  0.01573 |   0.01679 |   227.7      False\n| 116   |  0.01566 |   0.01691 |   229.6      False\n| 117   |  0.01576 |   0.01692 |   231.5      False\n| 118   |  0.01570 |   0.01680 |   233.4      False\n| 119   |  0.01563 |   0.01692 |   235.3      False\n| 120   |  0.01563 |   0.01697 |   237.4      False\n| 121   |  0.01571 |   0.01702 |   239.3      False\n| 122   |  0.01563 |   0.01690 |   241.2      False\n| 123   |  0.01573 |   0.01673 |   243.5      False\n| 124   |  0.01581 |   0.01690 |   245.4      False\n| 125   |  0.01566 |   0.01699 |   247.5      False\n| 126   |  0.01561 |   0.01704 |   249.4      False\n| 127   |  0.01585 |   0.01703 |   251.3      False\n| 128   |  0.01567 |   0.01678 |   253.1      False\n| 129   |  0.01570 |   0.01697 |   255.1      False\n| 130   |  0.01565 |   0.01757 |   257.0      False\n| 131   |  0.01558 |   0.01693 |   259.1      False\n| 132   |  0.01561 |   0.01731 |   261.0      False\n| 133   |  0.01580 |   0.01709 |   262.9      False\n| 134   |  0.01573 |   0.01711 |   264.7      False\n| 135   |  0.01573 |   0.01691 |   266.7      False\n| 136   |  0.01564 |   0.01682 |   268.9      False\n| 137   |  0.01564 |   0.01709 |   270.8      False\nEarly stopping occured at epoch 137\nTraining done in 270.803 seconds.\n---------------------------------------\nSuccessfully saved model at tabnet_raw_step1_fold0_0.zip\nvalidation fold 0 : 0.016706212160394536\nFOLDS :  1\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","name":"stderr"},{"output_type":"stream","text":"| 1     |  0.37059 |   0.03865 |   1.9        True\n| 2     |  0.02757 |   0.02524 |   3.8        True\n| 3     |  0.02215 |   0.02103 |   6.0        True\n| 4     |  0.02104 |   0.02056 |   8.0        True\n| 5     |  0.02062 |   0.02030 |   9.8        True\n| 6     |  0.02040 |   0.02011 |   11.7       True\n| 7     |  0.02011 |   0.01978 |   13.7       True\n| 8     |  0.01959 |   0.01971 |   15.5       True\n| 9     |  0.01907 |   0.01909 |   17.6       True\n| 10    |  0.01881 |   0.01899 |   19.6       True\n| 11    |  0.01843 |   0.01860 |   21.4       True\n| 12    |  0.01831 |   0.01864 |   23.3       False\n| 13    |  0.01813 |   0.01835 |   25.3       True\n| 14    |  0.01796 |   0.01944 |   27.2       False\n| 15    |  0.01791 |   0.01911 |   29.2       False\n| 16    |  0.01781 |   0.01923 |   31.3       False\n| 17    |  0.01769 |   0.01823 |   33.4       True\n| 18    |  0.01746 |   0.01992 |   35.3       False\n| 19    |  0.01745 |   0.02123 |   37.1       False\n| 20    |  0.01727 |   0.01915 |   39.3       False\n| 21    |  0.01709 |   0.01800 |   41.2       True\n| 22    |  0.01710 |   0.02030 |   43.0       False\n| 23    |  0.01697 |   0.01772 |   44.9       True\n| 24    |  0.01699 |   0.01790 |   46.9       False\n| 25    |  0.01697 |   0.01877 |   48.8       False\n| 26    |  0.01691 |   0.01863 |   50.9       False\n| 27    |  0.01683 |   0.01808 |   53.0       False\n| 28    |  0.01677 |   0.01801 |   54.9       False\n| 29    |  0.01666 |   0.01739 |   56.8       True\n| 30    |  0.01670 |   0.01815 |   59.2       False\n| 31    |  0.01666 |   0.01735 |   61.1       True\n| 32    |  0.01667 |   0.01788 |   63.7       False\n| 33    |  0.01656 |   0.02113 |   66.0       False\n| 34    |  0.01662 |   0.01750 |   67.9       False\n| 35    |  0.01662 |   0.01731 |   69.9       True\n| 36    |  0.01653 |   0.01700 |   72.1       True\n| 37    |  0.01641 |   0.01707 |   74.1       False\n| 38    |  0.01640 |   0.01696 |   76.0       True\n| 39    |  0.01636 |   0.01719 |   78.0       False\n| 40    |  0.01639 |   0.01703 |   79.8       False\n| 41    |  0.01626 |   0.01798 |   81.7       False\n| 42    |  0.01647 |   0.01746 |   83.9       False\n| 43    |  0.01650 |   0.01688 |   85.9       True\n| 44    |  0.01633 |   0.01681 |   87.8       True\n| 45    |  0.01635 |   0.01701 |   89.8       False\n| 46    |  0.01631 |   0.01723 |   91.8       False\n| 47    |  0.01635 |   0.01693 |   93.9       False\n| 48    |  0.01617 |   0.01678 |   96.3       True\n| 49    |  0.01614 |   0.01690 |   98.2       False\n| 50    |  0.01615 |   0.01711 |   100.2      False\n| 51    |  0.01616 |   0.01715 |   102.0      False\n| 52    |  0.01627 |   0.01695 |   104.0      False\n| 53    |  0.01619 |   0.01690 |   106.2      False\n| 54    |  0.01611 |   0.01688 |   108.2      False\n| 55    |  0.01622 |   0.01692 |   110.1      False\n| 56    |  0.01622 |   0.01690 |   112.0      False\n| 57    |  0.01619 |   0.01698 |   114.1      False\n| 58    |  0.01611 |   0.01704 |   116.1      False\n| 59    |  0.01614 |   0.01693 |   118.4      False\n| 60    |  0.01610 |   0.01688 |   120.6      False\n| 61    |  0.01600 |   0.01713 |   122.5      False\n| 62    |  0.01618 |   0.01695 |   124.4      False\n| 63    |  0.01623 |   0.01697 |   126.6      False\n| 64    |  0.01605 |   0.01672 |   129.0      True\n| 65    |  0.01600 |   0.01676 |   130.8      False\n| 66    |  0.01614 |   0.01705 |   132.7      False\n| 67    |  0.01608 |   0.01707 |   134.7      False\n| 68    |  0.01598 |   0.01708 |   136.6      False\n| 69    |  0.01598 |   0.01679 |   138.6      False\n| 70    |  0.01598 |   0.01683 |   140.6      False\n| 71    |  0.01601 |   0.01694 |   142.5      False\n| 72    |  0.01613 |   0.01694 |   144.5      False\n| 73    |  0.01607 |   0.01694 |   146.4      False\n| 74    |  0.01604 |   0.01684 |   148.4      False\n| 75    |  0.01600 |   0.01692 |   150.4      False\n| 76    |  0.01598 |   0.01697 |   152.3      False\n| 77    |  0.01602 |   0.01690 |   154.2      False\n| 78    |  0.01606 |   0.01693 |   156.2      False\n| 79    |  0.01604 |   0.01707 |   158.1      False\n| 80    |  0.01604 |   0.01697 |   160.7      False\n| 81    |  0.01596 |   0.01706 |   162.6      False\n| 82    |  0.01602 |   0.01693 |   164.5      False\n| 83    |  0.01602 |   0.01692 |   166.3      False\n| 84    |  0.01602 |   0.01698 |   168.3      False\n| 85    |  0.01597 |   0.01690 |   170.3      False\n| 86    |  0.01612 |   0.01742 |   172.3      False\n| 87    |  0.01602 |   0.01695 |   174.3      False\n| 88    |  0.01604 |   0.01700 |   176.2      False\n| 89    |  0.01604 |   0.01685 |   178.4      False\n| 90    |  0.01595 |   0.01709 |   180.4      False\n| 91    |  0.01598 |   0.01693 |   182.5      False\n| 92    |  0.01613 |   0.01696 |   184.7      False\n| 93    |  0.01603 |   0.01684 |   186.6      False\n| 94    |  0.01607 |   0.01703 |   188.6      False\n| 95    |  0.01609 |   0.01721 |   190.5      False\n| 96    |  0.01606 |   0.01691 |   193.1      False\n| 97    |  0.01598 |   0.01796 |   195.0      False\n| 98    |  0.01596 |   0.01707 |   196.8      False\n| 99    |  0.01597 |   0.01690 |   198.8      False\n| 100   |  0.01601 |   0.01694 |   200.7      False\n| 101   |  0.01592 |   0.01689 |   202.5      False\n| 102   |  0.01583 |   0.01682 |   204.7      False\n| 103   |  0.01575 |   0.01708 |   206.6      False\n| 104   |  0.01596 |   0.01719 |   208.5      False\n| 105   |  0.01593 |   0.01685 |   210.5      False\n| 106   |  0.01590 |   0.01686 |   212.3      False\n| 107   |  0.01582 |   0.01718 |   214.4      False\n| 108   |  0.01589 |   0.01688 |   216.5      False\n| 109   |  0.01579 |   0.01681 |   218.3      False\n| 110   |  0.01585 |   0.01699 |   220.2      False\n| 111   |  0.01571 |   0.01697 |   222.2      False\n| 112   |  0.01579 |   0.01696 |   224.4      False\n| 113   |  0.01584 |   0.01683 |   226.6      False\n| 114   |  0.01578 |   0.01683 |   228.6      False\nEarly stopping occured at epoch 114\nTraining done in 228.641 seconds.\n---------------------------------------\nSuccessfully saved model at tabnet_raw_step1_fold1_0.zip\nvalidation fold 1 : 0.016721699695957336\nFOLDS :  2\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","name":"stderr"},{"output_type":"stream","text":"| 1     |  0.37172 |   0.03809 |   1.9        True\n| 2     |  0.02792 |   0.02544 |   3.8        True\n| 3     |  0.02231 |   0.02170 |   6.1        True\n| 4     |  0.02121 |   0.02115 |   8.4        True\n| 5     |  0.02069 |   0.02065 |   10.6       True\n| 6     |  0.02031 |   0.02028 |   12.6       True\n| 7     |  0.01993 |   0.02012 |   14.5       True\n| 8     |  0.01943 |   0.01975 |   16.6       True\n| 9     |  0.01908 |   0.01944 |   18.7       True\n| 10    |  0.01872 |   0.02109 |   20.6       False\n| 11    |  0.01836 |   0.01909 |   22.5       True\n| 12    |  0.01816 |   0.01975 |   24.4       False\n| 13    |  0.01791 |   0.01946 |   26.7       False\n| 14    |  0.01780 |   0.01871 |   28.8       True\n| 15    |  0.01763 |   0.01844 |   30.8       True\n| 16    |  0.01764 |   0.01859 |   32.7       False\n| 17    |  0.01761 |   0.01955 |   34.6       False\n| 18    |  0.01739 |   0.02063 |   36.6       False\n| 19    |  0.01729 |   0.01807 |   38.7       True\n| 20    |  0.01722 |   0.01821 |   40.6       False\n| 21    |  0.01713 |   0.01857 |   42.6       False\n| 22    |  0.01714 |   0.01841 |   44.5       False\n| 23    |  0.01701 |   0.01853 |   46.4       False\n| 24    |  0.01710 |   0.01845 |   48.3       False\n| 25    |  0.01697 |   0.01793 |   50.4       True\n| 26    |  0.01682 |   0.01810 |   52.3       False\n| 27    |  0.01675 |   0.01766 |   54.3       True\n| 28    |  0.01675 |   0.01986 |   56.2       False\n| 29    |  0.01678 |   0.01787 |   58.4       False\n| 30    |  0.01668 |   0.01789 |   60.5       False\n| 31    |  0.01655 |   0.01978 |   62.5       False\n| 32    |  0.01656 |   0.01746 |   64.5       True\n| 33    |  0.01654 |   0.01790 |   66.6       False\n| 34    |  0.01648 |   0.01830 |   68.9       False\n| 35    |  0.01657 |   0.01917 |   71.0       False\n| 36    |  0.01662 |   0.01780 |   73.2       False\n| 37    |  0.01660 |   0.01800 |   75.2       False\n| 38    |  0.01645 |   0.01907 |   77.2       False\n| 39    |  0.01633 |   0.01846 |   79.0       False\n| 40    |  0.01641 |   0.01745 |   80.9       True\n| 41    |  0.01636 |   0.01807 |   83.2       False\n| 42    |  0.01640 |   0.01722 |   85.1       True\n| 43    |  0.01623 |   0.01742 |   87.0       False\n| 44    |  0.01625 |   0.01724 |   89.2       False\n| 45    |  0.01622 |   0.01744 |   91.4       False\n| 46    |  0.01620 |   0.01729 |   93.4       False\n| 47    |  0.01632 |   0.01745 |   95.5       False\n| 48    |  0.01627 |   0.01732 |   97.4       False\n| 49    |  0.01630 |   0.01701 |   99.3       True\n| 50    |  0.01619 |   0.01732 |   101.3      False\n| 51    |  0.01614 |   0.01714 |   103.2      False\n| 52    |  0.01602 |   0.01721 |   105.4      False\n| 53    |  0.01605 |   0.01732 |   107.4      False\n| 54    |  0.01613 |   0.01749 |   109.3      False\n| 55    |  0.01619 |   0.01731 |   111.2      False\n| 56    |  0.01600 |   0.01715 |   113.2      False\n| 57    |  0.01596 |   0.01697 |   115.1      True\n| 58    |  0.01599 |   0.01703 |   117.2      False\n| 59    |  0.01604 |   0.01708 |   119.2      False\n| 60    |  0.01599 |   0.01730 |   121.1      False\n| 61    |  0.01596 |   0.01705 |   123.4      False\n| 62    |  0.01599 |   0.01706 |   125.3      False\n| 63    |  0.01595 |   0.01701 |   128.0      False\n| 64    |  0.01602 |   0.01695 |   130.0      True\n| 65    |  0.01601 |   0.01687 |   132.2      True\n| 66    |  0.01594 |   0.01711 |   134.2      False\n| 67    |  0.01601 |   0.01715 |   136.2      False\n| 68    |  0.01597 |   0.01707 |   138.4      False\n| 69    |  0.01590 |   0.01720 |   140.3      False\n| 70    |  0.01592 |   0.01702 |   142.3      False\n| 71    |  0.01591 |   0.01708 |   144.2      False\n| 72    |  0.01592 |   0.01700 |   146.1      False\n| 73    |  0.01618 |   0.01717 |   148.1      False\n| 74    |  0.01595 |   0.01716 |   150.3      False\n| 75    |  0.01586 |   0.01697 |   152.3      False\n| 76    |  0.01582 |   0.01766 |   154.6      False\n| 77    |  0.01587 |   0.01700 |   156.5      False\n| 78    |  0.01603 |   0.01718 |   158.4      False\n| 79    |  0.01594 |   0.01705 |   160.7      False\n| 80    |  0.01598 |   0.01694 |   162.6      False\n| 81    |  0.01607 |   0.01883 |   164.5      False\n| 82    |  0.01623 |   0.01717 |   166.6      False\n| 83    |  0.01604 |   0.01725 |   168.6      False\n| 84    |  0.01594 |   0.01696 |   170.7      False\n| 85    |  0.01594 |   0.01718 |   172.8      False\n| 86    |  0.01587 |   0.01704 |   174.7      False\n| 87    |  0.01591 |   0.01748 |   176.6      False\n| 88    |  0.01590 |   0.01749 |   178.6      False\n| 89    |  0.01603 |   0.01733 |   180.5      False\n| 90    |  0.01590 |   0.01725 |   182.7      False\n| 91    |  0.01586 |   0.01694 |   184.7      False\n| 92    |  0.01593 |   0.01688 |   187.3      False\n| 93    |  0.01584 |   0.01693 |   189.2      False\n| 94    |  0.01586 |   0.01717 |   191.3      False\n| 95    |  0.01591 |   0.01720 |   193.7      False\n| 96    |  0.01589 |   0.01698 |   195.6      False\n| 97    |  0.01581 |   0.01720 |   197.7      False\n| 98    |  0.01587 |   0.01715 |   199.6      False\n| 99    |  0.01587 |   0.01701 |   201.5      False\n| 100   |  0.01589 |   0.01704 |   203.6      False\n| 101   |  0.01577 |   0.01705 |   205.7      False\n| 102   |  0.01576 |   0.01721 |   207.7      False\n| 103   |  0.01580 |   0.01723 |   209.7      False\n| 104   |  0.01584 |   0.01703 |   211.6      False\n| 105   |  0.01573 |   0.01703 |   213.5      False\n| 106   |  0.01561 |   0.01766 |   215.7      False\n| 107   |  0.01589 |   0.01698 |   217.8      False\n| 108   |  0.01576 |   0.01718 |   219.8      False\n| 109   |  0.01576 |   0.01711 |   221.8      False\n| 110   |  0.01577 |   0.01723 |   223.7      False\n| 111   |  0.01570 |   0.01691 |   225.8      False\n| 112   |  0.01566 |   0.01718 |   227.8      False\n| 113   |  0.01575 |   0.01720 |   229.7      False\n| 114   |  0.01577 |   0.01705 |   231.7      False\n| 115   |  0.01577 |   0.01715 |   233.7      False\nEarly stopping occured at epoch 115\nTraining done in 233.713 seconds.\n---------------------------------------\nSuccessfully saved model at tabnet_raw_step1_fold2_0.zip\nvalidation fold 2 : 0.0168662614486343\nFOLDS :  3\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","name":"stderr"},{"output_type":"stream","text":"| 1     |  0.36457 |   0.03327 |   2.2        True\n| 2     |  0.02746 |   0.02463 |   4.2        True\n| 3     |  0.02222 |   0.02123 |   6.1        True\n| 4     |  0.02119 |   0.02073 |   8.1        True\n| 5     |  0.02076 |   0.02056 |   10.0       True\n| 6     |  0.02054 |   0.02071 |   12.9       False\n| 7     |  0.02032 |   0.02084 |   15.3       False\n| 8     |  0.02001 |   0.02029 |   17.4       True\n| 9     |  0.01957 |   0.02144 |   19.4       False\n| 10    |  0.01920 |   0.01992 |   21.3       True\n| 11    |  0.01891 |   0.01922 |   23.5       True\n| 12    |  0.01863 |   0.02093 |   25.6       False\n| 13    |  0.01848 |   0.02244 |   27.5       False\n| 14    |  0.01832 |   0.02048 |   29.5       False\n| 15    |  0.01805 |   0.02021 |   31.4       False\n| 16    |  0.01785 |   0.01843 |   33.4       True\n| 17    |  0.01769 |   0.01862 |   35.5       False\n| 18    |  0.01764 |   0.01888 |   37.5       False\n| 19    |  0.01752 |   0.01807 |   39.5       True\n| 20    |  0.01736 |   0.02047 |   41.4       False\n| 21    |  0.01729 |   0.01991 |   43.3       False\n| 22    |  0.01717 |   0.01937 |   45.6       False\n| 23    |  0.01722 |   0.01810 |   47.7       False\n| 24    |  0.01716 |   0.01879 |   49.6       False\n| 25    |  0.01704 |   0.01797 |   51.5       True\n| 26    |  0.01692 |   0.01781 |   53.5       True\n| 27    |  0.01683 |   0.01733 |   55.5       True\n| 28    |  0.01697 |   0.01828 |   57.6       False\n| 29    |  0.01685 |   0.01901 |   59.6       False\n| 30    |  0.01673 |   0.01944 |   61.6       False\n| 31    |  0.01675 |   0.01724 |   63.5       True\n| 32    |  0.01666 |   0.01708 |   65.4       True\n| 33    |  0.01665 |   0.01749 |   67.7       False\n| 34    |  0.01660 |   0.01721 |   69.6       False\n| 35    |  0.01661 |   0.01729 |   72.1       False\n| 36    |  0.01669 |   0.01748 |   74.1       False\n| 37    |  0.01662 |   0.01716 |   76.1       False\n| 38    |  0.01650 |   0.01737 |   78.9       False\n| 39    |  0.01653 |   0.01775 |   81.0       False\n| 40    |  0.01646 |   0.01697 |   82.9       True\n| 41    |  0.01635 |   0.01746 |   84.8       False\n| 42    |  0.01640 |   0.01724 |   86.8       False\n| 43    |  0.01642 |   0.01738 |   89.0       False\n| 44    |  0.01640 |   0.01794 |   91.0       False\n| 45    |  0.01647 |   0.01705 |   92.9       False\n| 46    |  0.01637 |   0.01714 |   94.9       False\n| 47    |  0.01643 |   0.01750 |   96.9       False\n| 48    |  0.01643 |   0.01762 |   98.8       False\n| 49    |  0.01635 |   0.01705 |   100.9      False\n| 50    |  0.01627 |   0.01704 |   102.9      False\n| 51    |  0.01622 |   0.01706 |   104.8      False\n| 52    |  0.01638 |   0.01773 |   106.8      False\n| 53    |  0.01639 |   0.01689 |   108.8      True\n| 54    |  0.01614 |   0.01725 |   111.2      False\n| 55    |  0.01627 |   0.01725 |   113.5      False\n| 56    |  0.01632 |   0.01709 |   115.4      False\n| 57    |  0.01623 |   0.01716 |   117.4      False\n| 58    |  0.01630 |   0.01717 |   119.3      False\n| 59    |  0.01632 |   0.01703 |   121.2      False\n| 60    |  0.01625 |   0.01709 |   123.5      False\n| 61    |  0.01608 |   0.01678 |   125.3      True\n| 62    |  0.01613 |   0.01707 |   127.3      False\n| 63    |  0.01618 |   0.01713 |   129.3      False\n| 64    |  0.01623 |   0.01701 |   131.7      False\n| 65    |  0.01627 |   0.01736 |   134.0      False\n| 66    |  0.01612 |   0.01697 |   135.9      False\n| 67    |  0.01611 |   0.01687 |   138.1      False\n| 68    |  0.01623 |   0.01690 |   140.1      False\n| 69    |  0.01614 |   0.01710 |   142.1      False\n| 70    |  0.01612 |   0.01694 |   144.8      False\n| 71    |  0.01609 |   0.01694 |   146.8      False\n| 72    |  0.01618 |   0.01722 |   148.8      False\n| 73    |  0.01607 |   0.01685 |   150.7      False\n| 74    |  0.01619 |   0.01805 |   152.6      False\n| 75    |  0.01627 |   0.01701 |   154.7      False\n| 76    |  0.01615 |   0.01690 |   156.7      False\n| 77    |  0.01613 |   0.01679 |   158.7      False\n| 78    |  0.01610 |   0.01693 |   160.6      False\n| 79    |  0.01612 |   0.01695 |   162.5      False\n| 80    |  0.01608 |   0.01694 |   164.4      False\n| 81    |  0.01615 |   0.01688 |   166.6      False\n| 82    |  0.01613 |   0.01672 |   168.5      True\n| 83    |  0.01613 |   0.01689 |   170.5      False\n| 84    |  0.01609 |   0.01682 |   172.5      False\n| 85    |  0.01615 |   0.01701 |   174.4      False\n| 86    |  0.01605 |   0.01688 |   176.7      False\n| 87    |  0.01609 |   0.01694 |   178.9      False\n| 88    |  0.01607 |   0.01690 |   180.7      False\n| 89    |  0.01598 |   0.01686 |   182.6      False\n| 90    |  0.01616 |   0.01681 |   184.6      False\n| 91    |  0.01608 |   0.01695 |   186.6      False\n| 92    |  0.01609 |   0.01694 |   188.7      False\n| 93    |  0.01616 |   0.01708 |   190.9      False\n| 94    |  0.01623 |   0.01690 |   193.2      False\n| 95    |  0.01597 |   0.01690 |   195.1      False\n| 96    |  0.01599 |   0.01688 |   197.2      False\n| 97    |  0.01611 |   0.01688 |   199.3      False\n| 98    |  0.01618 |   0.01704 |   201.3      False\n| 99    |  0.01613 |   0.01690 |   203.3      False\n| 100   |  0.01617 |   0.01684 |   205.2      False\n| 101   |  0.01599 |   0.01691 |   207.2      False\n| 102   |  0.01588 |   0.01712 |   209.6      False\n| 103   |  0.01595 |   0.01690 |   211.7      False\n| 104   |  0.01591 |   0.01698 |   213.7      False\n| 105   |  0.01600 |   0.01687 |   215.7      False\n| 106   |  0.01596 |   0.01692 |   217.6      False\n| 107   |  0.01585 |   0.01688 |   219.5      False\n| 108   |  0.01598 |   0.01715 |   221.7      False\n| 109   |  0.01595 |   0.01964 |   223.6      False\n| 110   |  0.01611 |   0.01700 |   225.5      False\n| 111   |  0.01597 |   0.01696 |   227.6      False\n| 112   |  0.01587 |   0.01694 |   229.5      False\n| 113   |  0.01587 |   0.01672 |   231.4      False\n| 114   |  0.01587 |   0.01711 |   233.6      False\n| 115   |  0.01586 |   0.01710 |   235.5      False\n| 116   |  0.01584 |   0.01689 |   237.5      False\n| 117   |  0.01583 |   0.01693 |   239.5      False\n| 118   |  0.01578 |   0.01698 |   241.7      False\n| 119   |  0.01582 |   0.01707 |   243.9      False\n| 120   |  0.01589 |   0.01694 |   245.9      False\n| 121   |  0.01592 |   0.01709 |   247.8      False\n| 122   |  0.01585 |   0.01692 |   249.8      False\n| 123   |  0.01578 |   0.01740 |   252.4      False\n| 124   |  0.01590 |   0.01717 |   254.6      False\n| 125   |  0.01590 |   0.01691 |   256.6      False\n| 126   |  0.01590 |   0.01685 |   258.7      False\n| 127   |  0.01586 |   0.01722 |   260.6      False\n| 128   |  0.01582 |   0.01843 |   262.6      False\n| 129   |  0.01627 |   0.01704 |   264.7      False\n| 130   |  0.01606 |   0.01681 |   266.7      False\n| 131   |  0.01583 |   0.01683 |   268.7      False\n| 132   |  0.01581 |   0.01682 |   270.7      False\nEarly stopping occured at epoch 132\nTraining done in 270.689 seconds.\n---------------------------------------\nSuccessfully saved model at tabnet_raw_step1_fold3_0.zip\nvalidation fold 3 : 0.016715327434036066\nFOLDS :  4\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","name":"stderr"},{"output_type":"stream","text":"| 1     |  0.36613 |   0.03563 |   2.0        True\n| 2     |  0.02776 |   0.02472 |   4.2        True\n| 3     |  0.02230 |   0.02131 |   6.2        True\n| 4     |  0.02107 |   0.02080 |   8.1        True\n| 5     |  0.02061 |   0.02051 |   10.0       True\n| 6     |  0.02037 |   0.02062 |   12.0       False\n| 7     |  0.02004 |   0.02010 |   14.1       True\n| 8     |  0.01954 |   0.02004 |   16.0       True\n| 9     |  0.01910 |   0.01933 |   18.0       True\n| 10    |  0.01870 |   0.02040 |   19.9       False\n| 11    |  0.01844 |   0.01969 |   21.8       False\n| 12    |  0.01814 |   0.01872 |   23.8       True\n| 13    |  0.01795 |   0.02041 |   26.0       False\n| 14    |  0.01785 |   0.02085 |   27.9       False\n| 15    |  0.01776 |   0.01892 |   29.9       False\n| 16    |  0.01754 |   0.01895 |   32.2       False\n| 17    |  0.01751 |   0.01896 |   34.2       False\n| 18    |  0.01742 |   0.02002 |   36.3       False\n| 19    |  0.01740 |   0.01829 |   38.5       True\n| 20    |  0.01733 |   0.02012 |   40.6       False\n| 21    |  0.01721 |   0.01848 |   42.7       False\n| 22    |  0.01719 |   0.01875 |   44.7       False\n| 23    |  0.01714 |   0.02153 |   47.0       False\n| 24    |  0.01703 |   0.01841 |   49.0       False\n| 25    |  0.01699 |   0.02029 |   50.9       False\n| 26    |  0.01695 |   0.01862 |   52.8       False\n| 27    |  0.01691 |   0.01846 |   54.7       False\n| 28    |  0.01678 |   0.02088 |   56.6       False\n| 29    |  0.01688 |   0.01843 |   58.7       False\n| 30    |  0.01682 |   0.02075 |   60.7       False\n| 31    |  0.01678 |   0.01845 |   62.8       False\n| 32    |  0.01673 |   0.01920 |   64.8       False\n| 33    |  0.01667 |   0.01743 |   66.7       True\n| 34    |  0.01664 |   0.01749 |   68.8       False\n| 35    |  0.01662 |   0.01758 |   70.8       False\n| 36    |  0.01673 |   0.01854 |   72.6       False\n| 37    |  0.01664 |   0.01793 |   74.6       False\n| 38    |  0.01654 |   0.01800 |   76.5       False\n| 39    |  0.01647 |   0.01755 |   78.3       False\n| 40    |  0.01642 |   0.01729 |   80.4       True\n| 41    |  0.01638 |   0.01740 |   82.4       False\n| 42    |  0.01641 |   0.01746 |   84.2       False\n| 43    |  0.01644 |   0.01730 |   86.0       False\n| 44    |  0.01633 |   0.01707 |   88.0       True\n| 45    |  0.01638 |   0.01722 |   89.8       False\n| 46    |  0.01624 |   0.01795 |   91.9       False\n| 47    |  0.01637 |   0.01721 |   93.9       False\n| 48    |  0.01630 |   0.01692 |   96.1       True\n| 49    |  0.01622 |   0.01709 |   98.3       False\n| 50    |  0.01624 |   0.01733 |   100.2      False\n| 51    |  0.01638 |   0.01732 |   102.7      False\n| 52    |  0.01628 |   0.01731 |   104.7      False\n| 53    |  0.01627 |   0.01925 |   106.7      False\n| 54    |  0.01624 |   0.01740 |   108.7      False\n| 55    |  0.01628 |   0.01721 |   110.6      False\n| 56    |  0.01626 |   0.01698 |   112.6      False\n| 57    |  0.01607 |   0.01727 |   114.6      False\n| 58    |  0.01609 |   0.01725 |   116.6      False\n| 59    |  0.01630 |   0.01720 |   118.6      False\n| 60    |  0.01624 |   0.01717 |   120.5      False\n| 61    |  0.01625 |   0.01720 |   122.5      False\n| 62    |  0.01614 |   0.01714 |   124.7      False\n| 63    |  0.01616 |   0.01746 |   127.0      False\n| 64    |  0.01623 |   0.01710 |   129.0      False\n| 65    |  0.01625 |   0.01722 |   130.9      False\n| 66    |  0.01611 |   0.01713 |   132.8      False\n| 67    |  0.01611 |   0.01696 |   135.1      False\n| 68    |  0.01625 |   0.01720 |   137.2      False\n| 69    |  0.01621 |   0.01701 |   139.2      False\n| 70    |  0.01611 |   0.01723 |   141.3      False\n| 71    |  0.01608 |   0.01689 |   143.3      True\n| 72    |  0.01608 |   0.01706 |   145.3      False\n| 73    |  0.01609 |   0.01725 |   147.5      False\n| 74    |  0.01608 |   0.01704 |   149.4      False\n| 75    |  0.01611 |   0.01712 |   151.6      False\n| 76    |  0.01604 |   0.01702 |   153.5      False\n| 77    |  0.01614 |   0.01693 |   155.4      False\n| 78    |  0.01612 |   0.01709 |   157.9      False\n| 79    |  0.01612 |   0.01700 |   160.9      False\n| 80    |  0.01613 |   0.01708 |   162.9      False\n| 81    |  0.01606 |   0.01705 |   164.9      False\n| 82    |  0.01608 |   0.01706 |   167.1      False\n| 83    |  0.01615 |   0.01692 |   169.4      False\n| 84    |  0.01610 |   0.01739 |   171.5      False\n| 85    |  0.01609 |   0.01701 |   173.5      False\n| 86    |  0.01610 |   0.01697 |   175.4      False\n| 87    |  0.01608 |   0.01706 |   177.4      False\n| 88    |  0.01612 |   0.01702 |   179.6      False\n| 89    |  0.01610 |   0.01693 |   181.5      False\n| 90    |  0.01600 |   0.01702 |   183.6      False\n| 91    |  0.01610 |   0.01701 |   185.5      False\n| 92    |  0.01607 |   0.01688 |   187.5      True\n| 93    |  0.01601 |   0.01696 |   189.7      False\n| 94    |  0.01606 |   0.01692 |   191.9      False\n| 95    |  0.01615 |   0.01710 |   194.1      False\n| 96    |  0.01612 |   0.01739 |   196.1      False\n| 97    |  0.01610 |   0.01696 |   198.0      False\n| 98    |  0.01612 |   0.01695 |   199.9      False\n| 99    |  0.01603 |   0.01701 |   202.1      False\n| 100   |  0.01604 |   0.01698 |   204.0      False\n| 101   |  0.01603 |   0.01706 |   205.9      False\n| 102   |  0.01600 |   0.01689 |   208.0      False\n| 103   |  0.01584 |   0.01699 |   209.9      False\n| 104   |  0.01603 |   0.01699 |   212.0      False\n| 105   |  0.01594 |   0.01689 |   214.0      False\n| 106   |  0.01586 |   0.01701 |   215.9      False\n| 107   |  0.01603 |   0.01751 |   218.3      False\n| 108   |  0.01613 |   0.01688 |   220.4      True\n| 109   |  0.01597 |   0.01716 |   222.4      False\n| 110   |  0.01590 |   0.01677 |   225.5      True\n| 111   |  0.01584 |   0.01695 |   227.5      False\n| 112   |  0.01592 |   0.01698 |   229.5      False\n| 113   |  0.01590 |   0.01689 |   231.4      False\n| 114   |  0.01585 |   0.01696 |   233.6      False\n| 115   |  0.01589 |   0.01700 |   235.6      False\n| 116   |  0.01581 |   0.01685 |   237.5      False\n| 117   |  0.01584 |   0.01681 |   239.5      False\n| 118   |  0.01586 |   0.01691 |   241.4      False\n| 119   |  0.01595 |   0.01701 |   243.4      False\n| 120   |  0.01597 |   0.01685 |   245.5      False\n| 121   |  0.01588 |   0.01680 |   247.5      False\n| 122   |  0.01581 |   0.01694 |   249.4      False\n| 123   |  0.01577 |   0.01687 |   251.3      False\n| 124   |  0.01581 |   0.01680 |   253.4      False\n| 125   |  0.01578 |   0.01688 |   255.4      False\n| 126   |  0.01577 |   0.01692 |   257.9      False\n| 127   |  0.01581 |   0.01712 |   259.9      False\n| 128   |  0.01586 |   0.01704 |   261.9      False\n| 129   |  0.01581 |   0.01685 |   263.8      False\n| 130   |  0.01580 |   0.01685 |   265.7      False\n| 131   |  0.01579 |   0.01733 |   267.9      False\n| 132   |  0.01592 |   0.01706 |   269.8      False\n| 133   |  0.01583 |   0.01692 |   271.7      False\n| 134   |  0.01582 |   0.01679 |   273.8      False\n| 135   |  0.01579 |   0.01703 |   275.7      False\n| 136   |  0.01582 |   0.01692 |   278.4      False\n| 137   |  0.01578 |   0.01733 |   280.7      False\n| 138   |  0.01593 |   0.01717 |   282.8      False\n| 139   |  0.01576 |   0.01698 |   284.8      False\n| 140   |  0.01576 |   0.01698 |   286.9      False\n| 141   |  0.01582 |   0.01689 |   289.7      False\n| 142   |  0.01581 |   0.01682 |   291.7      False\n| 143   |  0.01584 |   0.01701 |   293.6      False\n| 144   |  0.01589 |   0.01713 |   295.7      False\n| 145   |  0.01587 |   0.01715 |   297.6      False\n| 146   |  0.01576 |   0.01671 |   299.7      True\n| 147   |  0.01585 |   0.01711 |   301.7      False\n| 148   |  0.01590 |   0.01703 |   303.8      False\n| 149   |  0.01573 |   0.01704 |   305.8      False\n| 150   |  0.01578 |   0.01710 |   307.7      False\n| 151   |  0.01576 |   0.01706 |   309.8      False\n| 152   |  0.01567 |   0.01692 |   312.0      False\n| 153   |  0.01560 |   0.01699 |   314.0      False\n| 154   |  0.01568 |   0.01699 |   315.9      False\n| 155   |  0.01572 |   0.01694 |   317.9      False\n| 156   |  0.01560 |   0.01689 |   319.9      False\n| 157   |  0.01561 |   0.01685 |   322.4      False\n| 158   |  0.01560 |   0.01685 |   324.3      False\n","name":"stdout"},{"output_type":"stream","text":"| 159   |  0.01561 |   0.01721 |   326.3      False\n| 160   |  0.01584 |   0.01698 |   328.2      False\n| 161   |  0.01574 |   0.01678 |   330.2      False\n| 162   |  0.01560 |   0.01707 |   332.1      False\n| 163   |  0.01562 |   0.01694 |   334.4      False\n| 164   |  0.01557 |   0.01704 |   336.3      False\n| 165   |  0.01566 |   0.01703 |   338.7      False\n| 166   |  0.01564 |   0.01687 |   340.7      False\n| 167   |  0.01568 |   0.01701 |   342.7      False\n| 168   |  0.01572 |   0.01681 |   345.1      False\n| 169   |  0.01557 |   0.01707 |   347.2      False\n| 170   |  0.01557 |   0.01688 |   349.1      False\n| 171   |  0.01554 |   0.01677 |   351.1      False\n| 172   |  0.01556 |   0.01700 |   353.2      False\n| 173   |  0.01562 |   0.01687 |   355.7      False\n| 174   |  0.01557 |   0.01692 |   357.6      False\n| 175   |  0.01561 |   0.01686 |   359.7      False\n| 176   |  0.01557 |   0.01727 |   361.6      False\n| 177   |  0.01562 |   0.01699 |   363.6      False\n| 178   |  0.01552 |   0.01706 |   365.8      False\n| 179   |  0.01563 |   0.01692 |   367.9      False\n| 180   |  0.01568 |   0.01704 |   369.9      False\n| 181   |  0.01558 |   0.01689 |   371.9      False\n| 182   |  0.01558 |   0.01698 |   373.8      False\n| 183   |  0.01567 |   0.01699 |   375.8      False\n| 184   |  0.01555 |   0.01700 |   378.1      False\n| 185   |  0.01563 |   0.01725 |   380.1      False\n| 186   |  0.01558 |   0.01688 |   382.0      False\n| 187   |  0.01558 |   0.01745 |   384.1      False\n| 188   |  0.01567 |   0.01681 |   386.4      False\n| 189   |  0.01554 |   0.01700 |   388.6      False\n| 190   |  0.01561 |   0.01686 |   390.6      False\n| 191   |  0.01552 |   0.01689 |   392.6      False\n| 192   |  0.01552 |   0.01714 |   394.5      False\n| 193   |  0.01551 |   0.01700 |   396.5      False\n| 194   |  0.01558 |   0.01701 |   399.3      False\n| 195   |  0.01556 |   0.01684 |   401.3      False\n| 196   |  0.01547 |   0.01702 |   403.4      False\nEarly stopping occured at epoch 196\nTraining done in 403.390 seconds.\n---------------------------------------\nSuccessfully saved model at tabnet_raw_step1_fold4_0.zip\nvalidation fold 4 : 0.016713403525888848\nFOLDS :  5\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","name":"stderr"},{"output_type":"stream","text":"| 1     |  0.37090 |   0.04070 |   2.0        True\n| 2     |  0.02835 |   0.02652 |   4.0        True\n| 3     |  0.02288 |   0.02142 |   6.2        True\n| 4     |  0.02120 |   0.02076 |   8.2        True\n| 5     |  0.02075 |   0.02054 |   10.1       True\n| 6     |  0.02048 |   0.02026 |   12.5       True\n| 7     |  0.02026 |   0.02011 |   14.5       True\n| 8     |  0.01996 |   0.01983 |   16.7       True\n| 9     |  0.01956 |   0.02061 |   18.8       False\n| 10    |  0.01922 |   0.02041 |   20.8       False\n| 11    |  0.01888 |   0.01901 |   22.7       True\n| 12    |  0.01849 |   0.02036 |   24.6       False\n| 13    |  0.01827 |   0.01881 |   26.8       True\n| 14    |  0.01803 |   0.01831 |   28.8       True\n| 15    |  0.01781 |   0.01860 |   30.7       False\n| 16    |  0.01779 |   0.01853 |   32.7       False\n| 17    |  0.01761 |   0.01851 |   34.7       False\n| 18    |  0.01746 |   0.01898 |   36.6       False\n| 19    |  0.01729 |   0.01764 |   38.8       True\n| 20    |  0.01726 |   0.01789 |   40.8       False\n| 21    |  0.01729 |   0.01757 |   42.8       True\n| 22    |  0.01714 |   0.01743 |   45.1       True\n| 23    |  0.01706 |   0.01754 |   47.2       False\n| 24    |  0.01703 |   0.01723 |   49.3       True\n| 25    |  0.01689 |   0.01764 |   51.3       False\n| 26    |  0.01687 |   0.01761 |   53.7       False\n| 27    |  0.01684 |   0.01731 |   55.7       False\n| 28    |  0.01682 |   0.01841 |   57.9       False\n| 29    |  0.01685 |   0.02000 |   60.2       False\n| 30    |  0.01672 |   0.01728 |   62.2       False\n| 31    |  0.01663 |   0.01721 |   64.2       True\n| 32    |  0.01662 |   0.01739 |   66.2       False\n| 33    |  0.01658 |   0.01831 |   68.1       False\n| 34    |  0.01647 |   0.01721 |   70.3       False\n| 35    |  0.01654 |   0.01727 |   72.2       False\n| 36    |  0.01644 |   0.01749 |   74.1       False\n| 37    |  0.01657 |   0.01711 |   76.1       True\n| 38    |  0.01650 |   0.01730 |   78.3       False\n| 39    |  0.01637 |   0.01716 |   80.2       False\n| 40    |  0.01635 |   0.01720 |   82.4       False\n| 41    |  0.01638 |   0.01675 |   84.3       True\n| 42    |  0.01649 |   0.01717 |   86.2       False\n| 43    |  0.01627 |   0.01707 |   88.2       False\n| 44    |  0.01638 |   0.01730 |   90.1       False\n| 45    |  0.01631 |   0.01693 |   92.2       False\n| 46    |  0.01615 |   0.01708 |   94.2       False\n| 47    |  0.01624 |   0.01692 |   96.1       False\n| 48    |  0.01616 |   0.01697 |   98.1       False\n| 49    |  0.01623 |   0.01719 |   100.0      False\n| 50    |  0.01613 |   0.01792 |   101.9      False\n| 51    |  0.01626 |   0.01695 |   104.0      False\n| 52    |  0.01612 |   0.01683 |   105.9      False\n| 53    |  0.01610 |   0.01684 |   108.0      False\n| 54    |  0.01619 |   0.01672 |   110.2      True\n| 55    |  0.01609 |   0.01681 |   112.5      False\n| 56    |  0.01606 |   0.01682 |   114.7      False\n| 57    |  0.01606 |   0.01656 |   117.1      True\n| 58    |  0.01602 |   0.01707 |   119.0      False\n| 59    |  0.01608 |   0.01682 |   121.0      False\n| 60    |  0.01601 |   0.01702 |   123.0      False\n| 61    |  0.01603 |   0.01685 |   125.1      False\n| 62    |  0.01613 |   0.01660 |   127.1      False\n| 63    |  0.01597 |   0.01712 |   129.0      False\n| 64    |  0.01598 |   0.01690 |   131.1      False\n| 65    |  0.01600 |   0.01688 |   133.0      False\n| 66    |  0.01603 |   0.01687 |   134.8      False\n| 67    |  0.01603 |   0.01684 |   136.9      False\n| 68    |  0.01604 |   0.01673 |   139.0      False\n| 69    |  0.01595 |   0.01665 |   141.2      False\n| 70    |  0.01594 |   0.01694 |   143.2      False\n| 71    |  0.01594 |   0.01669 |   145.1      False\n| 72    |  0.01597 |   0.01757 |   147.4      False\n| 73    |  0.01603 |   0.01681 |   149.4      False\n| 74    |  0.01585 |   0.01685 |   151.3      False\n| 75    |  0.01595 |   0.01666 |   153.3      False\n| 76    |  0.01601 |   0.01679 |   155.2      False\n| 77    |  0.01597 |   0.01683 |   157.1      False\n| 78    |  0.01603 |   0.01673 |   159.4      False\n| 79    |  0.01604 |   0.01684 |   161.3      False\n| 80    |  0.01607 |   0.01686 |   163.2      False\n| 81    |  0.01604 |   0.01662 |   165.3      False\n| 82    |  0.01585 |   0.01695 |   167.2      False\n| 83    |  0.01584 |   0.01707 |   169.4      False\n| 84    |  0.01608 |   0.01693 |   171.4      False\n| 85    |  0.01596 |   0.01692 |   174.2      False\n| 86    |  0.01592 |   0.01678 |   176.3      False\n| 87    |  0.01592 |   0.01699 |   178.4      False\n| 88    |  0.01615 |   0.01698 |   180.7      False\n| 89    |  0.01602 |   0.01702 |   182.7      False\n| 90    |  0.01592 |   0.01678 |   184.7      False\n| 91    |  0.01588 |   0.01673 |   186.7      False\n| 92    |  0.01590 |   0.01676 |   188.6      False\n| 93    |  0.01596 |   0.01677 |   190.7      False\n| 94    |  0.01596 |   0.01658 |   192.8      False\n| 95    |  0.01595 |   0.01677 |   194.8      False\n| 96    |  0.01582 |   0.01666 |   196.8      False\n| 97    |  0.01591 |   0.01679 |   198.7      False\n| 98    |  0.01597 |   0.01668 |   200.7      False\n| 99    |  0.01583 |   0.01673 |   202.9      False\n| 100   |  0.01593 |   0.01741 |   205.0      False\n| 101   |  0.01597 |   0.01674 |   207.1      False\n| 102   |  0.01580 |   0.01744 |   209.1      False\n| 103   |  0.01585 |   0.01728 |   211.0      False\n| 104   |  0.01598 |   0.01680 |   213.1      False\n| 105   |  0.01584 |   0.01685 |   215.2      False\n| 106   |  0.01581 |   0.01652 |   217.1      True\n| 107   |  0.01574 |   0.01749 |   219.1      False\n| 108   |  0.01581 |   0.01669 |   221.1      False\n| 109   |  0.01575 |   0.01673 |   223.1      False\n| 110   |  0.01576 |   0.01681 |   225.3      False\n| 111   |  0.01577 |   0.01668 |   227.3      False\n| 112   |  0.01573 |   0.01666 |   229.2      False\n| 113   |  0.01577 |   0.01683 |   231.2      False\n| 114   |  0.01571 |   0.01690 |   233.6      False\n| 115   |  0.01569 |   0.01667 |   235.8      False\n| 116   |  0.01579 |   0.01694 |   238.4      False\n| 117   |  0.01580 |   0.01677 |   240.4      False\n| 118   |  0.01571 |   0.01693 |   242.4      False\n| 119   |  0.01584 |   0.01659 |   244.4      False\n| 120   |  0.01582 |   0.01706 |   246.6      False\n| 121   |  0.01573 |   0.01665 |   248.5      False\n| 122   |  0.01573 |   0.01672 |   250.5      False\n| 123   |  0.01568 |   0.01684 |   252.4      False\n| 124   |  0.01579 |   0.01683 |   254.3      False\n| 125   |  0.01573 |   0.01668 |   256.3      False\n| 126   |  0.01576 |   0.01670 |   258.5      False\n| 127   |  0.01571 |   0.01679 |   260.4      False\n| 128   |  0.01575 |   0.01668 |   262.5      False\n| 129   |  0.01572 |   0.01689 |   264.4      False\n| 130   |  0.01584 |   0.01675 |   266.3      False\n| 131   |  0.01568 |   0.01661 |   268.6      False\n| 132   |  0.01575 |   0.01661 |   270.9      False\n| 133   |  0.01564 |   0.01664 |   272.8      False\n| 134   |  0.01571 |   0.01671 |   274.8      False\n| 135   |  0.01563 |   0.01692 |   276.7      False\n| 136   |  0.01573 |   0.01692 |   278.7      False\n| 137   |  0.01571 |   0.01677 |   280.9      False\n| 138   |  0.01561 |   0.01660 |   282.8      False\n| 139   |  0.01562 |   0.01672 |   284.8      False\n| 140   |  0.01572 |   0.01685 |   286.8      False\n| 141   |  0.01579 |   0.01667 |   288.8      False\n| 142   |  0.01576 |   0.01750 |   291.0      False\n| 143   |  0.01568 |   0.01671 |   293.8      False\n| 144   |  0.01561 |   0.01724 |   295.8      False\n| 145   |  0.01561 |   0.01688 |   297.9      False\n| 146   |  0.01564 |   0.01668 |   300.0      False\n| 147   |  0.01568 |   0.01694 |   302.7      False\n| 148   |  0.01582 |   0.01661 |   304.7      False\n| 149   |  0.01573 |   0.01679 |   306.7      False\n| 150   |  0.01572 |   0.01655 |   308.7      False\n| 151   |  0.01564 |   0.01681 |   310.6      False\n| 152   |  0.01557 |   0.01657 |   312.9      False\n| 153   |  0.01547 |   0.01673 |   314.8      False\n| 154   |  0.01554 |   0.01661 |   316.8      False\n| 155   |  0.01556 |   0.01663 |   318.8      False\n| 156   |  0.01551 |   0.01656 |   320.7      False\nEarly stopping occured at epoch 156\nTraining done in 320.741 seconds.\n---------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"Successfully saved model at tabnet_raw_step1_fold5_0.zip\nvalidation fold 5 : 0.016518833799495643\nFOLDS :  6\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","name":"stderr"},{"output_type":"stream","text":"| 1     |  0.36926 |   0.03842 |   2.1        True\n| 2     |  0.02753 |   0.02527 |   4.1        True\n| 3     |  0.02221 |   0.02119 |   6.1        True\n| 4     |  0.02111 |   0.02072 |   8.1        True\n| 5     |  0.02066 |   0.02063 |   10.1       True\n| 6     |  0.02033 |   0.02030 |   12.8       True\n| 7     |  0.02003 |   0.01996 |   14.7       True\n| 8     |  0.01961 |   0.01977 |   16.8       True\n| 9     |  0.01916 |   0.01954 |   18.7       True\n| 10    |  0.01875 |   0.01894 |   20.7       True\n| 11    |  0.01845 |   0.01922 |   23.0       False\n| 12    |  0.01819 |   0.01997 |   25.1       False\n| 13    |  0.01798 |   0.01950 |   27.0       False\n| 14    |  0.01790 |   0.01857 |   29.2       True\n| 15    |  0.01767 |   0.01809 |   31.4       True\n| 16    |  0.01759 |   0.01849 |   33.6       False\n| 17    |  0.01750 |   0.02077 |   35.9       False\n| 18    |  0.01736 |   0.01869 |   37.9       False\n| 19    |  0.01732 |   0.01790 |   39.9       True\n| 20    |  0.01737 |   0.01818 |   42.0       False\n| 21    |  0.01713 |   0.01914 |   44.4       False\n| 22    |  0.01714 |   0.01803 |   46.6       False\n| 23    |  0.01712 |   0.01980 |   48.5       False\n| 24    |  0.01712 |   0.01789 |   50.6       True\n| 25    |  0.01694 |   0.01994 |   52.5       False\n| 26    |  0.01690 |   0.01795 |   54.4       False\n| 27    |  0.01692 |   0.01785 |   56.7       True\n| 28    |  0.01695 |   0.01771 |   58.7       True\n| 29    |  0.01684 |   0.01715 |   60.6       True\n| 30    |  0.01664 |   0.01716 |   62.7       False\n| 31    |  0.01685 |   0.01761 |   64.6       False\n| 32    |  0.01672 |   0.01980 |   66.8       False\n| 33    |  0.01696 |   0.01879 |   68.7       False\n| 34    |  0.01665 |   0.01706 |   70.7       True\n| 35    |  0.01672 |   0.01743 |   72.7       False\n| 36    |  0.01669 |   0.01708 |   74.6       False\n| 37    |  0.01660 |   0.01724 |   76.7       False\n| 38    |  0.01646 |   0.01710 |   79.3       False\n| 39    |  0.01649 |   0.01730 |   81.2       False\n| 40    |  0.01658 |   0.01703 |   83.1       True\n| 41    |  0.01649 |   0.01708 |   85.2       False\n| 42    |  0.01651 |   0.01694 |   87.2       True\n| 43    |  0.01658 |   0.01701 |   89.8       False\n| 44    |  0.01645 |   0.01731 |   92.0       False\n| 45    |  0.01633 |   0.01705 |   94.0       False\n| 46    |  0.01638 |   0.01725 |   96.1       False\n| 47    |  0.01637 |   0.01701 |   98.1       False\n| 48    |  0.01639 |   0.01686 |   100.3      True\n| 49    |  0.01638 |   0.01680 |   102.3      True\n| 50    |  0.01635 |   0.01683 |   104.2      False\n| 51    |  0.01644 |   0.01716 |   106.2      False\n| 52    |  0.01637 |   0.01698 |   108.2      False\n| 53    |  0.01636 |   0.01697 |   110.8      False\n| 54    |  0.01627 |   0.01712 |   112.7      False\n| 55    |  0.01631 |   0.01757 |   114.7      False\n| 56    |  0.01635 |   0.01690 |   116.7      False\n| 57    |  0.01630 |   0.01692 |   118.6      False\n| 58    |  0.01625 |   0.01726 |   120.6      False\n| 59    |  0.01628 |   0.01707 |   122.8      False\n| 60    |  0.01629 |   0.01683 |   124.8      False\n| 61    |  0.01609 |   0.01704 |   126.7      False\n| 62    |  0.01634 |   0.01692 |   128.7      False\n| 63    |  0.01624 |   0.01722 |   130.7      False\n| 64    |  0.01621 |   0.01683 |   132.9      False\n| 65    |  0.01614 |   0.01687 |   134.8      False\n| 66    |  0.01617 |   0.01698 |   136.8      False\n| 67    |  0.01619 |   0.01691 |   138.7      False\n| 68    |  0.01612 |   0.01695 |   140.8      False\n| 69    |  0.01626 |   0.01713 |   143.3      False\n| 70    |  0.01632 |   0.01675 |   145.4      True\n| 71    |  0.01623 |   0.01701 |   147.3      False\n| 72    |  0.01614 |   0.01679 |   149.7      False\n| 73    |  0.01611 |   0.01689 |   151.9      False\n| 74    |  0.01609 |   0.01704 |   154.0      False\n| 75    |  0.01625 |   0.01682 |   156.2      False\n| 76    |  0.01613 |   0.01696 |   158.2      False\n| 77    |  0.01618 |   0.01690 |   160.3      False\n| 78    |  0.01619 |   0.01719 |   162.2      False\n| 79    |  0.01616 |   0.01676 |   164.2      False\n| 80    |  0.01606 |   0.01672 |   166.3      True\n| 81    |  0.01612 |   0.01688 |   168.3      False\n| 82    |  0.01612 |   0.01741 |   170.3      False\n| 83    |  0.01623 |   0.01698 |   172.2      False\n| 84    |  0.01630 |   0.01667 |   174.6      True\n| 85    |  0.01612 |   0.01685 |   176.8      False\n| 86    |  0.01605 |   0.01689 |   178.8      False\n| 87    |  0.01615 |   0.01679 |   180.8      False\n| 88    |  0.01613 |   0.01706 |   182.8      False\n| 89    |  0.01617 |   0.01666 |   184.7      True\n| 90    |  0.01610 |   0.01691 |   186.7      False\n| 91    |  0.01613 |   0.01677 |   188.9      False\n| 92    |  0.01609 |   0.01676 |   190.8      False\n| 93    |  0.01602 |   0.01666 |   192.7      False\n| 94    |  0.01607 |   0.01682 |   194.7      False\n| 95    |  0.01619 |   0.01714 |   196.7      False\n| 96    |  0.01616 |   0.01678 |   198.8      False\n| 97    |  0.01606 |   0.01671 |   200.8      False\n| 98    |  0.01603 |   0.01676 |   202.8      False\n| 99    |  0.01605 |   0.01685 |   204.8      False\n| 100   |  0.01615 |   0.01689 |   207.0      False\n| 101   |  0.01609 |   0.01699 |   209.7      False\n| 102   |  0.01596 |   0.01678 |   211.7      False\n| 103   |  0.01596 |   0.01724 |   213.8      False\n| 104   |  0.01594 |   0.01677 |   216.1      False\n| 105   |  0.01595 |   0.01683 |   218.1      False\n| 106   |  0.01590 |   0.01684 |   220.4      False\n| 107   |  0.01587 |   0.01686 |   222.4      False\n| 108   |  0.01589 |   0.01683 |   224.4      False\n| 109   |  0.01588 |   0.01710 |   226.4      False\n| 110   |  0.01601 |   0.01686 |   228.3      False\n| 111   |  0.01584 |   0.01697 |   230.4      False\n| 112   |  0.01591 |   0.01678 |   232.5      False\n| 113   |  0.01585 |   0.01680 |   234.4      False\n| 114   |  0.01587 |   0.01682 |   236.5      False\n| 115   |  0.01575 |   0.01666 |   238.7      False\n| 116   |  0.01593 |   0.01680 |   240.6      False\n| 117   |  0.01588 |   0.01676 |   242.9      False\n| 118   |  0.01587 |   0.01680 |   244.8      False\n| 119   |  0.01584 |   0.01688 |   246.8      False\n| 120   |  0.01590 |   0.01713 |   248.7      False\n| 121   |  0.01584 |   0.01668 |   250.7      False\n| 122   |  0.01582 |   0.01691 |   252.7      False\n| 123   |  0.01597 |   0.01719 |   254.9      False\n| 124   |  0.01583 |   0.01684 |   256.9      False\n| 125   |  0.01582 |   0.01677 |   258.8      False\n| 126   |  0.01578 |   0.01691 |   260.8      False\n| 127   |  0.01590 |   0.01682 |   262.8      False\n| 128   |  0.01587 |   0.01710 |   265.0      False\n| 129   |  0.01582 |   0.01689 |   266.9      False\n| 130   |  0.01576 |   0.01690 |   269.5      False\n| 131   |  0.01582 |   0.01709 |   272.1      False\n| 132   |  0.01594 |   0.01710 |   274.2      False\n| 133   |  0.01598 |   0.01677 |   276.6      False\n| 134   |  0.01590 |   0.01704 |   278.7      False\n| 135   |  0.01584 |   0.01682 |   280.7      False\n| 136   |  0.01579 |   0.01697 |   282.6      False\n| 137   |  0.01580 |   0.01682 |   284.7      False\n| 138   |  0.01577 |   0.01678 |   286.8      False\n| 139   |  0.01588 |   0.01666 |   288.8      True\n| 140   |  0.01584 |   0.01681 |   290.7      False\n| 141   |  0.01587 |   0.01663 |   292.8      True\n| 142   |  0.01580 |   0.01689 |   294.7      False\n| 143   |  0.01585 |   0.01696 |   296.7      False\n| 144   |  0.01587 |   0.01682 |   299.0      False\n| 145   |  0.01582 |   0.01674 |   300.9      False\n| 146   |  0.01584 |   0.01688 |   303.2      False\n| 147   |  0.01573 |   0.01688 |   305.1      False\n| 148   |  0.01579 |   0.01692 |   307.2      False\n| 149   |  0.01583 |   0.01681 |   309.3      False\n| 150   |  0.01578 |   0.01675 |   311.3      False\n| 151   |  0.01565 |   0.01713 |   313.3      False\n| 152   |  0.01568 |   0.01717 |   315.3      False\n| 153   |  0.01565 |   0.01670 |   317.2      False\n| 154   |  0.01564 |   0.01721 |   319.4      False\n| 155   |  0.01569 |   0.01690 |   321.4      False\n| 156   |  0.01570 |   0.01690 |   323.4      False\n| 157   |  0.01565 |   0.01674 |   325.5      False\n| 158   |  0.01556 |   0.01683 |   327.4      False\n| 159   |  0.01567 |   0.01670 |   329.9      False\n","name":"stdout"},{"output_type":"stream","text":"| 160   |  0.01558 |   0.01729 |   332.4      False\n| 161   |  0.01566 |   0.01760 |   334.7      False\n| 162   |  0.01573 |   0.01716 |   336.7      False\n| 163   |  0.01568 |   0.01694 |   338.8      False\n| 164   |  0.01561 |   0.01673 |   340.9      False\n| 165   |  0.01560 |   0.01684 |   343.0      False\n| 166   |  0.01562 |   0.01683 |   345.1      False\n| 167   |  0.01564 |   0.01688 |   347.1      False\n| 168   |  0.01563 |   0.01684 |   349.0      False\n| 169   |  0.01563 |   0.01689 |   351.0      False\n| 170   |  0.01566 |   0.01726 |   353.2      False\n| 171   |  0.01563 |   0.01701 |   355.2      False\n| 172   |  0.01555 |   0.01715 |   357.2      False\n| 173   |  0.01554 |   0.01680 |   359.2      False\n| 174   |  0.01559 |   0.01681 |   361.1      False\n| 175   |  0.01559 |   0.01666 |   363.3      False\n| 176   |  0.01558 |   0.01686 |   365.3      False\n| 177   |  0.01564 |   0.01674 |   367.6      False\n| 178   |  0.01551 |   0.01698 |   369.7      False\n| 179   |  0.01553 |   0.01677 |   371.6      False\n| 180   |  0.01554 |   0.01671 |   373.5      False\n| 181   |  0.01557 |   0.01694 |   375.8      False\n| 182   |  0.01563 |   0.01668 |   377.7      False\n| 183   |  0.01545 |   0.01683 |   379.7      False\n| 184   |  0.01545 |   0.01682 |   381.7      False\n| 185   |  0.01547 |   0.01690 |   383.7      False\n| 186   |  0.01555 |   0.01677 |   385.9      False\n| 187   |  0.01552 |   0.01719 |   388.0      False\n| 188   |  0.01560 |   0.01768 |   390.4      False\n| 189   |  0.01566 |   0.01700 |   392.6      False\n| 190   |  0.01567 |   0.01681 |   394.6      False\n| 191   |  0.01555 |   0.01678 |   396.9      False\nEarly stopping occured at epoch 191\nTraining done in 396.860 seconds.\n---------------------------------------\nSuccessfully saved model at tabnet_raw_step1_fold6_0.zip\nvalidation fold 6 : 0.016631023900400287\nFOLDS :  7\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","name":"stderr"},{"output_type":"stream","text":"| 1     |  0.37010 |   0.03648 |   2.0        True\n| 2     |  0.02788 |   0.02499 |   4.0        True\n| 3     |  0.02230 |   0.02162 |   5.9        True\n| 4     |  0.02105 |   0.02098 |   8.1        True\n| 5     |  0.02063 |   0.02067 |   10.1       True\n| 6     |  0.02039 |   0.02045 |   12.1       True\n| 7     |  0.02013 |   0.02022 |   14.0       True\n| 8     |  0.01966 |   0.01983 |   16.0       True\n| 9     |  0.01915 |   0.01943 |   17.9       True\n| 10    |  0.01875 |   0.02020 |   20.1       False\n| 11    |  0.01853 |   0.01934 |   22.1       True\n| 12    |  0.01826 |   0.01900 |   24.0       True\n| 13    |  0.01809 |   0.02099 |   25.9       False\n| 14    |  0.01782 |   0.02084 |   27.9       False\n| 15    |  0.01781 |   0.01824 |   30.1       True\n| 16    |  0.01769 |   0.02035 |   32.4       False\n| 17    |  0.01748 |   0.01996 |   34.4       False\n| 18    |  0.01742 |   0.01935 |   36.4       False\n| 19    |  0.01736 |   0.02102 |   38.2       False\n| 20    |  0.01728 |   0.01911 |   40.2       False\n| 21    |  0.01724 |   0.01865 |   42.3       False\n| 22    |  0.01727 |   0.01983 |   44.2       False\n| 23    |  0.01733 |   0.01846 |   46.3       False\n| 24    |  0.01703 |   0.01774 |   48.2       True\n| 25    |  0.01704 |   0.02120 |   50.5       False\n| 26    |  0.01692 |   0.01852 |   52.8       False\n| 27    |  0.01693 |   0.01843 |   54.7       False\n| 28    |  0.01691 |   0.01775 |   57.0       False\n| 29    |  0.01690 |   0.01780 |   59.0       False\n| 30    |  0.01693 |   0.01761 |   61.0       True\n| 31    |  0.01689 |   0.01767 |   63.5       False\n| 32    |  0.01678 |   0.01763 |   65.6       False\n| 33    |  0.01675 |   0.01735 |   67.6       True\n| 34    |  0.01671 |   0.01734 |   69.5       True\n| 35    |  0.01663 |   0.01732 |   71.5       True\n| 36    |  0.01665 |   0.01825 |   73.6       False\n| 37    |  0.01686 |   0.01842 |   75.7       False\n| 38    |  0.01680 |   0.01729 |   77.7       True\n| 39    |  0.01657 |   0.01726 |   79.7       True\n| 40    |  0.01649 |   0.01730 |   81.6       False\n| 41    |  0.01663 |   0.01773 |   83.5       False\n| 42    |  0.01652 |   0.01848 |   85.8       False\n| 43    |  0.01648 |   0.01731 |   87.7       False\n| 44    |  0.01663 |   0.01749 |   89.7       False\n| 45    |  0.01646 |   0.01738 |   91.7       False\n| 46    |  0.01629 |   0.01739 |   93.7       False\n| 47    |  0.01634 |   0.01714 |   96.0       True\n| 48    |  0.01641 |   0.01718 |   98.3       False\n| 49    |  0.01638 |   0.01708 |   100.2      True\n| 50    |  0.01631 |   0.01724 |   102.1      False\n| 51    |  0.01620 |   0.01708 |   104.1      False\n| 52    |  0.01623 |   0.01714 |   106.1      False\n| 53    |  0.01628 |   0.01725 |   108.2      False\n| 54    |  0.01624 |   0.01755 |   110.6      False\n| 55    |  0.01625 |   0.01727 |   112.9      False\n| 56    |  0.01625 |   0.01704 |   114.8      True\n| 57    |  0.01617 |   0.01700 |   116.7      True\n| 58    |  0.01613 |   0.01704 |   118.9      False\n| 59    |  0.01620 |   0.01703 |   121.0      False\n| 60    |  0.01616 |   0.01712 |   122.9      False\n| 61    |  0.01616 |   0.01700 |   124.8      False\n| 62    |  0.01610 |   0.01706 |   126.9      False\n| 63    |  0.01621 |   0.01707 |   129.4      False\n| 64    |  0.01619 |   0.01736 |   131.5      False\n| 65    |  0.01616 |   0.01742 |   133.5      False\n| 66    |  0.01619 |   0.01711 |   135.5      False\n| 67    |  0.01618 |   0.01686 |   137.4      True\n| 68    |  0.01615 |   0.01697 |   139.4      False\n| 69    |  0.01614 |   0.01696 |   141.5      False\n| 70    |  0.01615 |   0.01680 |   143.5      True\n| 71    |  0.01608 |   0.01704 |   145.5      False\n| 72    |  0.01605 |   0.01707 |   147.4      False\n| 73    |  0.01602 |   0.01705 |   149.4      False\n| 74    |  0.01602 |   0.01693 |   151.5      False\n| 75    |  0.01601 |   0.01711 |   153.4      False\n| 76    |  0.01617 |   0.01708 |   155.4      False\n| 77    |  0.01612 |   0.01688 |   157.4      False\n| 78    |  0.01603 |   0.01702 |   159.3      False\n| 79    |  0.01602 |   0.01695 |   161.2      False\n| 80    |  0.01605 |   0.01797 |   163.8      False\n| 81    |  0.01623 |   0.01706 |   165.8      False\n| 82    |  0.01610 |   0.01730 |   167.7      False\n| 83    |  0.01599 |   0.01713 |   170.0      False\n| 84    |  0.01602 |   0.01688 |   172.3      False\n| 85    |  0.01607 |   0.01685 |   174.5      False\n| 86    |  0.01617 |   0.01695 |   176.6      False\n| 87    |  0.01599 |   0.01688 |   178.7      False\n| 88    |  0.01605 |   0.01706 |   180.6      False\n| 89    |  0.01600 |   0.01754 |   182.7      False\n| 90    |  0.01601 |   0.01683 |   184.9      False\n| 91    |  0.01602 |   0.01744 |   186.8      False\n| 92    |  0.01610 |   0.01715 |   188.8      False\n| 93    |  0.01608 |   0.01722 |   190.8      False\n| 94    |  0.01616 |   0.01701 |   192.7      False\n| 95    |  0.01604 |   0.01715 |   195.3      False\n| 96    |  0.01603 |   0.01689 |   197.4      False\n| 97    |  0.01600 |   0.01690 |   199.3      False\n| 98    |  0.01600 |   0.01688 |   201.4      False\n| 99    |  0.01595 |   0.01694 |   203.4      False\n| 100   |  0.01599 |   0.01696 |   205.5      False\n| 101   |  0.01588 |   0.01695 |   207.6      False\n| 102   |  0.01588 |   0.01695 |   209.5      False\n| 103   |  0.01586 |   0.01687 |   211.4      False\n| 104   |  0.01579 |   0.01715 |   213.4      False\n| 105   |  0.01583 |   0.01684 |   215.2      False\n| 106   |  0.01590 |   0.01686 |   217.3      False\n| 107   |  0.01579 |   0.01710 |   219.2      False\n| 108   |  0.01588 |   0.01705 |   221.1      False\n| 109   |  0.01577 |   0.01694 |   223.0      False\n| 110   |  0.01580 |   0.01720 |   225.0      False\n| 111   |  0.01589 |   0.01713 |   226.9      False\n| 112   |  0.01580 |   0.01692 |   229.8      False\n| 113   |  0.01578 |   0.01683 |   232.0      False\n| 114   |  0.01579 |   0.01778 |   234.0      False\n| 115   |  0.01589 |   0.01697 |   235.9      False\n| 116   |  0.01586 |   0.01695 |   237.9      False\n| 117   |  0.01576 |   0.01700 |   240.1      False\n| 118   |  0.01576 |   0.01745 |   241.9      False\n| 119   |  0.01596 |   0.01703 |   243.8      False\n| 120   |  0.01583 |   0.01705 |   245.7      False\nEarly stopping occured at epoch 120\nTraining done in 245.693 seconds.\n---------------------------------------\nSuccessfully saved model at tabnet_raw_step1_fold7_0.zip\nvalidation fold 7 : 0.01680101149600198\nFOLDS :  8\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","name":"stderr"},{"output_type":"stream","text":"| 1     |  0.37008 |   0.03554 |   2.0        True\n| 2     |  0.02827 |   0.02586 |   4.0        True\n| 3     |  0.02248 |   0.02131 |   5.9        True\n| 4     |  0.02114 |   0.02070 |   7.7        True\n| 5     |  0.02073 |   0.02055 |   9.7        True\n| 6     |  0.02049 |   0.02030 |   11.6       True\n| 7     |  0.02020 |   0.02014 |   13.9       True\n| 8     |  0.01983 |   0.01979 |   15.8       True\n| 9     |  0.01936 |   0.01956 |   17.7       True\n| 10    |  0.01897 |   0.01915 |   19.5       True\n| 11    |  0.01869 |   0.01906 |   21.5       True\n| 12    |  0.01846 |   0.01883 |   23.3       True\n| 13    |  0.01834 |   0.02068 |   25.4       False\n| 14    |  0.01805 |   0.01915 |   27.4       False\n| 15    |  0.01796 |   0.02063 |   29.4       False\n| 16    |  0.01780 |   0.01857 |   31.3       True\n| 17    |  0.01761 |   0.01831 |   33.4       True\n| 18    |  0.01752 |   0.02045 |   35.6       False\n| 19    |  0.01745 |   0.02162 |   37.5       False\n| 20    |  0.01736 |   0.01996 |   39.7       False\n| 21    |  0.01726 |   0.01983 |   42.1       False\n| 22    |  0.01720 |   0.01865 |   44.5       False\n| 23    |  0.01709 |   0.01759 |   46.8       True\n| 24    |  0.01697 |   0.01976 |   49.1       False\n| 25    |  0.01694 |   0.01996 |   51.1       False\n| 26    |  0.01681 |   0.01860 |   53.1       False\n| 27    |  0.01678 |   0.02039 |   55.1       False\n| 28    |  0.01678 |   0.01971 |   57.3       False\n| 29    |  0.01683 |   0.01970 |   59.2       False\n| 30    |  0.01677 |   0.01741 |   61.3       True\n| 31    |  0.01667 |   0.01735 |   63.2       True\n| 32    |  0.01659 |   0.02054 |   65.2       False\n| 33    |  0.01648 |   0.01779 |   67.1       False\n| 34    |  0.01649 |   0.01742 |   69.4       False\n| 35    |  0.01649 |   0.01783 |   71.4       False\n| 36    |  0.01658 |   0.01771 |   73.3       False\n| 37    |  0.01660 |   0.01729 |   75.3       True\n| 38    |  0.01646 |   0.01791 |   77.4       False\n| 39    |  0.01649 |   0.01718 |   79.5       True\n| 40    |  0.01641 |   0.01737 |   81.4       False\n| 41    |  0.01639 |   0.01713 |   83.4       True\n| 42    |  0.01640 |   0.01705 |   85.3       True\n| 43    |  0.01639 |   0.01719 |   87.1       False\n| 44    |  0.01650 |   0.01747 |   89.1       False\n| 45    |  0.01632 |   0.01759 |   91.2       False\n| 46    |  0.01630 |   0.01700 |   93.1       True\n| 47    |  0.01630 |   0.01707 |   95.1       False\n| 48    |  0.01624 |   0.01726 |   96.9       False\n| 49    |  0.01632 |   0.01721 |   98.8       False\n| 50    |  0.01626 |   0.01708 |   100.9      False\n| 51    |  0.01629 |   0.01711 |   103.7      False\n| 52    |  0.01626 |   0.01735 |   105.7      False\n| 53    |  0.01620 |   0.01698 |   108.0      True\n| 54    |  0.01622 |   0.01697 |   110.0      True\n| 55    |  0.01622 |   0.01715 |   112.1      False\n| 56    |  0.01611 |   0.01702 |   114.1      False\n| 57    |  0.01610 |   0.01710 |   116.0      False\n| 58    |  0.01627 |   0.01708 |   117.8      False\n| 59    |  0.01614 |   0.01730 |   119.8      False\n| 60    |  0.01624 |   0.01709 |   121.7      False\n| 61    |  0.01621 |   0.01762 |   123.8      False\n| 62    |  0.01617 |   0.01696 |   125.8      True\n| 63    |  0.01613 |   0.01698 |   127.7      False\n| 64    |  0.01611 |   0.01709 |   129.5      False\n| 65    |  0.01626 |   0.01713 |   131.4      False\n| 66    |  0.01622 |   0.01719 |   133.3      False\n| 67    |  0.01609 |   0.01697 |   135.3      False\n| 68    |  0.01603 |   0.01703 |   137.2      False\n| 69    |  0.01608 |   0.01688 |   139.2      True\n| 70    |  0.01604 |   0.01699 |   141.4      False\n| 71    |  0.01610 |   0.01696 |   143.2      False\n| 72    |  0.01605 |   0.01724 |   145.4      False\n| 73    |  0.01608 |   0.01693 |   147.3      False\n| 74    |  0.01614 |   0.01703 |   149.1      False\n| 75    |  0.01606 |   0.01712 |   151.0      False\n| 76    |  0.01620 |   0.01702 |   152.9      False\n| 77    |  0.01612 |   0.01706 |   154.8      False\n| 78    |  0.01602 |   0.01692 |   156.9      False\n| 79    |  0.01599 |   0.01707 |   158.8      False\n| 80    |  0.01608 |   0.01701 |   160.7      False\n| 81    |  0.01606 |   0.01696 |   163.0      False\n| 82    |  0.01604 |   0.01705 |   164.9      False\n| 83    |  0.01610 |   0.01718 |   167.7      False\n| 84    |  0.01615 |   0.01704 |   169.6      False\n| 85    |  0.01608 |   0.01695 |   171.9      False\n| 86    |  0.01602 |   0.01711 |   173.8      False\n| 87    |  0.01604 |   0.01690 |   175.7      False\n| 88    |  0.01609 |   0.01711 |   177.7      False\n| 89    |  0.01618 |   0.01727 |   179.7      False\n| 90    |  0.01604 |   0.01694 |   181.7      False\n| 91    |  0.01601 |   0.01716 |   183.5      False\n| 92    |  0.01606 |   0.01692 |   185.4      False\n| 93    |  0.01599 |   0.01698 |   187.3      False\n| 94    |  0.01606 |   0.01689 |   189.5      False\n| 95    |  0.01601 |   0.01710 |   191.5      False\n| 96    |  0.01602 |   0.01688 |   193.4      False\n| 97    |  0.01603 |   0.01711 |   195.4      False\n| 98    |  0.01607 |   0.01712 |   197.3      False\n| 99    |  0.01602 |   0.01696 |   199.2      False\n| 100   |  0.01601 |   0.01712 |   201.4      False\n| 101   |  0.01594 |   0.01701 |   203.4      False\n| 102   |  0.01583 |   0.01691 |   205.7      False\n| 103   |  0.01590 |   0.01696 |   207.6      False\n| 104   |  0.01583 |   0.01703 |   209.5      False\n| 105   |  0.01590 |   0.01695 |   211.6      False\n| 106   |  0.01588 |   0.01691 |   213.5      False\n| 107   |  0.01590 |   0.01718 |   215.4      False\n| 108   |  0.01587 |   0.01710 |   217.3      False\n| 109   |  0.01587 |   0.01694 |   219.3      False\n| 110   |  0.01589 |   0.01980 |   221.2      False\n| 111   |  0.01601 |   0.01703 |   224.0      False\n| 112   |  0.01588 |   0.01710 |   226.0      False\n| 113   |  0.01588 |   0.01687 |   228.0      True\n| 114   |  0.01583 |   0.01701 |   229.9      False\n| 115   |  0.01585 |   0.01689 |   232.1      False\n| 116   |  0.01593 |   0.01700 |   234.3      False\n| 117   |  0.01584 |   0.01699 |   236.5      False\n| 118   |  0.01580 |   0.01712 |   238.5      False\n| 119   |  0.01596 |   0.01701 |   240.4      False\n| 120   |  0.01592 |   0.01695 |   242.2      False\n| 121   |  0.01585 |   0.01690 |   244.4      False\n| 122   |  0.01580 |   0.01718 |   246.3      False\n| 123   |  0.01578 |   0.01693 |   248.2      False\n| 124   |  0.01582 |   0.01697 |   250.2      False\n| 125   |  0.01586 |   0.01707 |   252.0      False\n| 126   |  0.01584 |   0.01697 |   253.9      False\n| 127   |  0.01590 |   0.01704 |   256.1      False\n| 128   |  0.01582 |   0.01698 |   258.0      False\n| 129   |  0.01575 |   0.01707 |   259.9      False\n| 130   |  0.01594 |   0.01717 |   261.8      False\n| 131   |  0.01591 |   0.01691 |   263.7      False\n| 132   |  0.01590 |   0.01700 |   265.8      False\n| 133   |  0.01592 |   0.01707 |   268.2      False\n| 134   |  0.01572 |   0.01706 |   270.2      False\n| 135   |  0.01580 |   0.01713 |   272.1      False\n| 136   |  0.01587 |   0.01728 |   274.1      False\n| 137   |  0.01588 |   0.01706 |   276.2      False\n| 138   |  0.01586 |   0.01707 |   278.4      False\n| 139   |  0.01585 |   0.01731 |   280.5      False\n| 140   |  0.01582 |   0.01692 |   282.9      False\n| 141   |  0.01584 |   0.01704 |   284.8      False\n| 142   |  0.01580 |   0.01693 |   287.0      False\n| 143   |  0.01579 |   0.01693 |   289.4      False\n| 144   |  0.01574 |   0.01702 |   291.5      False\n| 145   |  0.01582 |   0.01696 |   293.5      False\n| 146   |  0.01579 |   0.01715 |   295.4      False\n| 147   |  0.01581 |   0.01735 |   297.4      False\n| 148   |  0.01576 |   0.01695 |   299.7      False\n| 149   |  0.01584 |   0.01750 |   301.8      False\n| 150   |  0.01587 |   0.01701 |   303.7      False\n| 151   |  0.01574 |   0.01696 |   305.7      False\n| 152   |  0.01556 |   0.01749 |   307.7      False\n| 153   |  0.01567 |   0.01717 |   309.6      False\n| 154   |  0.01577 |   0.01684 |   311.8      True\n| 155   |  0.01561 |   0.01686 |   313.7      False\n| 156   |  0.01551 |   0.01719 |   315.6      False\n| 157   |  0.01566 |   0.01697 |   317.6      False\n| 158   |  0.01565 |   0.01692 |   319.5      False\n| 159   |  0.01556 |   0.01689 |   321.6      False\n","name":"stdout"},{"output_type":"stream","text":"| 160   |  0.01557 |   0.01696 |   323.6      False\n| 161   |  0.01561 |   0.01693 |   325.4      False\n| 162   |  0.01564 |   0.01707 |   327.3      False\n| 163   |  0.01560 |   0.01680 |   329.3      True\n| 164   |  0.01559 |   0.01700 |   331.3      False\n| 165   |  0.01555 |   0.01698 |   333.9      False\n| 166   |  0.01562 |   0.01705 |   335.9      False\n| 167   |  0.01556 |   0.01709 |   337.9      False\n| 168   |  0.01566 |   0.01715 |   339.8      False\n| 169   |  0.01553 |   0.01697 |   342.0      False\n| 170   |  0.01556 |   0.01692 |   344.6      False\n| 171   |  0.01556 |   0.01713 |   346.5      False\n| 172   |  0.01559 |   0.01706 |   348.5      False\n| 173   |  0.01559 |   0.01703 |   350.5      False\n| 174   |  0.01546 |   0.01682 |   352.5      False\n| 175   |  0.01549 |   0.01691 |   354.7      False\n| 176   |  0.01557 |   0.01716 |   356.5      False\n| 177   |  0.01554 |   0.01706 |   358.5      False\n| 178   |  0.01553 |   0.01815 |   360.5      False\n| 179   |  0.01560 |   0.01710 |   362.4      False\n| 180   |  0.01552 |   0.01700 |   364.7      False\n| 181   |  0.01563 |   0.01704 |   366.9      False\n| 182   |  0.01550 |   0.01708 |   368.9      False\n| 183   |  0.01544 |   0.01709 |   370.8      False\n| 184   |  0.01552 |   0.01686 |   372.8      False\n| 185   |  0.01543 |   0.01687 |   374.7      False\n| 186   |  0.01556 |   0.01705 |   376.9      False\n| 187   |  0.01561 |   0.01698 |   378.9      False\n| 188   |  0.01544 |   0.01726 |   380.8      False\n| 189   |  0.01543 |   0.01700 |   382.7      False\n| 190   |  0.01545 |   0.01716 |   384.6      False\n| 191   |  0.01538 |   0.01702 |   386.7      False\n| 192   |  0.01541 |   0.01697 |   388.7      False\n| 193   |  0.01560 |   0.01709 |   390.6      False\n| 194   |  0.01559 |   0.01683 |   392.6      False\n| 195   |  0.01540 |   0.01699 |   394.5      False\n| 196   |  0.01538 |   0.01722 |   396.7      False\n| 197   |  0.01550 |   0.01719 |   399.0      False\n| 198   |  0.01545 |   0.01693 |   400.8      False\n| 199   |  0.01542 |   0.01719 |   403.1      False\n| 200   |  0.01543 |   0.01706 |   405.3      False\nTraining done in 405.262 seconds.\n---------------------------------------\nSuccessfully saved model at tabnet_raw_step1_fold8_0.zip\nvalidation fold 8 : 0.0168018410365477\nFOLDS :  9\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","name":"stderr"},{"output_type":"stream","text":"| 1     |  0.37098 |   0.03649 |   2.2        True\n| 2     |  0.02872 |   0.02653 |   4.1        True\n| 3     |  0.02296 |   0.02162 |   6.2        True\n| 4     |  0.02136 |   0.02104 |   8.1        True\n| 5     |  0.02088 |   0.02084 |   9.9        True\n| 6     |  0.02056 |   0.02057 |   11.9       True\n| 7     |  0.02021 |   0.02043 |   14.1       True\n| 8     |  0.01992 |   0.02036 |   16.0       True\n| 9     |  0.01962 |   0.01990 |   18.0       True\n| 10    |  0.01939 |   0.01979 |   20.0       True\n| 11    |  0.01899 |   0.01936 |   22.2       True\n| 12    |  0.01863 |   0.01931 |   24.4       True\n| 13    |  0.01831 |   0.01916 |   26.3       True\n| 14    |  0.01806 |   0.01884 |   28.2       True\n| 15    |  0.01784 |   0.01931 |   30.1       False\n| 16    |  0.01783 |   0.02193 |   32.1       False\n| 17    |  0.01774 |   0.02071 |   34.0       False\n| 18    |  0.01755 |   0.01848 |   36.1       True\n| 19    |  0.01755 |   0.02073 |   38.1       False\n| 20    |  0.01740 |   0.01996 |   40.1       False\n| 21    |  0.01737 |   0.01903 |   41.9       False\n| 22    |  0.01729 |   0.01795 |   43.9       True\n| 23    |  0.01722 |   0.01855 |   46.1       False\n| 24    |  0.01712 |   0.01781 |   48.0       True\n| 25    |  0.01711 |   0.01804 |   50.0       False\n| 26    |  0.01700 |   0.01801 |   52.1       False\n| 27    |  0.01694 |   0.01757 |   54.7       True\n| 28    |  0.01683 |   0.01748 |   56.9       True\n| 29    |  0.01673 |   0.01770 |   59.3       False\n| 30    |  0.01680 |   0.01749 |   61.3       False\n| 31    |  0.01680 |   0.01979 |   63.3       False\n| 32    |  0.01667 |   0.01799 |   65.4       False\n| 33    |  0.01672 |   0.01776 |   67.5       False\n| 34    |  0.01682 |   0.01990 |   69.5       False\n| 35    |  0.01667 |   0.01732 |   71.4       True\n| 36    |  0.01664 |   0.01749 |   73.4       False\n| 37    |  0.01664 |   0.01731 |   75.3       True\n| 38    |  0.01653 |   0.01737 |   77.2       False\n| 39    |  0.01645 |   0.01728 |   79.5       True\n| 40    |  0.01649 |   0.01742 |   81.5       False\n| 41    |  0.01647 |   0.01782 |   83.5       False\n| 42    |  0.01652 |   0.01907 |   85.7       False\n| 43    |  0.01650 |   0.01722 |   87.8       True\n| 44    |  0.01638 |   0.01724 |   89.9       False\n| 45    |  0.01644 |   0.01749 |   91.8       False\n| 46    |  0.01633 |   0.01695 |   93.8       True\n| 47    |  0.01626 |   0.01758 |   95.7       False\n| 48    |  0.01635 |   0.01702 |   97.6       False\n| 49    |  0.01628 |   0.01717 |   99.5       False\n| 50    |  0.01623 |   0.01710 |   101.7      False\n| 51    |  0.01625 |   0.01714 |   103.6      False\n| 52    |  0.01631 |   0.01725 |   105.6      False\n| 53    |  0.01634 |   0.01797 |   107.5      False\n| 54    |  0.01626 |   0.01709 |   109.5      False\n| 55    |  0.01621 |   0.01703 |   111.7      False\n| 56    |  0.01613 |   0.01710 |   113.6      False\n| 57    |  0.01616 |   0.01706 |   116.6      False\n| 58    |  0.01620 |   0.01708 |   118.7      False\n| 59    |  0.01609 |   0.01720 |   120.7      False\n| 60    |  0.01617 |   0.01709 |   123.0      False\n| 61    |  0.01616 |   0.01702 |   125.0      False\n| 62    |  0.01622 |   0.01736 |   126.9      False\n| 63    |  0.01625 |   0.01700 |   128.8      False\n| 64    |  0.01613 |   0.01695 |   130.9      True\n| 65    |  0.01608 |   0.01707 |   132.8      False\n| 66    |  0.01617 |   0.01719 |   135.1      False\n| 67    |  0.01611 |   0.01710 |   137.1      False\n| 68    |  0.01612 |   0.01692 |   139.1      True\n| 69    |  0.01604 |   0.01689 |   141.1      True\n| 70    |  0.01618 |   0.01705 |   143.0      False\n| 71    |  0.01601 |   0.01709 |   145.3      False\n| 72    |  0.01619 |   0.01714 |   147.2      False\n| 73    |  0.01621 |   0.01723 |   149.4      False\n| 74    |  0.01606 |   0.01686 |   151.3      True\n| 75    |  0.01598 |   0.01698 |   153.3      False\n| 76    |  0.01598 |   0.01720 |   155.4      False\n| 77    |  0.01612 |   0.01684 |   157.4      True\n| 78    |  0.01611 |   0.01675 |   159.4      True\n| 79    |  0.01606 |   0.01725 |   161.3      False\n| 80    |  0.01621 |   0.01694 |   163.2      False\n| 81    |  0.01611 |   0.01696 |   165.2      False\n| 82    |  0.01603 |   0.01713 |   167.4      False\n| 83    |  0.01607 |   0.01692 |   169.4      False\n| 84    |  0.01594 |   0.01694 |   171.4      False\n| 85    |  0.01595 |   0.01701 |   173.3      False\n| 86    |  0.01607 |   0.01695 |   175.6      False\n| 87    |  0.01603 |   0.01704 |   177.8      False\n| 88    |  0.01605 |   0.01699 |   179.8      False\n| 89    |  0.01592 |   0.01693 |   182.8      False\n| 90    |  0.01619 |   0.01712 |   184.7      False\n| 91    |  0.01617 |   0.01701 |   186.7      False\n| 92    |  0.01602 |   0.01704 |   188.9      False\n| 93    |  0.01607 |   0.01692 |   190.9      False\n| 94    |  0.01616 |   0.01709 |   192.8      False\n| 95    |  0.01606 |   0.01680 |   194.8      False\n| 96    |  0.01599 |   0.01698 |   196.7      False\n| 97    |  0.01596 |   0.01690 |   198.6      False\n| 98    |  0.01595 |   0.01703 |   200.8      False\n| 99    |  0.01602 |   0.01699 |   202.8      False\n| 100   |  0.01603 |   0.01705 |   204.7      False\n| 101   |  0.01604 |   0.01670 |   206.7      True\n| 102   |  0.01593 |   0.01689 |   208.6      False\n| 103   |  0.01578 |   0.01690 |   210.8      False\n| 104   |  0.01578 |   0.01692 |   212.8      False\n| 105   |  0.01585 |   0.01699 |   215.0      False\n| 106   |  0.01590 |   0.01701 |   217.0      False\n| 107   |  0.01581 |   0.01706 |   219.0      False\n| 108   |  0.01591 |   0.01712 |   220.9      False\n| 109   |  0.01591 |   0.01706 |   223.1      False\n| 110   |  0.01583 |   0.01697 |   225.1      False\n| 111   |  0.01602 |   0.01697 |   227.1      False\n| 112   |  0.01586 |   0.01677 |   229.0      False\n| 113   |  0.01587 |   0.01687 |   231.1      False\n| 114   |  0.01586 |   0.01689 |   233.2      False\n| 115   |  0.01585 |   0.01693 |   235.7      False\n| 116   |  0.01597 |   0.01694 |   238.0      False\n| 117   |  0.01580 |   0.01692 |   239.9      False\n| 118   |  0.01581 |   0.01678 |   241.9      False\n| 119   |  0.01580 |   0.01695 |   244.3      False\n| 120   |  0.01576 |   0.01702 |   246.6      False\n| 121   |  0.01576 |   0.01689 |   248.6      False\n| 122   |  0.01587 |   0.01685 |   250.7      False\n| 123   |  0.01577 |   0.01675 |   252.6      False\n| 124   |  0.01574 |   0.01693 |   254.7      False\n| 125   |  0.01582 |   0.01688 |   256.8      False\n| 126   |  0.01583 |   0.01706 |   258.7      False\n| 127   |  0.01597 |   0.01681 |   260.7      False\n| 128   |  0.01582 |   0.01698 |   262.7      False\n| 129   |  0.01579 |   0.01688 |   264.6      False\n| 130   |  0.01581 |   0.01681 |   266.7      False\n| 131   |  0.01575 |   0.01684 |   268.7      False\n| 132   |  0.01592 |   0.01684 |   270.6      False\n| 133   |  0.01575 |   0.01688 |   272.5      False\n| 134   |  0.01589 |   0.01676 |   274.5      False\n| 135   |  0.01575 |   0.01687 |   276.7      False\n| 136   |  0.01595 |   0.01713 |   278.9      False\n| 137   |  0.01578 |   0.01771 |   281.0      False\n| 138   |  0.01586 |   0.01711 |   282.9      False\n| 139   |  0.01579 |   0.01676 |   284.8      False\n| 140   |  0.01568 |   0.01686 |   286.8      False\n| 141   |  0.01578 |   0.01686 |   288.9      False\n| 142   |  0.01582 |   0.01687 |   290.8      False\n| 143   |  0.01581 |   0.01698 |   292.8      False\n| 144   |  0.01581 |   0.01700 |   295.1      False\n| 145   |  0.01577 |   0.01703 |   297.1      False\n| 146   |  0.01588 |   0.01723 |   299.6      False\n| 147   |  0.01585 |   0.01693 |   301.8      False\n| 148   |  0.01578 |   0.01708 |   303.8      False\n| 149   |  0.01576 |   0.01711 |   305.9      False\n| 150   |  0.01579 |   0.01681 |   307.8      False\n| 151   |  0.01571 |   0.01722 |   310.4      False\nEarly stopping occured at epoch 151\nTraining done in 310.363 seconds.\n---------------------------------------\nSuccessfully saved model at tabnet_raw_step1_fold9_0.zip\nvalidation fold 9 : 0.016703545252082744\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if cfg.strategy != \"KFOLD\":\n    i = 0\n    mskf = MultilabelStratifiedShuffleSplit(n_splits=1000, test_size=0.1, random_state=0)\n    oof_preds = []\n    oof_targets = []\n    scores = []\n    scores_auc = []\n    for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n        if i == cfg.SPLITS:\n            break\n            \n        if not check_targets(targets_tr[train_idx]):\n            continue\n        print(\"FOLDS : \", i, j)\n\n        ## model\n\n        ## model\n        X_train, y_train = torch.as_tensor(np.concatenate([cat_tr[train_idx], numerical_tr[train_idx] ], axis=1)), torch.as_tensor(targets_tr[train_idx])\n        X_val, y_val = torch.as_tensor(np.concatenate([cat_tr[val_idx], numerical_tr[val_idx] ], axis=1)), torch.as_tensor(targets_tr[val_idx])\n        model = TabNetRegressor(n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_dims=cat_dims, cat_emb_dim=cfg.cat_emb_dim, cat_idxs=cats_idx, optimizer_fn=torch.optim.Adam,\n                               optimizer_params=dict(lr=1e-3, amsgrad=True), mask_type=\"sparsemax\", device_name=cfg.device)\n        \n        name = cfg.save_name + f\"_{j}\"\n        model.load_model(name)\n        # preds on val\n        preds = model.predict(X_val)\n        preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n        score = log_loss_multi(y_val, preds)\n\n        # preds on test\n        temp = model.predict(X_test)\n        p.append(torch.sigmoid(torch.as_tensor(temp)).detach().cpu().numpy())\n        ## save oof to compute the CV later\n        oof_preds.append(preds)\n        oof_targets.append(y_val)\n        scores.append(score)\n        scores_auc.append(auc_multi(y_val,preds))\n        print(f\"validation fold {j} : {score}\")\n        i+=1\n        p = np.stack(p)\n        preds_test.append(p)\n        \n    preds_test = np.stack(preds_test)\n\n        ","execution_count":86,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if cfg.strategy == \"KFOLD\":\n    print(\"auc mean : \", sum(scores_auc)/len(scores_auc))\n    print(\"CV score : \", log_loss_multi(np.concatenate(oof_targets) , np.concatenate(oof_preds)))","execution_count":87,"outputs":[{"output_type":"stream","text":"auc mean :  0.722216329536552\nCV score :  0.016717929108796225\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores_all)","execution_count":88,"outputs":[{"output_type":"stream","text":"[array([0.01670621, 0.0167217 , 0.01686626, 0.01671533, 0.0167134 ,\n       0.01651883, 0.01663102, 0.01680101, 0.01680184, 0.01670355])]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model)","execution_count":89,"outputs":[{"output_type":"stream","text":"TabNetRegressor(cat_dims=[3, 2], cat_emb_dim=[1, 1], cat_idxs=[0, 1],\n                device_name='cuda', input_dim=1069, lambda_sparse=0,\n                mask_type='entmax', n_a=32, n_d=32, n_steps=1,\n                optimizer_params={'lr': 0.02, 'weight_decay': 1e-05},\n                output_dim=206,\n                scheduler_fn=<class 'torch.optim.lr_scheduler.MultiStepLR'>,\n                scheduler_params={'gamma': 0.9, 'milestones': [100, 150]})\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# SAVE CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[columns] = preds_test.mean(1).mean(0)\nsubmission.loc[test_features['cp_type']=='ctl_vehicle', submission.columns[1:]] = 0","execution_count":90,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":91,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":92,"outputs":[{"output_type":"execute_result","execution_count":92,"data":{"text/plain":"            sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n0     id_0004d9e33                     0.001492                0.001885   \n1     id_001897cda                     0.000878                0.001746   \n2     id_002429b5b                     0.000000                0.000000   \n3     id_00276f245                     0.001421                0.001484   \n4     id_0027f1083                     0.001667                0.001956   \n...            ...                          ...                     ...   \n3977  id_ff7004b87                     0.001323                0.002000   \n3978  id_ff925dd0d                     0.001499                0.001821   \n3979  id_ffb710450                     0.001565                0.001616   \n3980  id_ffbb869f2                     0.001474                0.001591   \n3981  id_ffd5800b6                     0.001415                0.001640   \n\n      acat_inhibitor  acetylcholine_receptor_agonist  \\\n0           0.002320                        0.013790   \n1           0.002983                        0.004512   \n2           0.000000                        0.000000   \n3           0.001839                        0.008887   \n4           0.002006                        0.011050   \n...              ...                             ...   \n3977        0.002356                        0.003968   \n3978        0.001812                        0.008157   \n3979        0.001499                        0.013091   \n3980        0.002047                        0.014482   \n3981        0.001991                        0.013413   \n\n      acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n0                              0.027188                        0.005963   \n1                              0.001945                        0.002236   \n2                              0.000000                        0.000000   \n3                              0.018762                        0.005104   \n4                              0.020228                        0.005513   \n...                                 ...                             ...   \n3977                           0.004965                        0.002659   \n3978                           0.017779                        0.006008   \n3979                           0.035023                        0.006815   \n3980                           0.022723                        0.005772   \n3981                           0.022862                        0.005864   \n\n      adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n0                       0.003252                       0.005324   \n1                       0.005395                       0.008958   \n2                       0.000000                       0.000000   \n3                       0.002898                       0.004580   \n4                       0.003414                       0.003215   \n...                          ...                            ...   \n3977                    0.002574                       0.003875   \n3978                    0.003159                       0.004622   \n3979                    0.003063                       0.005003   \n3980                    0.004033                       0.003793   \n3981                    0.002807                       0.004260   \n\n      adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n0                       0.000574  ...                               0.001314   \n1                       0.004675  ...                               0.001864   \n2                       0.000000  ...                               0.000000   \n3                       0.000727  ...                               0.001036   \n4                       0.000858  ...                               0.001246   \n...                          ...  ...                                    ...   \n3977                    0.001595  ...                               0.001512   \n3978                    0.000746  ...                               0.001163   \n3979                    0.000490  ...                               0.001041   \n3980                    0.000908  ...                               0.001215   \n3981                    0.000603  ...                               0.001200   \n\n      trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n0         0.001417         0.004545           0.001208   \n1         0.002398         0.005700           0.001218   \n2         0.000000         0.000000           0.000000   \n3         0.001847         0.002643           0.007301   \n4         0.001248         0.003329           0.002360   \n...            ...              ...                ...   \n3977      0.005205         0.003325           0.074602   \n3978      0.001422         0.002935           0.001643   \n3979      0.000952         0.003023           0.001348   \n3980      0.001283         0.003330           0.002149   \n3981      0.002031         0.003159           0.003171   \n\n      tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n0                      0.001264                               0.001068   \n1                      0.005020                               0.001163   \n2                      0.000000                               0.000000   \n3                      0.003770                               0.000954   \n4                      0.001897                               0.001097   \n...                         ...                                    ...   \n3977                   0.015459                               0.001550   \n3978                   0.003717                               0.001034   \n3979                   0.001515                               0.000841   \n3980                   0.001859                               0.000945   \n3981                   0.001859                               0.001069   \n\n      vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n0            0.001607   0.002246                    0.001369       0.001672  \n1            0.016643   0.002217                    0.002564       0.002710  \n2            0.000000   0.000000                    0.000000       0.000000  \n3            0.001711   0.001966                    0.000590       0.001677  \n4            0.001767   0.002168                    0.000934       0.001877  \n...               ...        ...                         ...            ...  \n3977         0.009944   0.001705                    0.000744       0.001744  \n3978         0.002956   0.002230                    0.000593       0.001869  \n3979         0.001276   0.002251                    0.000718       0.001499  \n3980         0.001134   0.002386                    0.000763       0.001802  \n3981         0.001012   0.002354                    0.000982       0.001712  \n\n[3982 rows x 207 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>5-alpha_reductase_inhibitor</th>\n      <th>11-beta-hsd1_inhibitor</th>\n      <th>acat_inhibitor</th>\n      <th>acetylcholine_receptor_agonist</th>\n      <th>acetylcholine_receptor_antagonist</th>\n      <th>acetylcholinesterase_inhibitor</th>\n      <th>adenosine_receptor_agonist</th>\n      <th>adenosine_receptor_antagonist</th>\n      <th>adenylyl_cyclase_activator</th>\n      <th>...</th>\n      <th>tropomyosin_receptor_kinase_inhibitor</th>\n      <th>trpv_agonist</th>\n      <th>trpv_antagonist</th>\n      <th>tubulin_inhibitor</th>\n      <th>tyrosine_kinase_inhibitor</th>\n      <th>ubiquitin_specific_protease_inhibitor</th>\n      <th>vegfr_inhibitor</th>\n      <th>vitamin_b</th>\n      <th>vitamin_d_receptor_agonist</th>\n      <th>wnt_inhibitor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_0004d9e33</td>\n      <td>0.001492</td>\n      <td>0.001885</td>\n      <td>0.002320</td>\n      <td>0.013790</td>\n      <td>0.027188</td>\n      <td>0.005963</td>\n      <td>0.003252</td>\n      <td>0.005324</td>\n      <td>0.000574</td>\n      <td>...</td>\n      <td>0.001314</td>\n      <td>0.001417</td>\n      <td>0.004545</td>\n      <td>0.001208</td>\n      <td>0.001264</td>\n      <td>0.001068</td>\n      <td>0.001607</td>\n      <td>0.002246</td>\n      <td>0.001369</td>\n      <td>0.001672</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_001897cda</td>\n      <td>0.000878</td>\n      <td>0.001746</td>\n      <td>0.002983</td>\n      <td>0.004512</td>\n      <td>0.001945</td>\n      <td>0.002236</td>\n      <td>0.005395</td>\n      <td>0.008958</td>\n      <td>0.004675</td>\n      <td>...</td>\n      <td>0.001864</td>\n      <td>0.002398</td>\n      <td>0.005700</td>\n      <td>0.001218</td>\n      <td>0.005020</td>\n      <td>0.001163</td>\n      <td>0.016643</td>\n      <td>0.002217</td>\n      <td>0.002564</td>\n      <td>0.002710</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_002429b5b</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_00276f245</td>\n      <td>0.001421</td>\n      <td>0.001484</td>\n      <td>0.001839</td>\n      <td>0.008887</td>\n      <td>0.018762</td>\n      <td>0.005104</td>\n      <td>0.002898</td>\n      <td>0.004580</td>\n      <td>0.000727</td>\n      <td>...</td>\n      <td>0.001036</td>\n      <td>0.001847</td>\n      <td>0.002643</td>\n      <td>0.007301</td>\n      <td>0.003770</td>\n      <td>0.000954</td>\n      <td>0.001711</td>\n      <td>0.001966</td>\n      <td>0.000590</td>\n      <td>0.001677</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_0027f1083</td>\n      <td>0.001667</td>\n      <td>0.001956</td>\n      <td>0.002006</td>\n      <td>0.011050</td>\n      <td>0.020228</td>\n      <td>0.005513</td>\n      <td>0.003414</td>\n      <td>0.003215</td>\n      <td>0.000858</td>\n      <td>...</td>\n      <td>0.001246</td>\n      <td>0.001248</td>\n      <td>0.003329</td>\n      <td>0.002360</td>\n      <td>0.001897</td>\n      <td>0.001097</td>\n      <td>0.001767</td>\n      <td>0.002168</td>\n      <td>0.000934</td>\n      <td>0.001877</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3977</th>\n      <td>id_ff7004b87</td>\n      <td>0.001323</td>\n      <td>0.002000</td>\n      <td>0.002356</td>\n      <td>0.003968</td>\n      <td>0.004965</td>\n      <td>0.002659</td>\n      <td>0.002574</td>\n      <td>0.003875</td>\n      <td>0.001595</td>\n      <td>...</td>\n      <td>0.001512</td>\n      <td>0.005205</td>\n      <td>0.003325</td>\n      <td>0.074602</td>\n      <td>0.015459</td>\n      <td>0.001550</td>\n      <td>0.009944</td>\n      <td>0.001705</td>\n      <td>0.000744</td>\n      <td>0.001744</td>\n    </tr>\n    <tr>\n      <th>3978</th>\n      <td>id_ff925dd0d</td>\n      <td>0.001499</td>\n      <td>0.001821</td>\n      <td>0.001812</td>\n      <td>0.008157</td>\n      <td>0.017779</td>\n      <td>0.006008</td>\n      <td>0.003159</td>\n      <td>0.004622</td>\n      <td>0.000746</td>\n      <td>...</td>\n      <td>0.001163</td>\n      <td>0.001422</td>\n      <td>0.002935</td>\n      <td>0.001643</td>\n      <td>0.003717</td>\n      <td>0.001034</td>\n      <td>0.002956</td>\n      <td>0.002230</td>\n      <td>0.000593</td>\n      <td>0.001869</td>\n    </tr>\n    <tr>\n      <th>3979</th>\n      <td>id_ffb710450</td>\n      <td>0.001565</td>\n      <td>0.001616</td>\n      <td>0.001499</td>\n      <td>0.013091</td>\n      <td>0.035023</td>\n      <td>0.006815</td>\n      <td>0.003063</td>\n      <td>0.005003</td>\n      <td>0.000490</td>\n      <td>...</td>\n      <td>0.001041</td>\n      <td>0.000952</td>\n      <td>0.003023</td>\n      <td>0.001348</td>\n      <td>0.001515</td>\n      <td>0.000841</td>\n      <td>0.001276</td>\n      <td>0.002251</td>\n      <td>0.000718</td>\n      <td>0.001499</td>\n    </tr>\n    <tr>\n      <th>3980</th>\n      <td>id_ffbb869f2</td>\n      <td>0.001474</td>\n      <td>0.001591</td>\n      <td>0.002047</td>\n      <td>0.014482</td>\n      <td>0.022723</td>\n      <td>0.005772</td>\n      <td>0.004033</td>\n      <td>0.003793</td>\n      <td>0.000908</td>\n      <td>...</td>\n      <td>0.001215</td>\n      <td>0.001283</td>\n      <td>0.003330</td>\n      <td>0.002149</td>\n      <td>0.001859</td>\n      <td>0.000945</td>\n      <td>0.001134</td>\n      <td>0.002386</td>\n      <td>0.000763</td>\n      <td>0.001802</td>\n    </tr>\n    <tr>\n      <th>3981</th>\n      <td>id_ffd5800b6</td>\n      <td>0.001415</td>\n      <td>0.001640</td>\n      <td>0.001991</td>\n      <td>0.013413</td>\n      <td>0.022862</td>\n      <td>0.005864</td>\n      <td>0.002807</td>\n      <td>0.004260</td>\n      <td>0.000603</td>\n      <td>...</td>\n      <td>0.001200</td>\n      <td>0.002031</td>\n      <td>0.003159</td>\n      <td>0.003171</td>\n      <td>0.001859</td>\n      <td>0.001069</td>\n      <td>0.001012</td>\n      <td>0.002354</td>\n      <td>0.000982</td>\n      <td>0.001712</td>\n    </tr>\n  </tbody>\n</table>\n<p>3982 rows × 207 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.feature_importances_","execution_count":93,"outputs":[{"output_type":"execute_result","execution_count":93,"data":{"text/plain":"array([9.22122375e-05, 8.10200132e-05, 8.53471553e-05, ...,\n       8.42314837e-05, 8.43182087e-05, 2.96372544e-02])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.reducing_matrix","execution_count":94,"outputs":[{"output_type":"execute_result","execution_count":94,"data":{"text/plain":"<1069x1069 sparse matrix of type '<class 'numpy.float64'>'\n\twith 1069 stored elements in Compressed Sparse Column format>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}
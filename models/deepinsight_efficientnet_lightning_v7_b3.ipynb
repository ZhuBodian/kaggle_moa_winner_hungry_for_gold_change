{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.6.0+cu101\n",
      "PyTorch Lightning Version: 1.0.4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Transform non-image features to an image matrix using t-SNE non-linear dimensionality reduction.\n",
    "\n",
    "[V2]\n",
    "* Reduce memory usage by transforming rows to images in real-time\n",
    "\n",
    "[V3]\n",
    "* Tuning architecture and performance\n",
    " - 1 FC ELU Layer without dropout, T_max=5 --> fold0 val_loss_epoch=0.014794\n",
    " \n",
    "[V4]\n",
    "* 1 FC ELU Layer with dropout=0.5, T_max=5     --> worse,  fold0 val_loss_epoch=0.014915\n",
    "* CosineAnnealingWarmRestarts (T_0=5 epochs)   --> worse,  fold0 val_loss_epoch=0.014866\n",
    "* nn.SELU() (scaled exponential linear units)  --> better, fold0 val_loss_epoch=0.014761\n",
    "* Normalized RGB for pretrained models         --> worse,  fold0 val_loss_epoch=0.014933\n",
    "* nn.GELU() (Gaussian Error Linear Units)      --> worse,  fold0 val_loss_epoch=0.014877\n",
    "* SwapNoise (randomly swap features with p=0.15, portion=0.1)\n",
    "\n",
    "[V5]\n",
    "* SwapNoise (randomly swap features with p=0.3, portion=0.1)\n",
    "* 400 resolution/image size --> better, fold0 val_loss_epoch=0.014776\n",
    "\n",
    "[V6]\n",
    "* KernelPCA + PCA as 2nd and 3rd channels\n",
    "\n",
    "[V7]\n",
    "* Add Max./Min. Channels\n",
    "\n",
    "[TODO]\n",
    "* PCGrad (Project Conflicting Gradients)\n",
    "* Separate gene expression, cell vaibility and other features\n",
    "* Tuning resolution and image size\n",
    "\n",
    "EfficientNet Setup Parameters:\n",
    "https://github.com/rwightman/gen-efficientnet-pytorch/blob/master/geffnet/gen_efficientnet.py#L502\n",
    "\"\"\"\n",
    "\n",
    "kernel_mode = True\n",
    "training_mode = False\n",
    "\n",
    "import sys\n",
    "if kernel_mode:\n",
    "    sys.path.insert(0, \"../input/iterative-stratification\")\n",
    "    sys.path.insert(0, \"../input/pytorch-lightning\")\n",
    "    sys.path.insert(0, \"../input/gen-efficientnet-pytorch\")\n",
    "    sys.path.insert(0, \"../input/pytorch-optimizer\")\n",
    "    sys.path.insert(0, \"../input/pytorch-ranger\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "import glob\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer, LabelEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch_optimizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.metrics.functional import classification\n",
    "\n",
    "import geffnet\n",
    "\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug.augmenters.size import CropToFixedSize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "rand_seed = 1120\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning Version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if kernel_mode:\n",
    "#     !mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "#     !cp ../input/gen-efficientnet-pretrained/tf_efficientnet_*.pth /root/.cache/torch/hub/checkpoints/\n",
    "#     !ls -la /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"b3\"\n",
    "pretrained_model = f\"tf_efficientnet_{model_type}_ns\"\n",
    "experiment_name = f\"deepinsight_efficientnet_v7_{model_type}\"\n",
    "\n",
    "if kernel_mode:\n",
    "    dataset_folder = \"../input/lish-moa\"\n",
    "    model_output_folder = f\"./{experiment_name}\" if training_mode \\\n",
    "        else f\"../input/deepinsight-efficientnet-v7-b3/{experiment_name}\"\n",
    "else:\n",
    "    dataset_folder = \"/workspace/Kaggle/MoA\"\n",
    "    model_output_folder = f\"{dataset_folder}/{experiment_name}\" if training_mode \\\n",
    "        else f\"/workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v7_b3/{experiment_name}\"\n",
    "\n",
    "if training_mode:\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    # Dedicated logger for experiment\n",
    "    exp_logger = TensorBoardLogger(model_output_folder,\n",
    "                                   name=f\"overall_logs\",\n",
    "                                   default_hp_metric=False)\n",
    "\n",
    "# debug_mode = True\n",
    "debug_mode = False\n",
    "\n",
    "num_workers = 2 if kernel_mode else 6\n",
    "# gpus = [0, 1]\n",
    "gpus = [0]\n",
    "# gpus = [1]\n",
    "\n",
    "epochs = 200\n",
    "patience = 12\n",
    "\n",
    "# learning_rate = 7e-4\n",
    "# learning_rate = 1e-3\n",
    "learning_rate = 0.000352  # Suggested Learning Rate from LR finder (V7)\n",
    "learning_rate *= len(gpus)\n",
    "weight_decay = 1e-6\n",
    "# weight_decay = 0\n",
    "\n",
    "# T_max = 10  # epochs\n",
    "T_max = 5  # epochs\n",
    "T_0 = 5  # epochs\n",
    "\n",
    "accumulate_grad_batches = 1\n",
    "gradient_clip_val = 10.0\n",
    "\n",
    "if model_type == \"b0\":\n",
    "    batch_size = 128\n",
    "    infer_batch_size = 256\n",
    "    image_size = 224  # B0\n",
    "    drop_rate = 0.2  # B0\n",
    "    resolution = 224\n",
    "elif model_type == \"b3\":\n",
    "    batch_size = 48\n",
    "    infer_batch_size = 96 if not kernel_mode else 256\n",
    "    image_size = 300  # B3\n",
    "    drop_rate = 0.3  # B3\n",
    "    resolution = 300\n",
    "elif model_type == \"b5\":\n",
    "    batch_size = 12\n",
    "    infer_batch_size = 24\n",
    "    image_size = 456  # B5\n",
    "    drop_rate = 0.4  # B5\n",
    "    resolution = 456\n",
    "elif model_type == \"b7\":\n",
    "    batch_size = 2\n",
    "    infer_batch_size = 4\n",
    "    # image_size = 800  # B7\n",
    "    image_size = 875  # B7\n",
    "    drop_rate = 0.5  # B7\n",
    "    resolution = 875\n",
    "\n",
    "# Prediction Clipping Thresholds\n",
    "prob_min = 0.001\n",
    "prob_max = 0.999\n",
    "\n",
    "# Swap Noise\n",
    "swap_prob = 0.1\n",
    "swap_portion = 0.15\n",
    "\n",
    "label_smoothing = 0.001\n",
    "\n",
    "# DeepInsight Transform\n",
    "perplexity = 5\n",
    "\n",
    "drop_connect_rate = 0.2\n",
    "fc_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(\n",
    "    f\"{dataset_folder}/train_features.csv\", engine='c')\n",
    "train_labels = pd.read_csv(\n",
    "    f\"{dataset_folder}/train_targets_scored.csv\", engine='c')\n",
    "\n",
    "train_extra_labels = pd.read_csv(\n",
    "    f\"{dataset_folder}/train_targets_nonscored.csv\", engine='c')\n",
    "\n",
    "test_features = pd.read_csv(\n",
    "    f\"{dataset_folder}/test_features.csv\", engine='c')\n",
    "\n",
    "sample_submission = pd.read_csv(\n",
    "    f\"{dataset_folder}/sample_submission.csv\", engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by sig_id to ensure that all row orders match\n",
    "train_features = train_features.sort_values(\n",
    "    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)\n",
    "train_labels = train_labels.sort_values(by=[\"sig_id\"], axis=0,\n",
    "                                        inplace=False).reset_index(drop=True)\n",
    "train_extra_labels = train_extra_labels.sort_values(\n",
    "    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)\n",
    "\n",
    "sample_submission = sample_submission.sort_values(\n",
    "    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23814, 876), (23814, 207), (23814, 403))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, train_labels.shape, train_extra_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3982, 876)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(873, 772, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_features = [\"cp_type\", \"cp_dose\"]\n",
    "numeric_features = [c for c in train_features.columns if c != \"sig_id\" and c not in category_features]\n",
    "all_features = category_features + numeric_features\n",
    "gene_experssion_features = [c for c in numeric_features if c.startswith(\"g-\")]\n",
    "cell_viability_features = [c for c in numeric_features if c.startswith(\"c-\")]\n",
    "len(numeric_features), len(gene_experssion_features), len(cell_viability_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206, 402)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes = [c for c in train_labels.columns if c != \"sig_id\"]\n",
    "train_extra_classes = [c for c in train_extra_labels.columns if c != \"sig_id\"]\n",
    "len(train_classes), len(train_extra_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for df in [train_features, test_features]:\n",
    "    df['cp_type'] = df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n",
    "    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    df['cp_time'] = df['cp_time'].map({24: 0, 48: 0.5, 72: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    21948\n",
       "0     1866\n",
       "Name: cp_type, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"cp_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12147\n",
       "1    11667\n",
       "Name: cp_dose, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"cp_dose\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5    8250\n",
       "1.0    7792\n",
       "0.0    7772\n",
       "Name: cp_time, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"cp_time\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DeepInsight Transform (t-SNE)\n",
    "Based on https://github.com/alok-ai-lab/DeepInsight, but with some minor corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Modified from DeepInsight Transform\n",
    "# https://github.com/alok-ai-lab/DeepInsight/blob/master/pyDeepInsight/image_transformer.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib import pyplot as plt\n",
    "import inspect\n",
    "\n",
    "\n",
    "class DeepInsightTransformer:\n",
    "    \"\"\"Transform features to an image matrix using dimensionality reduction\n",
    "\n",
    "    This class takes in data normalized between 0 and 1 and converts it to a\n",
    "    CNN compatible 'image' matrix\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_extractor='tsne',\n",
    "                 perplexity=30,\n",
    "                 pixels=100,\n",
    "                 random_state=None,\n",
    "                 n_jobs=None):\n",
    "        \"\"\"Generate an ImageTransformer instance\n",
    "\n",
    "        Args:\n",
    "            feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n",
    "                class instance with method `fit_transform` that returns a\n",
    "                2-dimensional array of extracted features.\n",
    "            pixels: int (square matrix) or tuple of ints (height, width) that\n",
    "                defines the size of the image matrix.\n",
    "            random_state: int or RandomState. Determines the random number\n",
    "                generator, if present, of a string defined feature_extractor.\n",
    "            n_jobs: The number of parallel jobs to run for a string defined\n",
    "                feature_extractor.\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "        if isinstance(feature_extractor, str):\n",
    "            fe = feature_extractor.casefold()\n",
    "            if fe == 'tsne_exact'.casefold():\n",
    "                fe = TSNE(n_components=2,\n",
    "                          metric='cosine',\n",
    "                          perplexity=perplexity,\n",
    "                          n_iter=1000,\n",
    "                          method='exact',\n",
    "                          random_state=self.random_state,\n",
    "                          n_jobs=self.n_jobs)\n",
    "            elif fe == 'tsne'.casefold():\n",
    "                fe = TSNE(n_components=2,\n",
    "                          metric='cosine',\n",
    "                          perplexity=perplexity,\n",
    "                          n_iter=1000,\n",
    "                          method='barnes_hut',\n",
    "                          random_state=self.random_state,\n",
    "                          n_jobs=self.n_jobs)\n",
    "            elif fe == 'pca'.casefold():\n",
    "                fe = PCA(n_components=2, random_state=self.random_state)\n",
    "            elif fe == 'kpca'.casefold():\n",
    "                fe = KernelPCA(n_components=2,\n",
    "                               kernel='rbf',\n",
    "                               random_state=self.random_state,\n",
    "                               n_jobs=self.n_jobs)\n",
    "            else:\n",
    "                raise ValueError((\"Feature extraction method '{}' not accepted\"\n",
    "                                  ).format(feature_extractor))\n",
    "            self._fe = fe\n",
    "        elif hasattr(feature_extractor, 'fit_transform') and \\\n",
    "                inspect.ismethod(feature_extractor.fit_transform):\n",
    "            self._fe = feature_extractor\n",
    "        else:\n",
    "            raise TypeError('Parameter feature_extractor is not a '\n",
    "                            'string nor has method \"fit_transform\"')\n",
    "\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "\n",
    "        # The resolution of transformed image\n",
    "        self._pixels = pixels\n",
    "        self._xrot = None\n",
    "\n",
    "    def fit(self, X, y=None, plot=False):\n",
    "        \"\"\"Train the image transformer from the training set (X)\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            y: Ignored. Present for continuity with scikit-learn\n",
    "            plot: boolean of whether to produce a scatter plot showing the\n",
    "                feature reduction, hull points, and minimum bounding rectangle\n",
    "\n",
    "        Returns:\n",
    "            self: object\n",
    "        \"\"\"\n",
    "        # Transpose to get (n_features, n_samples)\n",
    "        X = X.T\n",
    "\n",
    "        # Perform dimensionality reduction\n",
    "        x_new = self._fe.fit_transform(X)\n",
    "\n",
    "        # Get the convex hull for the points\n",
    "        chvertices = ConvexHull(x_new).vertices\n",
    "        hull_points = x_new[chvertices]\n",
    "\n",
    "        # Determine the minimum bounding rectangle\n",
    "        mbr, mbr_rot = self._minimum_bounding_rectangle(hull_points)\n",
    "\n",
    "        # Rotate the matrix\n",
    "        # Save the rotated matrix in case user wants to change the pixel size\n",
    "        self._xrot = np.dot(mbr_rot, x_new.T).T\n",
    "\n",
    "        # Determine feature coordinates based on pixel dimension\n",
    "        self._calculate_coords()\n",
    "\n",
    "        # plot rotation diagram if requested\n",
    "        if plot is True:\n",
    "            # Create subplots\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n",
    "            ax[0, 0].scatter(x_new[:, 0],\n",
    "                             x_new[:, 1],\n",
    "                             cmap=plt.cm.get_cmap(\"jet\", 10),\n",
    "                             marker=\"x\",\n",
    "                             alpha=1.0)\n",
    "            ax[0, 0].fill(x_new[chvertices, 0],\n",
    "                          x_new[chvertices, 1],\n",
    "                          edgecolor='r',\n",
    "                          fill=False)\n",
    "            ax[0, 0].fill(mbr[:, 0], mbr[:, 1], edgecolor='g', fill=False)\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "            plt.show()\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def pixels(self):\n",
    "        \"\"\"The image matrix dimensions\n",
    "\n",
    "        Returns:\n",
    "            tuple: the image matrix dimensions (height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        return self._pixels\n",
    "\n",
    "    @pixels.setter\n",
    "    def pixels(self, pixels):\n",
    "        \"\"\"Set the image matrix dimension\n",
    "\n",
    "        Args:\n",
    "            pixels: int or tuple with the dimensions (height, width)\n",
    "            of the image matrix\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "        self._pixels = pixels\n",
    "        # recalculate coordinates if already fit\n",
    "        if hasattr(self, '_coords'):\n",
    "            self._calculate_coords()\n",
    "\n",
    "    def _calculate_coords(self):\n",
    "        \"\"\"Calculate the matrix coordinates of each feature based on the\n",
    "        pixel dimensions.\n",
    "        \"\"\"\n",
    "        ax0_coord = np.digitize(self._xrot[:, 0],\n",
    "                                bins=np.linspace(min(self._xrot[:, 0]),\n",
    "                                                 max(self._xrot[:, 0]),\n",
    "                                                 self._pixels[0])) - 1\n",
    "        ax1_coord = np.digitize(self._xrot[:, 1],\n",
    "                                bins=np.linspace(min(self._xrot[:, 1]),\n",
    "                                                 max(self._xrot[:, 1]),\n",
    "                                                 self._pixels[1])) - 1\n",
    "        self._coords = np.stack((ax0_coord, ax1_coord))\n",
    "\n",
    "    def transform(self, X, empty_value=0):\n",
    "        \"\"\"Transform the input matrix into image matrices\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "                where n_features matches the training set.\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        img_coords = pd.DataFrame(np.vstack(\n",
    "            (self._coords, X.clip(0, 1))).T).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False).mean()\n",
    "\n",
    "        img_matrices = []\n",
    "        blank_mat = np.zeros(self._pixels)\n",
    "        if empty_value != 0:\n",
    "            blank_mat[:] = empty_value\n",
    "        for z in range(2, img_coords.shape[1]):\n",
    "            img_matrix = blank_mat.copy()\n",
    "            img_matrix[img_coords[0].astype(int),\n",
    "                       img_coords[1].astype(int)] = img_coords[z]\n",
    "            img_matrices.append(img_matrix)\n",
    "\n",
    "        return img_matrices\n",
    "\n",
    "    def transform_3d(self, X, empty_value=0):\n",
    "        \"\"\"Transform the input matrix into image matrices\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "                where n_features matches the training set.\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        img_coords = pd.DataFrame(np.vstack(\n",
    "            (self._coords, X.clip(0, 1))).T).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False)\n",
    "        avg_img_coords = img_coords.mean()\n",
    "        min_img_coords = img_coords.min()\n",
    "        max_img_coords = img_coords.max()\n",
    "\n",
    "        img_matrices = []\n",
    "        blank_mat = np.zeros((3, self._pixels[0], self._pixels[1]))\n",
    "        if empty_value != 0:\n",
    "            blank_mat[:, :, :] = empty_value\n",
    "        for z in range(2, avg_img_coords.shape[1]):\n",
    "            img_matrix = blank_mat.copy()\n",
    "            img_matrix[0, avg_img_coords[0].astype(int),\n",
    "                       avg_img_coords[1].astype(int)] = avg_img_coords[z]\n",
    "            img_matrix[1, min_img_coords[0].astype(int),\n",
    "                       min_img_coords[1].astype(int)] = min_img_coords[z]\n",
    "            img_matrix[2, max_img_coords[0].astype(int),\n",
    "                       max_img_coords[1].astype(int)] = max_img_coords[z]\n",
    "            img_matrices.append(img_matrix)\n",
    "\n",
    "        return img_matrices\n",
    "\n",
    "    def fit_transform(self, X, empty_value=0):\n",
    "        \"\"\"Train the image transformer from the training set (X) and return\n",
    "        the transformed data.\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X, empty_value=empty_value)\n",
    "\n",
    "    def fit_transform_3d(self, X, empty_value=0):\n",
    "        \"\"\"Train the image transformer from the training set (X) and return\n",
    "        the transformed data.\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform_3d(X, empty_value=empty_value)\n",
    "\n",
    "    def feature_density_matrix(self):\n",
    "        \"\"\"Generate image matrix with feature counts per pixel\n",
    "\n",
    "        Returns:\n",
    "            img_matrix (ndarray): matrix with feature counts per pixel\n",
    "        \"\"\"\n",
    "        fdmat = np.zeros(self._pixels)\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        coord_cnt = (\n",
    "            pd.DataFrame(self._coords.T).assign(count=1).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False).count())\n",
    "        fdmat[coord_cnt[0].astype(int),\n",
    "              coord_cnt[1].astype(int)] = coord_cnt['count']\n",
    "        return fdmat\n",
    "\n",
    "    @staticmethod\n",
    "    def _minimum_bounding_rectangle(hull_points):\n",
    "        \"\"\"Find the smallest bounding rectangle for a set of points.\n",
    "\n",
    "        Modified from JesseBuesking at https://stackoverflow.com/a/33619018\n",
    "        Returns a set of points representing the corners of the bounding box.\n",
    "\n",
    "        Args:\n",
    "            hull_points : an nx2 matrix of hull coordinates\n",
    "\n",
    "        Returns:\n",
    "            (tuple): tuple containing\n",
    "                coords (ndarray): coordinates of the corners of the rectangle\n",
    "                rotmat (ndarray): rotation matrix to align edges of rectangle\n",
    "                    to x and y\n",
    "        \"\"\"\n",
    "\n",
    "        pi2 = np.pi / 2.\n",
    "\n",
    "        # Calculate edge angles\n",
    "        edges = hull_points[1:] - hull_points[:-1]\n",
    "        angles = np.arctan2(edges[:, 1], edges[:, 0])\n",
    "        angles = np.abs(np.mod(angles, pi2))\n",
    "        angles = np.unique(angles)\n",
    "\n",
    "        # Find rotation matrices\n",
    "        rotations = np.vstack([\n",
    "            np.cos(angles),\n",
    "            np.cos(angles - pi2),\n",
    "            np.cos(angles + pi2),\n",
    "            np.cos(angles)\n",
    "        ]).T\n",
    "        rotations = rotations.reshape((-1, 2, 2))\n",
    "\n",
    "        # Apply rotations to the hull\n",
    "        rot_points = np.dot(rotations, hull_points.T)\n",
    "\n",
    "        # Find the bounding points\n",
    "        min_x = np.nanmin(rot_points[:, 0], axis=1)\n",
    "        max_x = np.nanmax(rot_points[:, 0], axis=1)\n",
    "        min_y = np.nanmin(rot_points[:, 1], axis=1)\n",
    "        max_y = np.nanmax(rot_points[:, 1], axis=1)\n",
    "\n",
    "        # Find the box with the best area\n",
    "        areas = (max_x - min_x) * (max_y - min_y)\n",
    "        best_idx = np.argmin(areas)\n",
    "\n",
    "        # Return the best box\n",
    "        x1 = max_x[best_idx]\n",
    "        x2 = min_x[best_idx]\n",
    "        y1 = max_y[best_idx]\n",
    "        y2 = min_y[best_idx]\n",
    "        rotmat = rotations[best_idx]\n",
    "\n",
    "        # Generate coordinates\n",
    "        coords = np.zeros((4, 2))\n",
    "        coords[0] = np.dot([x1, y2], rotmat)\n",
    "        coords[1] = np.dot([x2, y2], rotmat)\n",
    "        coords[2] = np.dot([x2, y1], rotmat)\n",
    "        coords[3] = np.dot([x1, y1], rotmat)\n",
    "\n",
    "        return coords, rotmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LogScaler:\n",
    "    \"\"\"Log normalize and scale data\n",
    "\n",
    "    Log normalization and scaling procedure as described as norm-2 in the\n",
    "    DeepInsight paper supplementary information.\n",
    "    \n",
    "    Note: The dimensions of input matrix is (N samples, d features)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._min0 = None\n",
    "        self._max = None\n",
    "\n",
    "    \"\"\"\n",
    "    Use this as a preprocessing step in inference mode.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Min. of training set per feature\n",
    "        self._min0 = X.min(axis=0)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(min=0, max=None)\n",
    "\n",
    "        # Global max. of training set from X_norm\n",
    "        self._max = X_norm.max()\n",
    "\n",
    "    \"\"\"\n",
    "    For training set only.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        # Min. of training set per feature\n",
    "        self._min0 = X.min(axis=0)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(min=0, max=None)\n",
    "\n",
    "        # Global max. of training set from X_norm\n",
    "        self._max = X_norm.max()\n",
    "\n",
    "        # Normalized again by global max. of training set\n",
    "        return (X_norm / self._max).clip(0, 1)\n",
    "\n",
    "    \"\"\"\n",
    "    For validation and test set only.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Adjust min. of each feature of X by _min0\n",
    "        for i in range(X.shape[1]):\n",
    "            X[:, i] = X[:, i].clip(min=self._min0[i], max=None)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(min=0, max=None)\n",
    "\n",
    "        # Normalized again by global max. of training set\n",
    "        return (X_norm / self._max).clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MoAImageMultiTransformDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, tsne_transformer,\n",
    "                 kernel_pca_transformer, pca_transformer):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.tsne_transformer = tsne_transformer\n",
    "        self.kernel_pca_transformer = kernel_pca_transformer\n",
    "        self.pca_transformer = pca_transformer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=1 to follow the setup in the paper\n",
    "        image = np.zeros((normalized.shape[1], normalized.shape[1], 3))\n",
    "        # image = np.zeros((3, image_size, image_size))\n",
    "        image[:, :, 0] = self.tsne_transformer.transform(normalized,\n",
    "                                                         empty_value=1)[0]\n",
    "        image[:, :,\n",
    "              1] = self.kernel_pca_transformer.transform(normalized,\n",
    "                                                         empty_value=1)[0]\n",
    "        image[:, :, 2] = self.pca_transformer.transform(normalized,\n",
    "                                                        empty_value=1)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image, (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        # print(image.shape)\n",
    "\n",
    "        return {\"x\": image, \"y\": self.labels[index, :]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "\n",
    "class MultiTransformTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, tsne_transformer,\n",
    "                 kernel_pca_transformer, pca_transformer):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.tsne_transformer = tsne_transformer\n",
    "        self.kernel_pca_transformer = kernel_pca_transformer\n",
    "        self.pca_transformer = pca_transformer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=1 to follow the setup in the paper\n",
    "        image = np.zeros((normalized.shape[1], normalized.shape[1], 3))\n",
    "        # image = np.zeros((3, image_size, image_size))\n",
    "        image[:, :, 0] = self.tsne_transformer.transform(normalized,\n",
    "                                                         empty_value=1)[0]\n",
    "        image[:, :,\n",
    "              1] = self.kernel_pca_transformer.transform(normalized,\n",
    "                                                         empty_value=1)[0]\n",
    "        image[:, :, 2] = self.pca_transformer.transform(normalized,\n",
    "                                                        empty_value=1)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image, (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "\n",
    "        return {\"x\": image, \"y\": -1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MoAImageSwapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 features,\n",
    "                 labels,\n",
    "                 transformer,\n",
    "                 swap_prob=0.15,\n",
    "                 swap_portion=0.1):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transformer = transformer\n",
    "        self.swap_prob = swap_prob\n",
    "        self.swap_portion = swap_portion\n",
    "\n",
    "        self.crop = CropToFixedSize(width=image_size, height=image_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "\n",
    "        # Swap row featurs randomly\n",
    "        normalized = self.add_swap_noise(index, normalized)\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=1 to follow the setup in the paper\n",
    "        #         image = self.transformer.transform(normalized, empty_value=1)[0]\n",
    "        image = self.transformer.transform_3d(normalized, empty_value=0)[0]\n",
    "\n",
    "        # Crop to target size\n",
    "        #         image = (image * 255).astype(np.uint8)\n",
    "        #         image_aug = self.crop(image=image)\n",
    "        #         image = (image_aug / 255).astype(np.float32).clip(0, 1)\n",
    "\n",
    "        # Resize to target size\n",
    "        #         gene_cht = cv2.resize(image, (image_size, image_size),\n",
    "        #                               interpolation=cv2.INTER_CUBIC)\n",
    "        #         image = cv2.resize(image, (image_size, image_size),\n",
    "        #                            interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # Convert to 3 channels\n",
    "        # image = np.repeat(image[np.newaxis, :, :], 3, axis=0)\n",
    "        # image = np.repeat(gene_cht[np.newaxis, :, :], 3, axis=0)\n",
    "\n",
    "        return {\"x\": image, \"y\": self.labels[index, :]}\n",
    "\n",
    "    def add_swap_noise(self, index, X):\n",
    "        if np.random.rand() < self.swap_prob:\n",
    "            swap_index = np.random.randint(self.features.shape[0], size=1)[0]\n",
    "            # Select only gene expression and cell viability features\n",
    "            swap_features = np.random.choice(\n",
    "                np.array(range(3, self.features.shape[1])),\n",
    "                size=int(self.features.shape[1] * self.swap_portion),\n",
    "                replace=False)\n",
    "            X[swap_features] = self.features[swap_index, swap_features]\n",
    "\n",
    "        return X\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MoAImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, transformer):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transformer = transformer\n",
    "\n",
    "\n",
    "#         self.transforms = transforms.Compose([\n",
    "#             # transforms.ToPILImage(),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=1 to follow the setup in the paper\n",
    "        image = self.transformer.transform_3d(normalized, empty_value=0)[0]\n",
    "        #         image = self.transformer.transform(normalized, empty_value=0)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        #         image = cv2.resize(image, (image_size, image_size),\n",
    "        #                            interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # Convert to 3 channels\n",
    "        #         image = np.repeat(image[np.newaxis, :, :], 3, axis=0)\n",
    "\n",
    "        # Normalized again by ImageNet mean and std\n",
    "        #         image[0, :, :] -= 0.485\n",
    "        #         image[1, :, :] -= 0.456\n",
    "        #         image[2, :, :] -= 0.406\n",
    "        #         image[0, :, :] /= 0.229\n",
    "        #         image[1, :, :] /= 0.224\n",
    "        #         image[2, :, :] /= 0.225\n",
    "\n",
    "        #         image = self.transforms(image.astype(np.float32))\n",
    "\n",
    "        return {\"x\": image, \"y\": self.labels[index, :]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, transformer):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=1 to follow the setup in the paper\n",
    "        #         image = self.transformer.transform(normalized, empty_value=0)[0]\n",
    "        image = self.transformer.transform_3d(normalized, empty_value=0)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        #         image = cv2.resize(image, (image_size, image_size),\n",
    "        #                            interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # Convert to 3 channels\n",
    "        #         image = np.repeat(image[np.newaxis, :, :], 3, axis=0)\n",
    "\n",
    "        # Normalized again by ImageNet mean and std\n",
    "        #         image[0, :, :] -= 0.485\n",
    "        #         image[1, :, :] -= 0.456\n",
    "        #         image[2, :, :] -= 0.406\n",
    "        #         image[0, :, :] /= 0.229\n",
    "        #         image[1, :, :] /= 0.224\n",
    "        #         image[2, :, :] /= 0.225\n",
    "\n",
    "        return {\"x\": image, \"y\": -1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/vbmokin/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual#4.7-Smoothing\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets: torch.Tensor, n_labels: int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "                                           self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight)\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Trick to fix NaN loss in PyTorch:\n",
    "# Reference: https://www.kaggle.com/c/lish-moa/discussion/188651\n",
    "def recalibrate_layer(layer):\n",
    "\n",
    "    if (torch.isnan(layer.weight_v).sum() > 0):\n",
    "        print('recalibrate layer.weight_v')\n",
    "        layer.weight_v = torch.nn.Parameter(\n",
    "            torch.where(torch.isnan(layer.weight_v),\n",
    "                        torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "        layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "    if (torch.isnan(layer.weight).sum() > 0):\n",
    "        print('recalibrate layer.weight')\n",
    "        layer.weight = torch.where(torch.isnan(layer.weight),\n",
    "                                   torch.zeros_like(layer.weight),\n",
    "                                   layer.weight)\n",
    "        layer.weight += 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://github.com/rwightman/gen-efficientnet-pytorch/blob/master/geffnet/efficientnet_builder.py#L672\n",
    "def initialize_weight_goog(m, n='', fix_group_fanout=True):\n",
    "    # weight init as per Tensorflow Official impl\n",
    "    # https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mnasnet_model.py\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        if fix_group_fanout:\n",
    "            fan_out //= m.groups\n",
    "        m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1.0)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        fan_out = m.weight.size(0)  # fan-out\n",
    "        fan_in = 0\n",
    "        if 'routing_fn' in n:\n",
    "            fan_in = m.weight.size(1)\n",
    "        init_range = 1.0 / math.sqrt(fan_in + fan_out)\n",
    "        m.weight.data.uniform_(-init_range, init_range)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def initialize_weight_default(m, n=''):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1.0)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight,\n",
    "                                 mode='fan_in',\n",
    "                                 nonlinearity='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MoAEfficientNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_model_name,\n",
    "            training_set=(None, None),  # tuple\n",
    "            valid_set=(None, None),  # tuple\n",
    "            test_set=None,\n",
    "            transformer=None,\n",
    "            num_classes=206,\n",
    "            in_chans=3,\n",
    "            drop_rate=0.,\n",
    "            drop_connect_rate=0.,\n",
    "            fc_size=512,\n",
    "            learning_rate=1e-3,\n",
    "            weight_init='goog'):\n",
    "        super(MoAEfficientNet, self).__init__()\n",
    "\n",
    "        self.train_data, self.train_labels = training_set\n",
    "        self.valid_data, self.valid_labels = valid_set\n",
    "        self.test_data = test_set\n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.backbone = getattr(geffnet, pretrained_model)(\n",
    "            pretrained=True,\n",
    "            in_chans=in_chans,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_connect_rate=drop_connect_rate,\n",
    "            weight_init=weight_init)\n",
    "\n",
    "        #         self.backbone.classifier = nn.Sequential(\n",
    "        #             # recalibrate_layer(),\n",
    "        #             nn.Linear(self.backbone.classifier.in_features, fc_size,\n",
    "        #                       bias=True),\n",
    "        #             nn.ReLU(),\n",
    "        #             nn.Dropout(p=drop_rate),\n",
    "        #             # recalibrate_layer(),\n",
    "        #             nn.Linear(fc_size, fc_size, bias=True),\n",
    "        #             nn.ReLU(),\n",
    "        #             nn.Dropout(p=drop_rate),\n",
    "        #             nn.Linear(fc_size, num_classes, bias=True))\n",
    "\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Linear(self.backbone.classifier.in_features, fc_size,\n",
    "                      bias=True), nn.ELU(),\n",
    "            nn.Linear(fc_size, num_classes, bias=True))\n",
    "\n",
    "        if self.training:\n",
    "            for m in self.backbone.classifier.modules():\n",
    "                initialize_weight_goog(m)\n",
    "\n",
    "        # Save passed hyperparameters\n",
    "        self.save_hyperparameters(\"pretrained_model_name\", \"num_classes\",\n",
    "                                  \"in_chans\", \"drop_rate\", \"drop_connect_rate\",\n",
    "                                  \"weight_init\", \"fc_size\", \"learning_rate\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        x = x.float()\n",
    "        y = y.type_as(x)\n",
    "        logits = self(x)\n",
    "\n",
    "        # loss = F.binary_cross_entropy_with_logits(logits, y, reduction=\"mean\")\n",
    "\n",
    "        # Label smoothing\n",
    "        loss = SmoothBCEwLogits(smoothing=label_smoothing)(logits, y)\n",
    "\n",
    "        self.log('train_loss',\n",
    "                 loss,\n",
    "                 on_step=True,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        x = x.float()\n",
    "        y = y.type_as(x)\n",
    "        logits = self(x)\n",
    "\n",
    "        val_loss = F.binary_cross_entropy_with_logits(logits,\n",
    "                                                      y,\n",
    "                                                      reduction=\"mean\")\n",
    "\n",
    "        self.log('val_loss',\n",
    "                 val_loss,\n",
    "                 on_step=True,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 logger=True)\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        x = x.float()\n",
    "        y = y.type_as(x)\n",
    "        logits = self(x)\n",
    "        return {\"pred_logits\": logits}\n",
    "\n",
    "    def test_epoch_end(self, output_results):\n",
    "        all_outputs = torch.cat([out[\"pred_logits\"] for out in output_results],\n",
    "                                dim=0)\n",
    "        print(\"Logits:\", all_outputs)\n",
    "        pred_probs = F.sigmoid(all_outputs).detach().cpu().numpy()\n",
    "        print(\"Predictions: \", pred_probs)\n",
    "        return {\"pred_probs\": pred_probs}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        #         self.train_dataset = MoAImageDataset(self.train_data,\n",
    "        #                                              self.train_labels,\n",
    "        #                                              self.transformer)\n",
    "        self.train_dataset = MoAImageSwapDataset(self.train_data,\n",
    "                                                 self.train_labels,\n",
    "                                                 self.transformer,\n",
    "                                                 swap_prob=swap_prob,\n",
    "                                                 swap_portion=swap_portion)\n",
    "\n",
    "        self.val_dataset = MoAImageDataset(self.valid_data, self.valid_labels,\n",
    "                                           self.transformer)\n",
    "\n",
    "        self.test_dataset = TestDataset(self.test_data, None, self.transformer)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = DataLoader(self.train_dataset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=num_workers,\n",
    "                                      pin_memory=True,\n",
    "                                      drop_last=False)\n",
    "        print(f\"Train iterations: {len(train_dataloader)}\")\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = DataLoader(self.val_dataset,\n",
    "                                    batch_size=infer_batch_size,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=num_workers,\n",
    "                                    pin_memory=True,\n",
    "                                    drop_last=False)\n",
    "        print(f\"Validate iterations: {len(val_dataloader)}\")\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataloader = DataLoader(self.test_dataset,\n",
    "                                     batch_size=infer_batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=num_workers,\n",
    "                                     pin_memory=True,\n",
    "                                     drop_last=False)\n",
    "        print(f\"Test iterations: {len(test_dataloader)}\")\n",
    "        return test_dataloader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print(f\"Initial Learning Rate: {self.hparams.learning_rate:.6f}\")\n",
    "        #         optimizer = optim.Adam(self.parameters(),\n",
    "        #                                lr=self.hparams.learning_rate,\n",
    "        #                                weight_decay=weight_decay)\n",
    "        #         optimizer = torch.optim.SGD(self.parameters(),\n",
    "        #                                     lr=self.hparams.learning_rate,\n",
    "        #                                     momentum=0.9,\n",
    "        #                                     dampening=0,\n",
    "        #                                     weight_decay=weight_decay,\n",
    "        #                                     nesterov=False)\n",
    "\n",
    "        optimizer = torch_optimizer.RAdam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                         T_max=T_max,\n",
    "                                                         eta_min=0,\n",
    "                                                         last_epoch=-1)\n",
    "\n",
    "        #         scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        #             optimizer,\n",
    "        #             T_0=T_0,\n",
    "        #             T_mult=1,\n",
    "        #             eta_min=0,\n",
    "        #             last_epoch=-1)\n",
    "\n",
    "        #         scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        #             optimizer=optimizer,\n",
    "        #             pct_start=0.1,\n",
    "        #             div_factor=1e3,\n",
    "        #             max_lr=1e-1,\n",
    "        #             # max_lr=1e-2,\n",
    "        #             epochs=epochs,\n",
    "        #             steps_per_epoch=len(self.train_images) // batch_size)\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = MoAEfficientNet(\n",
    "#     pretrained_model,\n",
    "#     training_set=(None, None),  # tuple\n",
    "#     valid_set=(None, None),  # tuple\n",
    "#     test_set=None,\n",
    "#     transformer=None,\n",
    "#     num_classes=len(train_classes),\n",
    "#     in_chans=3,\n",
    "#     drop_rate=drop_rate,\n",
    "#     drop_connect_rate=drop_connect_rate,\n",
    "#     fc_size=fc_size,\n",
    "#     learning_rate=learning_rate,\n",
    "#     weight_init='goog')\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = 10\n",
    "skf = MultilabelStratifiedKFold(n_splits=kfolds,\n",
    "                                shuffle=True,\n",
    "                                random_state=rand_seed)\n",
    "\n",
    "label_counts = np.sum(train_labels.drop(\"sig_id\", axis=1), axis=0)\n",
    "y_labels = label_counts.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(training_set, valid_set, test_set, transformer, model_path=None):\n",
    "    if training_mode:\n",
    "        model = MoAEfficientNet(\n",
    "            pretrained_model_name=pretrained_model,\n",
    "            training_set=training_set,  # tuple\n",
    "            valid_set=valid_set,  # tuple\n",
    "            test_set=test_set,\n",
    "            transformer=transformer,\n",
    "            num_classes=len(train_classes),\n",
    "            in_chans=3,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_connect_rate=drop_connect_rate,\n",
    "            fc_size=fc_size,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_init='goog')\n",
    "    else:\n",
    "        model = MoAEfficientNet.load_from_checkpoint(\n",
    "            model_path,\n",
    "            pretrained_model_name=pretrained_model,\n",
    "            training_set=training_set,  # tuple\n",
    "            valid_set=valid_set,  # tuple\n",
    "            test_set=test_set,\n",
    "            transformer=transformer,\n",
    "            num_classes=len(train_classes),\n",
    "            fc_size=fc_size)\n",
    "        model.freeze()\n",
    "        model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_pickle(obj, model_output_folder, fold_i, name):\n",
    "    dump(obj, open(f\"{model_output_folder}/fold{fold_i}_{name}.pkl\", 'wb'),\n",
    "         pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_pickle(model_output_folder, fold_i, name):\n",
    "    return load(open(f\"{model_output_folder}/fold{fold_i}_{name}.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm2_normalization(train, valid, test):\n",
    "    scaler = LogScaler()\n",
    "    train = scaler.fit_transform(train)\n",
    "    valid = scaler.transform(valid)\n",
    "    test = scaler.transform(test)\n",
    "    return train, valid, test, scaler\n",
    "\n",
    "\n",
    "def extract_feature_map(train,\n",
    "                        feature_extractor='tsne_exact',\n",
    "                        resolution=100,\n",
    "                        perplexity=30):\n",
    "    transformer = DeepInsightTransformer(feature_extractor=feature_extractor,\n",
    "                                         pixels=resolution,\n",
    "                                         perplexity=perplexity,\n",
    "                                         random_state=rand_seed,\n",
    "                                         n_jobs=-1)\n",
    "    transformer.fit(train)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_logloss(y_pred, y_true):\n",
    "    logloss = (1 - y_true) * np.log(1 - y_pred +\n",
    "                                    1e-15) + y_true * np.log(y_pred + 1e-15)\n",
    "    return np.mean(-logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing on Fold 0 ......\n",
      "(21432,) (2382,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v7_b3/deepinsight_efficientnet_v7_b3/fold0/epoch24-train_loss_epoch0.016526-val_loss_epoch0.014525-image_size300-resolution300-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ba6c7dffa8439099c76c4177384384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ -5.9648,  -5.7891,  -6.5156,  ...,  -6.2695,  -8.2188,  -6.5938],\n",
      "        [-10.2031,  -8.5469,  -8.2656,  ...,  -8.4375,  -8.7656,  -6.9492],\n",
      "        [ -7.4727,  -7.3242,  -7.0898,  ...,  -7.5469,  -7.7734,  -6.9453],\n",
      "        ...,\n",
      "        [ -5.5820,  -6.8477,  -7.1406,  ...,  -6.8594,  -8.3125,  -6.9766],\n",
      "        [ -6.3242,  -6.1211,  -6.9609,  ...,  -6.0000,  -9.5859,  -5.2188],\n",
      "        [ -6.3438,  -6.7930,  -6.7031,  ...,  -7.0273,  -7.4766,  -7.0391]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[2.5616e-03 3.0518e-03 1.4782e-03 ... 1.8892e-03 2.6941e-04 1.3666e-03]\n",
      " [3.7074e-05 1.9407e-04 2.5725e-04 ... 2.1660e-04 1.5593e-04 9.5844e-04]\n",
      " [5.6791e-04 6.5899e-04 8.3303e-04 ... 5.2738e-04 4.2057e-04 9.6226e-04]\n",
      " ...\n",
      " [3.7498e-03 1.0605e-03 7.9155e-04 ... 1.0481e-03 2.4533e-04 9.3269e-04]\n",
      " [1.7891e-03 2.1915e-03 9.4748e-04 ... 2.4719e-03 6.8665e-05 5.3864e-03]\n",
      " [1.7548e-03 1.1206e-03 1.2255e-03 ... 8.8644e-04 5.6601e-04 8.7595e-04]]\n",
      "\n",
      "Inferencing on Fold 1 ......\n",
      "(21432,) (2382,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v7_b3/deepinsight_efficientnet_v7_b3/fold1/epoch25-train_loss_epoch0.015616-val_loss_epoch0.015010-image_size300-resolution300-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2557af9db7d7431e8835ca98fc458064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ -8.5312,  -7.8906,  -6.5664,  ...,  -5.9102,  -6.1758,  -7.2109],\n",
      "        [ -8.6406, -10.6641,  -7.2969,  ...,  -8.1406, -10.0312,  -5.2109],\n",
      "        [ -7.7969,  -8.1953,  -7.3086,  ...,  -7.6367,  -8.0625,  -7.8867],\n",
      "        ...,\n",
      "        [ -4.6719,  -6.8984,  -7.7969,  ...,  -6.6914,  -8.0312,  -5.3008],\n",
      "        [ -5.2539,  -6.4609,  -6.3984,  ...,  -5.4688,  -9.2109,  -6.8789],\n",
      "        [ -7.9883,  -7.5625,  -7.1602,  ...,  -5.5625,  -6.8672,  -6.8789]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[1.972e-04 3.741e-04 1.405e-03 ... 2.705e-03 2.075e-03 7.381e-04]\n",
      " [1.768e-04 2.337e-05 6.771e-04 ... 2.913e-04 4.399e-05 5.428e-03]\n",
      " [4.108e-04 2.759e-04 6.695e-04 ... 4.821e-04 3.150e-04 3.755e-04]\n",
      " ...\n",
      " [9.270e-03 1.008e-03 4.108e-04 ... 1.240e-03 3.250e-04 4.963e-03]\n",
      " [5.199e-03 1.561e-03 1.661e-03 ... 4.200e-03 9.996e-05 1.028e-03]\n",
      " [3.393e-04 5.193e-04 7.763e-04 ... 3.824e-03 1.040e-03 1.028e-03]]\n",
      "\n",
      "Inferencing on Fold 2 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v7_b3/deepinsight_efficientnet_v7_b3/fold2/epoch25-train_loss_epoch0.016164-val_loss_epoch0.014812-image_size300-resolution300-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b0ff16356c4b5e893079431e1cc81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-7.4023, -7.2188, -5.2383,  ..., -6.0430, -7.8086, -5.3477],\n",
      "        [-8.7266, -8.4219, -8.3125,  ..., -7.9922, -7.2227, -7.3125],\n",
      "        [-7.9688, -8.0391, -7.1367,  ..., -7.0625, -7.6641, -7.5156],\n",
      "        ...,\n",
      "        [-5.5234, -5.9570, -6.8008,  ..., -6.6758, -9.6797, -6.9180],\n",
      "        [-7.1914, -8.9922, -6.5977,  ..., -5.4219, -7.7344, -5.6406],\n",
      "        [-6.3555, -7.3008, -6.3750,  ..., -6.1562, -7.3086, -6.5234]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[6.094e-04 7.319e-04 5.280e-03 ... 2.369e-03 4.060e-04 4.738e-03]\n",
      " [1.622e-04 2.199e-04 2.453e-04 ... 3.381e-04 7.296e-04 6.666e-04]\n",
      " [3.459e-04 3.226e-04 7.949e-04 ... 8.559e-04 4.692e-04 5.441e-04]\n",
      " ...\n",
      " [3.975e-03 2.581e-03 1.112e-03 ... 1.260e-03 6.253e-05 9.890e-04]\n",
      " [7.524e-04 1.243e-04 1.362e-03 ... 4.398e-03 4.373e-04 3.538e-03]\n",
      " [1.734e-03 6.747e-04 1.700e-03 ... 2.115e-03 6.695e-04 1.467e-03]]\n",
      "\n",
      "Inferencing on Fold 3 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v7_b3/deepinsight_efficientnet_v7_b3/fold3/epoch25-train_loss_epoch0.016056-val_loss_epoch0.014747-image_size300-resolution300-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452a2f19766741e891da798d4da63324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ -7.2227,  -6.5898,  -5.8242,  ...,  -6.3555,  -6.3164,  -7.1016],\n",
      "        [-10.7344,  -9.4453,  -8.7109,  ..., -10.0781,  -8.5625,  -5.7227],\n",
      "        [ -7.7617,  -7.8984,  -7.3086,  ...,  -7.8789,  -8.0234,  -7.4023],\n",
      "        ...,\n",
      "        [ -5.2227,  -5.8906,  -7.2031,  ...,  -6.7812,  -9.5547,  -5.4375],\n",
      "        [ -5.7344,  -5.8867,  -6.0898,  ...,  -6.2031,  -8.9062,  -4.5156],\n",
      "        [ -8.3672,  -8.1641,  -6.9531,  ...,  -6.7812,  -7.0938,  -7.1016]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[7.296e-04 1.372e-03 2.947e-03 ... 1.734e-03 1.803e-03 8.230e-04]\n",
      " [2.176e-05 7.904e-05 1.647e-04 ... 4.196e-05 1.911e-04 3.260e-03]\n",
      " [4.256e-04 3.712e-04 6.695e-04 ... 3.786e-04 3.276e-04 6.094e-04]\n",
      " ...\n",
      " [5.363e-03 2.758e-03 7.439e-04 ... 1.134e-03 7.087e-05 4.330e-03]\n",
      " [3.223e-03 2.768e-03 2.260e-03 ... 2.020e-03 1.355e-04 1.082e-02]\n",
      " [2.323e-04 2.847e-04 9.546e-04 ... 1.134e-03 8.297e-04 8.230e-04]]\n",
      "\n",
      "Inferencing on Fold 4 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v7_b3/deepinsight_efficientnet_v7_b3/fold4/epoch24-train_loss_epoch0.016542-val_loss_epoch0.014812-image_size300-resolution300-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd458dc59f84085823e57ffcdee65c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-5.4023, -6.2852, -7.3555,  ..., -6.7344, -7.6016, -8.0156],\n",
      "        [-8.5547, -7.1094, -7.9023,  ..., -7.9688, -7.8789, -6.0430],\n",
      "        [-7.6211, -8.2266, -7.9492,  ..., -7.4766, -7.1523, -7.5312],\n",
      "        ...,\n",
      "        [-5.7266, -6.9805, -6.1367,  ..., -6.4844, -9.3672, -6.1172],\n",
      "        [-5.6992, -6.6992, -6.6602,  ..., -6.0430, -8.7500, -5.8242],\n",
      "        [-7.5352, -7.6328, -6.4414,  ..., -5.6953, -6.8984, -6.6445]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[4.486e-03 1.861e-03 6.385e-04 ... 1.188e-03 4.992e-04 3.302e-04]\n",
      " [1.926e-04 8.168e-04 3.698e-04 ... 3.459e-04 3.786e-04 2.369e-03]\n",
      " [4.897e-04 2.673e-04 3.529e-04 ... 5.660e-04 7.825e-04 5.360e-04]\n",
      " ...\n",
      " [3.248e-03 9.289e-04 2.157e-03 ... 1.525e-03 8.547e-05 2.199e-03]\n",
      " [3.338e-03 1.230e-03 1.279e-03 ... 2.369e-03 1.584e-04 2.947e-03]\n",
      " [5.336e-04 4.840e-04 1.592e-03 ... 3.351e-03 1.008e-03 1.300e-03]]\n",
      "\n",
      "Inferencing on Fold 5 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v7_b3/deepinsight_efficientnet_v7_b3/fold5/epoch23-train_loss_epoch0.017304-val_loss_epoch0.014848-image_size300-resolution300-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60195ecd5fff40ac8de90437349d3e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ -7.5195,  -7.1797,  -5.7695,  ...,  -5.7656,  -5.8906,  -6.6289],\n",
      "        [ -9.2266,  -8.9688,  -7.6719,  ...,  -7.7109, -12.4219,  -6.6680],\n",
      "        [ -8.0469,  -8.3359,  -7.4805,  ...,  -7.5156,  -8.4531,  -7.3125],\n",
      "        ...,\n",
      "        [ -5.4414,  -7.2773,  -8.2266,  ...,  -7.5508,  -9.5469,  -6.4297],\n",
      "        [ -7.6445,  -7.4336,  -6.3711,  ...,  -6.5156,  -9.1406,  -5.6562],\n",
      "        [ -7.6172,  -6.6367,  -6.3750,  ...,  -5.7422,  -7.8867,  -6.3633]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[5.422e-04 7.615e-04 3.111e-03 ... 3.124e-03 2.758e-03 1.320e-03]\n",
      " [9.841e-05 1.273e-04 4.656e-04 ... 4.478e-04 4.053e-06 1.269e-03]\n",
      " [3.200e-04 2.397e-04 5.636e-04 ... 5.441e-04 2.131e-04 6.666e-04]\n",
      " ...\n",
      " [4.314e-03 6.905e-04 2.673e-04 ... 5.255e-04 7.141e-05 1.611e-03]\n",
      " [4.785e-04 5.908e-04 1.707e-03 ... 1.478e-03 1.072e-04 3.483e-03]\n",
      " [4.916e-04 1.309e-03 1.700e-03 ... 3.197e-03 3.755e-04 1.720e-03]]\n",
      "\n",
      "Inferencing on Fold 6 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v7_b3/deepinsight_efficientnet_v7_b3/fold6/epoch23-train_loss_epoch0.017195-val_loss_epoch0.014892-image_size300-resolution300-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f53092b09324f3c9a52245da4456eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ -6.6211,  -6.1680,  -6.5859,  ...,  -6.0234,  -6.3281,  -7.1602],\n",
      "        [ -9.5859,  -7.7070,  -6.9180,  ...,  -8.5938, -11.2266,  -6.4531],\n",
      "        [ -7.2695,  -7.4727,  -7.5430,  ...,  -8.1797,  -8.2734,  -7.6602],\n",
      "        ...,\n",
      "        [ -6.3320,  -7.2930,  -7.2930,  ...,  -6.6562,  -7.8516,  -6.3438],\n",
      "        [ -6.6016,  -6.3555,  -6.6484,  ...,  -5.6523,  -6.9102,  -6.7188],\n",
      "        [ -6.9531,  -7.3867,  -7.8164,  ...,  -6.4062,  -9.0781,  -6.7266]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[1.3304e-03 2.0905e-03 1.3781e-03 ... 2.4147e-03 1.7824e-03 7.7629e-04]\n",
      " [6.8665e-05 4.4942e-04 9.8896e-04 ... 1.8525e-04 1.3292e-05 1.5736e-03]\n",
      " [6.9618e-04 5.6791e-04 5.2977e-04 ... 2.8014e-04 2.5511e-04 4.7112e-04]\n",
      " ...\n",
      " [1.7748e-03 6.7997e-04 6.7997e-04 ... 1.2846e-03 3.8910e-04 1.7548e-03]\n",
      " [1.3561e-03 1.7338e-03 1.2941e-03 ... 3.4962e-03 9.9659e-04 1.2064e-03]\n",
      " [9.5463e-04 6.1893e-04 4.0293e-04 ... 1.6489e-03 1.1414e-04 1.1969e-03]]\n",
      "\n",
      "Inferencing on Fold 7 ......\n",
      "(21432,) (2382,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v7_b3/deepinsight_efficientnet_v7_b3/fold7/epoch25-train_loss_epoch0.015632-val_loss_epoch0.014796-image_size300-resolution300-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f21f182bcc4187a090827222eee310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.7969, -7.1133, -6.4141,  ..., -6.2109, -7.3438, -6.5430],\n",
      "        [-9.2109, -7.9102, -5.9805,  ..., -8.8828, -8.3359, -6.7344],\n",
      "        [-7.8945, -7.7617, -7.1797,  ..., -7.6172, -8.3125, -7.8398],\n",
      "        ...,\n",
      "        [-5.3828, -7.0195, -7.8242,  ..., -6.6562, -9.4688, -6.7812],\n",
      "        [-5.9062, -6.4805, -7.1133,  ..., -4.9609, -7.1797, -6.3203],\n",
      "        [-7.4961, -7.0742, -7.2344,  ..., -6.4414, -8.9219, -7.2734]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[1.116e-03 8.135e-04 1.636e-03 ... 2.003e-03 6.461e-04 1.438e-03]\n",
      " [9.996e-05 3.669e-04 2.522e-03 ... 1.388e-04 2.397e-04 1.188e-03]\n",
      " [3.726e-04 4.256e-04 7.615e-04 ... 4.916e-04 2.453e-04 3.936e-04]\n",
      " ...\n",
      " [4.574e-03 8.936e-04 3.998e-04 ... 1.285e-03 7.725e-05 1.134e-03]\n",
      " [2.714e-03 1.531e-03 8.135e-04 ... 6.958e-03 7.615e-04 1.796e-03]\n",
      " [5.550e-04 8.459e-04 7.210e-04 ... 1.592e-03 1.334e-04 6.933e-04]]\n",
      "\n",
      "Inferencing on Fold 8 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v7_b3/deepinsight_efficientnet_v7_b3/fold8/epoch25-train_loss_epoch0.016081-val_loss_epoch0.014736-image_size300-resolution300-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c28b949e0c4ff29bbbde4f54ced228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ -7.3281,  -7.3711,  -5.9453,  ...,  -6.0273,  -6.2734,  -6.5273],\n",
      "        [ -7.0273,  -8.2422,  -8.2344,  ...,  -6.5820, -12.1172,  -7.0312],\n",
      "        [ -7.2266,  -7.9180,  -7.6406,  ...,  -8.1094,  -8.3125,  -7.9805],\n",
      "        ...,\n",
      "        [ -5.8164,  -7.2734,  -6.9609,  ...,  -7.2109,  -9.5938,  -6.3398],\n",
      "        [ -6.2969,  -8.2500,  -6.5859,  ...,  -6.3789,  -8.7734,  -5.1055],\n",
      "        [ -6.6602,  -7.8203,  -6.8359,  ...,  -6.1328,  -8.4609,  -6.4492]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[6.566e-04 6.289e-04 2.611e-03 ... 2.405e-03 1.883e-03 1.461e-03]\n",
      " [8.864e-04 2.632e-04 2.654e-04 ... 1.383e-03 5.484e-06 8.831e-04]\n",
      " [7.267e-04 3.641e-04 4.804e-04 ... 3.006e-04 2.453e-04 3.419e-04]\n",
      " ...\n",
      " [2.970e-03 6.933e-04 9.475e-04 ... 7.381e-04 6.813e-05 1.761e-03]\n",
      " [1.839e-03 2.613e-04 1.378e-03 ... 1.694e-03 1.547e-04 6.027e-03]\n",
      " [1.279e-03 4.013e-04 1.073e-03 ... 2.165e-03 2.115e-04 1.579e-03]]\n",
      "\n",
      "Inferencing on Fold 9 ......\n",
      "(21432,) (2382,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v7_b3/deepinsight_efficientnet_v7_b3/fold9/epoch24-train_loss_epoch0.016557-val_loss_epoch0.014794-image_size300-resolution300-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3685e0b3c1b94f19aba86108b8a70ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.2773, -6.4180, -4.9336,  ..., -5.1562, -5.2578, -7.1016],\n",
      "        [-8.0391, -8.4766, -6.6328,  ..., -6.0156, -7.9258, -5.4180],\n",
      "        [-7.4570, -7.5430, -7.2070,  ..., -7.4375, -8.0156, -7.5078],\n",
      "        ...,\n",
      "        [-5.5625, -6.2695, -6.9688,  ..., -7.1914, -8.9062, -7.2148],\n",
      "        [-6.7891, -7.1992, -7.0742,  ..., -6.7578, -7.9844, -6.5156],\n",
      "        [-7.5898, -7.5391, -6.4336,  ..., -5.1836, -7.1797, -6.5977]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[0.001875  0.001629  0.00715   ... 0.00573   0.00518   0.000823 ]\n",
      " [0.0003226 0.0002083 0.001315  ... 0.002434  0.0003612 0.004417 ]\n",
      " [0.000577  0.00053   0.000741  ... 0.0005884 0.0003302 0.0005484]\n",
      " ...\n",
      " [0.003824  0.001889  0.00094   ... 0.0007524 0.0001355 0.0007353]\n",
      " [0.001124  0.0007467 0.000846  ... 0.001161  0.0003407 0.001478 ]\n",
      " [0.0005054 0.0005317 0.001604  ... 0.005577  0.0007615 0.001362 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure Reproducibility\n",
    "seed_everything(rand_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "best_model = None\n",
    "oof_predictions = np.zeros((train_features.shape[0], len(train_classes)))\n",
    "kfold_submit_preds = np.zeros((test_features.shape[0], len(train_classes)))\n",
    "for i, (train_index, val_index) in enumerate(\n",
    "        skf.split(train_features, train_labels[y_labels])):\n",
    "    if training_mode:\n",
    "        print(f\"Training on Fold {i} ......\")\n",
    "        print(train_index.shape, val_index.shape)\n",
    "\n",
    "        logger = TensorBoardLogger(model_output_folder,\n",
    "                                   name=f\"fold{i}/logs\",\n",
    "                                   default_hp_metric=False)\n",
    "\n",
    "        train = train_features.loc[train_index, all_features].copy().values\n",
    "        fold_train_labels = train_labels.loc[train_index,\n",
    "                                             train_classes].copy().values\n",
    "        valid = train_features.loc[val_index, all_features].copy().values\n",
    "        fold_valid_labels = train_labels.loc[val_index,\n",
    "                                             train_classes].copy().values\n",
    "        test = test_features[all_features].copy().values\n",
    "\n",
    "        # LogScaler (Norm-2 Normalization)\n",
    "        print(\"Running norm-2 normalization ......\")\n",
    "        train, valid, test, scaler = norm2_normalization(train, valid, test)\n",
    "        save_pickle(scaler, model_output_folder, i, \"log-scaler\")\n",
    "\n",
    "        # Extract DeepInsight Feature Map\n",
    "        print(\"Extracting feature map ......\")\n",
    "        transformer = extract_feature_map(train,\n",
    "                                          feature_extractor='tsne_exact',\n",
    "                                          resolution=resolution,\n",
    "                                          perplexity=perplexity)\n",
    "        save_pickle(transformer, model_output_folder, i, \"deepinsight-transform\")\n",
    "\n",
    "        model = get_model(training_set=(train, fold_train_labels),\n",
    "                          valid_set=(valid, fold_valid_labels),\n",
    "                          test_set=test,\n",
    "                          transformer=transformer)\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss_epoch',\n",
    "                          min_delta=1e-6,\n",
    "                          patience=patience,\n",
    "                          verbose=True,\n",
    "                          mode='min',\n",
    "                          strict=True),\n",
    "            LearningRateMonitor(logging_interval='step')\n",
    "        ]\n",
    "        # https://pytorch-lightning.readthedocs.io/en/latest/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            filepath=f\"{model_output_folder}/fold{i}\" +\n",
    "            \"/{epoch}-{train_loss_epoch:.6f}-{val_loss_epoch:.6f}\" +\n",
    "            f\"-image_size={image_size}-resolution={resolution}-perplexity={perplexity}-fc={fc_size}\",\n",
    "            save_top_k=1,\n",
    "            save_weights_only=False,\n",
    "            save_last=False,\n",
    "            verbose=True,\n",
    "            monitor='val_loss_epoch',\n",
    "            mode='min',\n",
    "            prefix='')\n",
    "\n",
    "        if debug_mode:\n",
    "            # Find best LR\n",
    "            # https://pytorch-lightning.readthedocs.io/en/latest/lr_finder.html\n",
    "            trainer = Trainer(\n",
    "                gpus=[gpus[0]],\n",
    "                distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n",
    "                auto_lr_find=True,\n",
    "                benchmark=False,\n",
    "                deterministic=True,\n",
    "                logger=logger,\n",
    "                accumulate_grad_batches=accumulate_grad_batches,\n",
    "                gradient_clip_val=gradient_clip_val,\n",
    "                precision=16,\n",
    "                max_epochs=1)\n",
    "\n",
    "            # Run learning rate finder\n",
    "            lr_finder = trainer.tuner.lr_find(\n",
    "                model,\n",
    "                min_lr=1e-7,\n",
    "                max_lr=1e2,\n",
    "                num_training=500,\n",
    "                mode='exponential',\n",
    "                early_stop_threshold=10.0,\n",
    "            )\n",
    "            fig = lr_finder.plot(suggest=True)\n",
    "            fig.show()\n",
    "\n",
    "            # Pick point based on plot, or get suggestion\n",
    "            suggested_lr = lr_finder.suggestion()\n",
    "\n",
    "            # Update hparams of the model\n",
    "            model.hparams.learning_rate = suggested_lr\n",
    "            print(f\"Suggested Learning Rate: {model.hparams.learning_rate:.6f}\")\n",
    "\n",
    "            # trainer.fit(model)\n",
    "        else:\n",
    "            trainer = Trainer(\n",
    "                gpus=gpus,\n",
    "                distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n",
    "                max_epochs=epochs,\n",
    "                benchmark=False,\n",
    "                deterministic=True,\n",
    "                # fast_dev_run=True,\n",
    "                checkpoint_callback=checkpoint_callback,\n",
    "                callbacks=callbacks,\n",
    "                accumulate_grad_batches=accumulate_grad_batches,\n",
    "                gradient_clip_val=gradient_clip_val,\n",
    "                precision=16,\n",
    "                logger=logger)\n",
    "            trainer.fit(model)\n",
    "\n",
    "            # Load best model\n",
    "            seed_everything(rand_seed)\n",
    "            best_model = MoAEfficientNet.load_from_checkpoint(\n",
    "                checkpoint_callback.best_model_path,\n",
    "                pretrained_model_name=pretrained_model,\n",
    "                training_set=(train, fold_train_labels),  # tuple\n",
    "                valid_Set=(valid, fold_valid_labels),  # tuple\n",
    "                test_set=test,\n",
    "                transformer=transformer,\n",
    "                fc_size=fc_size)\n",
    "            best_model.freeze()\n",
    "\n",
    "            print(\"Predicting on validation set ......\")\n",
    "            output = trainer.test(ckpt_path=\"best\",\n",
    "                                  test_dataloaders=model.val_dataloader(),\n",
    "                                  verbose=False)[0]\n",
    "            fold_preds = output[\"pred_probs\"]\n",
    "            oof_predictions[val_index, :] = fold_preds\n",
    "\n",
    "            print(fold_preds[:5, :])\n",
    "            fold_valid_loss = mean_logloss(fold_preds, fold_valid_labels)\n",
    "            print(f\"Fold {i} Validation Loss: {fold_valid_loss:.6f}\")\n",
    "\n",
    "            # Generate submission predictions\n",
    "            print(\"Predicting on test set ......\")\n",
    "            best_model.setup()\n",
    "            output = trainer.test(best_model, verbose=False)[0]\n",
    "            submit_preds = output[\"pred_probs\"]\n",
    "            print(test_features.shape, submit_preds.shape)\n",
    "\n",
    "            kfold_submit_preds += submit_preds / kfolds\n",
    "\n",
    "        del model, trainer, train, valid, test, scaler, transformer\n",
    "    else:\n",
    "        print(f\"Inferencing on Fold {i} ......\")\n",
    "        print(train_index.shape, val_index.shape)\n",
    "\n",
    "        model_path = glob.glob(f'{model_output_folder}/fold{i}/epoch*.ckpt')[0]\n",
    "\n",
    "        test = test_features[all_features].copy().values\n",
    "\n",
    "        # Load LogScaler (Norm-2 Normalization)\n",
    "        scaler = load_pickle(f'{model_output_folder}', i, \"log-scaler\")\n",
    "        test = scaler.transform(test)\n",
    "\n",
    "        # Load DeepInsight Feature Map\n",
    "        transformer = load_pickle(f'{model_output_folder}', i,\n",
    "                                  \"deepinsight-transform\")\n",
    "\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        model = get_model(training_set=(None, None),\n",
    "                          valid_set=(None, None),\n",
    "                          test_set=test,\n",
    "                          transformer=transformer,\n",
    "                          model_path=model_path)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            logger=False,\n",
    "            gpus=gpus,\n",
    "            distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n",
    "            precision=16,\n",
    "            benchmark=False,\n",
    "            deterministic=True)\n",
    "        output = trainer.test(model, verbose=False)[0]\n",
    "        submit_preds = output[\"pred_probs\"]\n",
    "        kfold_submit_preds += submit_preds / kfolds\n",
    "\n",
    "        del model, trainer, scaler, transformer, test\n",
    "\n",
    "    if debug_mode:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF Validation Loss: 0.014802\n"
     ]
    }
   ],
   "source": [
    "if training_mode:\n",
    "    print(oof_predictions.shape)\n",
    "else:\n",
    "    oof_predictions = glob.glob(f'{model_output_folder}/../oof_*.npy')[0]\n",
    "    oof_predictions = np.load(oof_predictions)\n",
    "\n",
    "oof_loss = mean_logloss(oof_predictions,\n",
    "                        train_labels[train_classes].values)\n",
    "print(f\"OOF Validation Loss: {oof_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Avg/Min/Max Channels]\n",
    "# OOF Validation Loss: 0.014802\n",
    "# \"drop_connect_rate\":     0.2\n",
    "# \"drop_rate\":             0.3\n",
    "# \"fc_size\":               512\n",
    "# \"in_chans\":              3\n",
    "# \"learning_rate\":         0.000352\n",
    "# \"num_classes\":           206\n",
    "# \"pretrained_model_name\": tf_efficientnet_b3_ns\n",
    "# \"weight_init\":           goog\n",
    "# (23814, 206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ELU]\n",
    "# OOF Validation Loss: 0.014932\n",
    "# \"drop_connect_rate\":     0.2\n",
    "# \"drop_rate\":             0.3\n",
    "# \"fc_size\":               512\n",
    "# \"in_chans\":              3\n",
    "# \"learning_rate\":         0.000282\n",
    "# \"num_classes\":           206\n",
    "# \"pretrained_model_name\": tf_efficientnet_b3_ns\n",
    "# \"weight_init\":           goog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [SELU]\n",
    "# OOF Validation Loss: 0.014948\n",
    "# \"drop_connect_rate\":     0.2\n",
    "# \"drop_rate\":             0.3\n",
    "# \"fc_size\":               512\n",
    "# \"in_chans\":              3\n",
    "# \"learning_rate\":         0.000282\n",
    "# \"num_classes\":           206\n",
    "# \"pretrained_model_name\": tf_efficientnet_b3_ns\n",
    "# \"weight_init\":           goog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_mode and best_model is not None:\n",
    "    print(best_model.hparams)\n",
    "    extra_params = {\n",
    "        \"gpus\": len(gpus),\n",
    "        # \"pos_weight\": True\n",
    "    }\n",
    "    exp_logger.experiment.add_hparams(hparam_dict={\n",
    "        **dict(best_model.hparams),\n",
    "        **extra_params\n",
    "    },\n",
    "                                      metric_dict={\"oof_loss\": oof_loss})\n",
    "\n",
    "    oof_filename = \"_\".join(\n",
    "        [f\"{k}={v}\" for k, v in dict(best_model.hparams).items()])\n",
    "    with open(f'oof_{experiment_name}_{oof_loss}.npy', 'wb') as f:\n",
    "        np.save(f, oof_predictions)\n",
    "\n",
    "    with open(f'oof_{experiment_name}_{oof_loss}.npy', 'rb') as f:\n",
    "        tmp = np.load(f)\n",
    "        print(tmp.shape)\n",
    "\n",
    "    # Rename model filename to remove `=` for Kaggle Dataset rule\n",
    "    model_files = glob.glob(f'{model_output_folder}/fold*/epoch*.ckpt')\n",
    "    for f in model_files:\n",
    "        new_filename = f.replace(\"=\", \"\")\n",
    "        os.rename(f, new_filename)\n",
    "        print(new_filename)\n",
    "\n",
    "    del best_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3982, 206)\n"
     ]
    }
   ],
   "source": [
    "print(kfold_submit_preds.shape)\n",
    "\n",
    "submission = pd.DataFrame(data=test_features[\"sig_id\"].values,\n",
    "                          columns=[\"sig_id\"])\n",
    "submission = submission.reindex(columns=[\"sig_id\"] + train_classes)\n",
    "submission[train_classes] = kfold_submit_preds\n",
    "# Set control type to 0 as control perturbations have no MoAs\n",
    "submission.loc[test_features['cp_type'] == 0, submission.columns[1:]] = 0\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "submission.to_csv('submission_effnet_v7_b3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>adrenergic_receptor_antagonist</th>\n",
       "      <th>akt_inhibitor</th>\n",
       "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
       "      <th>alk_inhibitor</th>\n",
       "      <th>ampk_activator</th>\n",
       "      <th>analgesic</th>\n",
       "      <th>androgen_receptor_agonist</th>\n",
       "      <th>androgen_receptor_antagonist</th>\n",
       "      <th>anesthetic_-_local</th>\n",
       "      <th>angiogenesis_inhibitor</th>\n",
       "      <th>angiotensin_receptor_antagonist</th>\n",
       "      <th>anti-inflammatory</th>\n",
       "      <th>antiarrhythmic</th>\n",
       "      <th>antibiotic</th>\n",
       "      <th>anticonvulsant</th>\n",
       "      <th>antifungal</th>\n",
       "      <th>antihistamine</th>\n",
       "      <th>antimalarial</th>\n",
       "      <th>antioxidant</th>\n",
       "      <th>antiprotozoal</th>\n",
       "      <th>antiviral</th>\n",
       "      <th>apoptosis_stimulant</th>\n",
       "      <th>aromatase_inhibitor</th>\n",
       "      <th>atm_kinase_inhibitor</th>\n",
       "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
       "      <th>atp_synthase_inhibitor</th>\n",
       "      <th>atpase_inhibitor</th>\n",
       "      <th>atr_kinase_inhibitor</th>\n",
       "      <th>aurora_kinase_inhibitor</th>\n",
       "      <th>autotaxin_inhibitor</th>\n",
       "      <th>bacterial_30s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_50s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_antifolate</th>\n",
       "      <th>bacterial_cell_wall_synthesis_inhibitor</th>\n",
       "      <th>bacterial_dna_gyrase_inhibitor</th>\n",
       "      <th>bacterial_dna_inhibitor</th>\n",
       "      <th>bacterial_membrane_integrity_inhibitor</th>\n",
       "      <th>bcl_inhibitor</th>\n",
       "      <th>bcr-abl_inhibitor</th>\n",
       "      <th>benzodiazepine_receptor_agonist</th>\n",
       "      <th>beta_amyloid_inhibitor</th>\n",
       "      <th>bromodomain_inhibitor</th>\n",
       "      <th>btk_inhibitor</th>\n",
       "      <th>calcineurin_inhibitor</th>\n",
       "      <th>calcium_channel_blocker</th>\n",
       "      <th>cannabinoid_receptor_agonist</th>\n",
       "      <th>cannabinoid_receptor_antagonist</th>\n",
       "      <th>carbonic_anhydrase_inhibitor</th>\n",
       "      <th>casein_kinase_inhibitor</th>\n",
       "      <th>caspase_activator</th>\n",
       "      <th>catechol_o_methyltransferase_inhibitor</th>\n",
       "      <th>cc_chemokine_receptor_antagonist</th>\n",
       "      <th>cck_receptor_antagonist</th>\n",
       "      <th>cdk_inhibitor</th>\n",
       "      <th>chelating_agent</th>\n",
       "      <th>chk_inhibitor</th>\n",
       "      <th>chloride_channel_blocker</th>\n",
       "      <th>cholesterol_inhibitor</th>\n",
       "      <th>cholinergic_receptor_antagonist</th>\n",
       "      <th>coagulation_factor_inhibitor</th>\n",
       "      <th>corticosteroid_agonist</th>\n",
       "      <th>cyclooxygenase_inhibitor</th>\n",
       "      <th>cytochrome_p450_inhibitor</th>\n",
       "      <th>dihydrofolate_reductase_inhibitor</th>\n",
       "      <th>dipeptidyl_peptidase_inhibitor</th>\n",
       "      <th>diuretic</th>\n",
       "      <th>dna_alkylating_agent</th>\n",
       "      <th>dna_inhibitor</th>\n",
       "      <th>dopamine_receptor_agonist</th>\n",
       "      <th>dopamine_receptor_antagonist</th>\n",
       "      <th>egfr_inhibitor</th>\n",
       "      <th>elastase_inhibitor</th>\n",
       "      <th>erbb2_inhibitor</th>\n",
       "      <th>estrogen_receptor_agonist</th>\n",
       "      <th>estrogen_receptor_antagonist</th>\n",
       "      <th>faah_inhibitor</th>\n",
       "      <th>farnesyltransferase_inhibitor</th>\n",
       "      <th>fatty_acid_receptor_agonist</th>\n",
       "      <th>fgfr_inhibitor</th>\n",
       "      <th>flt3_inhibitor</th>\n",
       "      <th>focal_adhesion_kinase_inhibitor</th>\n",
       "      <th>free_radical_scavenger</th>\n",
       "      <th>fungal_squalene_epoxidase_inhibitor</th>\n",
       "      <th>gaba_receptor_agonist</th>\n",
       "      <th>gaba_receptor_antagonist</th>\n",
       "      <th>gamma_secretase_inhibitor</th>\n",
       "      <th>glucocorticoid_receptor_agonist</th>\n",
       "      <th>glutamate_inhibitor</th>\n",
       "      <th>glutamate_receptor_agonist</th>\n",
       "      <th>glutamate_receptor_antagonist</th>\n",
       "      <th>gonadotropin_receptor_agonist</th>\n",
       "      <th>gsk_inhibitor</th>\n",
       "      <th>hcv_inhibitor</th>\n",
       "      <th>hdac_inhibitor</th>\n",
       "      <th>histamine_receptor_agonist</th>\n",
       "      <th>histamine_receptor_antagonist</th>\n",
       "      <th>histone_lysine_demethylase_inhibitor</th>\n",
       "      <th>histone_lysine_methyltransferase_inhibitor</th>\n",
       "      <th>hiv_inhibitor</th>\n",
       "      <th>hmgcr_inhibitor</th>\n",
       "      <th>hsp_inhibitor</th>\n",
       "      <th>igf-1_inhibitor</th>\n",
       "      <th>ikk_inhibitor</th>\n",
       "      <th>imidazoline_receptor_agonist</th>\n",
       "      <th>immunosuppressant</th>\n",
       "      <th>insulin_secretagogue</th>\n",
       "      <th>insulin_sensitizer</th>\n",
       "      <th>integrin_inhibitor</th>\n",
       "      <th>jak_inhibitor</th>\n",
       "      <th>kit_inhibitor</th>\n",
       "      <th>laxative</th>\n",
       "      <th>leukotriene_inhibitor</th>\n",
       "      <th>leukotriene_receptor_antagonist</th>\n",
       "      <th>lipase_inhibitor</th>\n",
       "      <th>lipoxygenase_inhibitor</th>\n",
       "      <th>lxr_agonist</th>\n",
       "      <th>mdm_inhibitor</th>\n",
       "      <th>mek_inhibitor</th>\n",
       "      <th>membrane_integrity_inhibitor</th>\n",
       "      <th>mineralocorticoid_receptor_antagonist</th>\n",
       "      <th>monoacylglycerol_lipase_inhibitor</th>\n",
       "      <th>monoamine_oxidase_inhibitor</th>\n",
       "      <th>monopolar_spindle_1_kinase_inhibitor</th>\n",
       "      <th>mtor_inhibitor</th>\n",
       "      <th>mucolytic_agent</th>\n",
       "      <th>neuropeptide_receptor_antagonist</th>\n",
       "      <th>nfkb_inhibitor</th>\n",
       "      <th>nicotinic_receptor_agonist</th>\n",
       "      <th>nitric_oxide_donor</th>\n",
       "      <th>nitric_oxide_production_inhibitor</th>\n",
       "      <th>nitric_oxide_synthase_inhibitor</th>\n",
       "      <th>norepinephrine_reuptake_inhibitor</th>\n",
       "      <th>nrf2_activator</th>\n",
       "      <th>opioid_receptor_agonist</th>\n",
       "      <th>opioid_receptor_antagonist</th>\n",
       "      <th>orexin_receptor_antagonist</th>\n",
       "      <th>p38_mapk_inhibitor</th>\n",
       "      <th>p-glycoprotein_inhibitor</th>\n",
       "      <th>parp_inhibitor</th>\n",
       "      <th>pdgfr_inhibitor</th>\n",
       "      <th>pdk_inhibitor</th>\n",
       "      <th>phosphodiesterase_inhibitor</th>\n",
       "      <th>phospholipase_inhibitor</th>\n",
       "      <th>pi3k_inhibitor</th>\n",
       "      <th>pkc_inhibitor</th>\n",
       "      <th>potassium_channel_activator</th>\n",
       "      <th>potassium_channel_antagonist</th>\n",
       "      <th>ppar_receptor_agonist</th>\n",
       "      <th>ppar_receptor_antagonist</th>\n",
       "      <th>progesterone_receptor_agonist</th>\n",
       "      <th>progesterone_receptor_antagonist</th>\n",
       "      <th>prostaglandin_inhibitor</th>\n",
       "      <th>prostanoid_receptor_antagonist</th>\n",
       "      <th>proteasome_inhibitor</th>\n",
       "      <th>protein_kinase_inhibitor</th>\n",
       "      <th>protein_phosphatase_inhibitor</th>\n",
       "      <th>protein_synthesis_inhibitor</th>\n",
       "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
       "      <th>radiopaque_medium</th>\n",
       "      <th>raf_inhibitor</th>\n",
       "      <th>ras_gtpase_inhibitor</th>\n",
       "      <th>retinoid_receptor_agonist</th>\n",
       "      <th>retinoid_receptor_antagonist</th>\n",
       "      <th>rho_associated_kinase_inhibitor</th>\n",
       "      <th>ribonucleoside_reductase_inhibitor</th>\n",
       "      <th>rna_polymerase_inhibitor</th>\n",
       "      <th>serotonin_receptor_agonist</th>\n",
       "      <th>serotonin_receptor_antagonist</th>\n",
       "      <th>serotonin_reuptake_inhibitor</th>\n",
       "      <th>sigma_receptor_agonist</th>\n",
       "      <th>sigma_receptor_antagonist</th>\n",
       "      <th>smoothened_receptor_antagonist</th>\n",
       "      <th>sodium_channel_inhibitor</th>\n",
       "      <th>sphingosine_receptor_agonist</th>\n",
       "      <th>src_inhibitor</th>\n",
       "      <th>steroid</th>\n",
       "      <th>syk_inhibitor</th>\n",
       "      <th>tachykinin_antagonist</th>\n",
       "      <th>tgf-beta_receptor_inhibitor</th>\n",
       "      <th>thrombin_inhibitor</th>\n",
       "      <th>thymidylate_synthase_inhibitor</th>\n",
       "      <th>tlr_agonist</th>\n",
       "      <th>tlr_antagonist</th>\n",
       "      <th>tnf_inhibitor</th>\n",
       "      <th>topoisomerase_inhibitor</th>\n",
       "      <th>transient_receptor_potential_channel_antagonist</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.002763</td>\n",
       "      <td>0.021013</td>\n",
       "      <td>0.029714</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.004226</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.006852</td>\n",
       "      <td>0.005753</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.008446</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>0.011519</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.012876</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.004460</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.052696</td>\n",
       "      <td>0.007150</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>0.024621</td>\n",
       "      <td>0.005350</td>\n",
       "      <td>0.015176</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.019696</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.012978</td>\n",
       "      <td>0.011431</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.009080</td>\n",
       "      <td>0.036030</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.004323</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.005648</td>\n",
       "      <td>0.012301</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.007170</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.020555</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.006456</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.017517</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.009744</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.003584</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.005486</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>0.016048</td>\n",
       "      <td>0.011812</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.030887</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.001381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.032429</td>\n",
       "      <td>0.007581</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.037927</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.005781</td>\n",
       "      <td>0.015054</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.019541</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.010068</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.001766</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.007205</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>0.013439</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.030998</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>0.021335</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.004560</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.006865</td>\n",
       "      <td>0.006427</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.011556</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.067902</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.008922</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.034004</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.010801</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.002201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.018807</td>\n",
       "      <td>0.015285</td>\n",
       "      <td>0.005960</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.003886</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.013941</td>\n",
       "      <td>0.054539</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.001872</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.009678</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.048854</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.003584</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.014560</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.005781</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.005940</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.005235</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.023050</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.014186</td>\n",
       "      <td>0.011093</td>\n",
       "      <td>0.066280</td>\n",
       "      <td>0.006345</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>0.018996</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.004851</td>\n",
       "      <td>0.039271</td>\n",
       "      <td>0.005564</td>\n",
       "      <td>0.011026</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>0.002689</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.002831</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>0.004616</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.005070</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.002314</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.010987</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.008718</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.010304</td>\n",
       "      <td>0.003329</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.003013</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.011595</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.020440</td>\n",
       "      <td>0.057859</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.006485</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.009968</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.004583</td>\n",
       "      <td>0.020798</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.003052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.018285</td>\n",
       "      <td>0.025038</td>\n",
       "      <td>0.003655</td>\n",
       "      <td>0.006521</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.014745</td>\n",
       "      <td>0.027953</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.005615</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.011994</td>\n",
       "      <td>0.004579</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.008012</td>\n",
       "      <td>0.007066</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>0.008518</td>\n",
       "      <td>0.016233</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.007546</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.005801</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.005523</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.004037</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.020639</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.017368</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>0.014142</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.008818</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.009581</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.011555</td>\n",
       "      <td>0.026131</td>\n",
       "      <td>0.003004</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>0.010849</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.002255</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.004102</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.009862</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.004851</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.003760</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.005402</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.009492</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.004839</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.003920</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.011775</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.008366</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.013573</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.001407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>id_ff7004b87</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.004240</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.026530</td>\n",
       "      <td>0.004456</td>\n",
       "      <td>0.112350</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.027340</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.011043</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.009438</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.008127</td>\n",
       "      <td>0.008068</td>\n",
       "      <td>0.013776</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.020919</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.023981</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.003751</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.010463</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.018441</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>0.483528</td>\n",
       "      <td>0.008911</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>id_ff925dd0d</td>\n",
       "      <td>0.007646</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.013394</td>\n",
       "      <td>0.025722</td>\n",
       "      <td>0.006499</td>\n",
       "      <td>0.007081</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.011941</td>\n",
       "      <td>0.025998</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>0.005382</td>\n",
       "      <td>0.002436</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>0.006891</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.004512</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>0.004691</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.004622</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.007945</td>\n",
       "      <td>0.005015</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.021463</td>\n",
       "      <td>0.009683</td>\n",
       "      <td>0.009197</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.006917</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.002851</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.021324</td>\n",
       "      <td>0.007050</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>0.005760</td>\n",
       "      <td>0.013443</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.013488</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.023382</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.002360</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.008076</td>\n",
       "      <td>0.012291</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.006742</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.006172</td>\n",
       "      <td>0.002390</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.021214</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.004564</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>0.006735</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>0.007983</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.008997</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.003127</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>id_ffb710450</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.013572</td>\n",
       "      <td>0.031303</td>\n",
       "      <td>0.008858</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.018338</td>\n",
       "      <td>0.027806</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>0.008036</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.005906</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.005684</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.004111</td>\n",
       "      <td>0.009140</td>\n",
       "      <td>0.009442</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.004699</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.021199</td>\n",
       "      <td>0.005352</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.004250</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.011886</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>0.015154</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.012508</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>0.021841</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.004924</td>\n",
       "      <td>0.009207</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.005852</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.006621</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.009528</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.005523</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.008010</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>0.024664</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.002621</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.002041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>id_ffbb869f2</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.032742</td>\n",
       "      <td>0.028802</td>\n",
       "      <td>0.004313</td>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.034145</td>\n",
       "      <td>0.043869</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>0.006215</td>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.003302</td>\n",
       "      <td>0.004512</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>0.012147</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>0.006529</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.013127</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.011153</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.015376</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.019773</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.024820</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>0.010428</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.030124</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.016248</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.005889</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>0.003015</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.016132</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.009681</td>\n",
       "      <td>0.003748</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.009628</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.016432</td>\n",
       "      <td>0.011804</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.009422</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.003771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>id_ffd5800b6</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.021114</td>\n",
       "      <td>0.018723</td>\n",
       "      <td>0.004641</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.011607</td>\n",
       "      <td>0.027705</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.005713</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.004984</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.006695</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.005757</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.008738</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.009720</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.006223</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.044420</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.014725</td>\n",
       "      <td>0.007521</td>\n",
       "      <td>0.019921</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.007748</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>0.019987</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.005013</td>\n",
       "      <td>0.026471</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>0.012351</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.005790</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.003621</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.009080</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.003091</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.007340</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.016059</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.009768</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.007876</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>0.022006</td>\n",
       "      <td>0.018463</td>\n",
       "      <td>0.004341</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.012824</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.001204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3982 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0     id_0004d9e33                     0.001410                0.001331   \n",
       "1     id_001897cda                     0.000207                0.000275   \n",
       "2     id_002429b5b                     0.000000                0.000000   \n",
       "3     id_00276f245                     0.000656                0.000545   \n",
       "4     id_0027f1083                     0.004949                0.001544   \n",
       "...            ...                          ...                     ...   \n",
       "3977  id_ff7004b87                     0.000242                0.000647   \n",
       "3978  id_ff925dd0d                     0.007646                0.002564   \n",
       "3979  id_ffb710450                     0.004306                0.001318   \n",
       "3980  id_ffbb869f2                     0.002181                0.001274   \n",
       "3981  id_ffd5800b6                     0.000838                0.000679   \n",
       "\n",
       "      acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0           0.002763                        0.021013   \n",
       "1           0.000727                        0.000354   \n",
       "2           0.000000                        0.000000   \n",
       "3           0.001844                        0.018807   \n",
       "4           0.001846                        0.018285   \n",
       "...              ...                             ...   \n",
       "3977        0.000574                        0.001658   \n",
       "3978        0.001191                        0.013394   \n",
       "3979        0.000845                        0.013572   \n",
       "3980        0.001355                        0.032742   \n",
       "3981        0.001175                        0.021114   \n",
       "\n",
       "      acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                              0.029714                        0.003917   \n",
       "1                              0.000247                        0.000757   \n",
       "2                              0.000000                        0.000000   \n",
       "3                              0.015285                        0.005960   \n",
       "4                              0.025038                        0.003655   \n",
       "...                                 ...                             ...   \n",
       "3977                           0.001284                        0.001040   \n",
       "3978                           0.025722                        0.006499   \n",
       "3979                           0.031303                        0.008858   \n",
       "3980                           0.028802                        0.004313   \n",
       "3981                           0.018723                        0.004641   \n",
       "\n",
       "      adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                       0.003427                       0.004226   \n",
       "1                       0.001517                       0.006504   \n",
       "2                       0.000000                       0.000000   \n",
       "3                       0.003156                       0.003886   \n",
       "4                       0.006521                       0.001211   \n",
       "...                          ...                            ...   \n",
       "3977                    0.000720                       0.001612   \n",
       "3978                    0.007081                       0.002212   \n",
       "3979                    0.004541                       0.001859   \n",
       "3980                    0.010413                       0.002082   \n",
       "3981                    0.006495                       0.004287   \n",
       "\n",
       "      adenylyl_cyclase_activator  adrenergic_receptor_agonist  \\\n",
       "0                       0.000233                     0.009398   \n",
       "1                       0.032429                     0.007581   \n",
       "2                       0.000000                     0.000000   \n",
       "3                       0.000490                     0.013941   \n",
       "4                       0.000262                     0.014745   \n",
       "...                          ...                          ...   \n",
       "3977                    0.000762                     0.001395   \n",
       "3978                    0.000332                     0.011941   \n",
       "3979                    0.000267                     0.018338   \n",
       "3980                    0.000773                     0.034145   \n",
       "3981                    0.000377                     0.011607   \n",
       "\n",
       "      adrenergic_receptor_antagonist  akt_inhibitor  \\\n",
       "0                           0.017592       0.000798   \n",
       "1                           0.002893       0.000696   \n",
       "2                           0.000000       0.000000   \n",
       "3                           0.054539       0.002031   \n",
       "4                           0.027953       0.000914   \n",
       "...                              ...            ...   \n",
       "3977                        0.001131       0.000853   \n",
       "3978                        0.025998       0.000967   \n",
       "3979                        0.027806       0.000423   \n",
       "3980                        0.043869       0.000811   \n",
       "3981                        0.027705       0.000570   \n",
       "\n",
       "      aldehyde_dehydrogenase_inhibitor  alk_inhibitor  ampk_activator  \\\n",
       "0                             0.000478       0.000292        0.001291   \n",
       "1                             0.000095       0.037927        0.000290   \n",
       "2                             0.000000       0.000000        0.000000   \n",
       "3                             0.000333       0.003306        0.001393   \n",
       "4                             0.000463       0.000424        0.002107   \n",
       "...                                ...            ...             ...   \n",
       "3977                          0.000929       0.001494        0.000591   \n",
       "3978                          0.000357       0.000191        0.001172   \n",
       "3979                          0.000291       0.000328        0.001033   \n",
       "3980                          0.000349       0.000706        0.001407   \n",
       "3981                          0.000610       0.000460        0.001469   \n",
       "\n",
       "      analgesic  androgen_receptor_agonist  androgen_receptor_antagonist  \\\n",
       "0      0.001392                   0.002384                      0.006852   \n",
       "1      0.000244                   0.000287                      0.001070   \n",
       "2      0.000000                   0.000000                      0.000000   \n",
       "3      0.002184                   0.001872                      0.004073   \n",
       "4      0.001794                   0.002243                      0.005615   \n",
       "...         ...                        ...                           ...   \n",
       "3977   0.000402                   0.000295                      0.001854   \n",
       "3978   0.001155                   0.004307                      0.006860   \n",
       "3979   0.000895                   0.004788                      0.008036   \n",
       "3980   0.002289                   0.003452                      0.006215   \n",
       "3981   0.000952                   0.001770                      0.004035   \n",
       "\n",
       "      anesthetic_-_local  angiogenesis_inhibitor  \\\n",
       "0               0.005753                0.001772   \n",
       "1               0.000975                0.002586   \n",
       "2               0.000000                0.000000   \n",
       "3               0.003622                0.003434   \n",
       "4               0.003923                0.001883   \n",
       "...                  ...                     ...   \n",
       "3977            0.001854                0.000912   \n",
       "3978            0.005382                0.002436   \n",
       "3979            0.007152                0.001585   \n",
       "3980            0.002906                0.001439   \n",
       "3981            0.005713                0.002153   \n",
       "\n",
       "      angiotensin_receptor_antagonist  anti-inflammatory  antiarrhythmic  \\\n",
       "0                            0.004321           0.003705        0.000664   \n",
       "1                            0.001986           0.000145        0.000407   \n",
       "2                            0.000000           0.000000        0.000000   \n",
       "3                            0.001334           0.003539        0.000689   \n",
       "4                            0.003711           0.005565        0.000827   \n",
       "...                               ...                ...             ...   \n",
       "3977                         0.000355           0.000209        0.000661   \n",
       "3978                         0.001933           0.006891        0.000979   \n",
       "3979                         0.002508           0.005906        0.000855   \n",
       "3980                         0.002471           0.004320        0.000631   \n",
       "3981                         0.001850           0.004984        0.000574   \n",
       "\n",
       "      antibiotic  anticonvulsant  antifungal  antihistamine  antimalarial  \\\n",
       "0       0.002724        0.000769    0.000493       0.000975      0.000864   \n",
       "1       0.000180        0.000479    0.001346       0.000353      0.000172   \n",
       "2       0.000000        0.000000    0.000000       0.000000      0.000000   \n",
       "3       0.002870        0.001314    0.000623       0.002200      0.002773   \n",
       "4       0.003910        0.000731    0.000505       0.000625      0.000898   \n",
       "...          ...             ...         ...            ...           ...   \n",
       "3977    0.000456        0.000651    0.001271       0.000851      0.001071   \n",
       "3978    0.004512        0.001615    0.001161       0.000571      0.000651   \n",
       "3979    0.001615        0.001064    0.000621       0.000503      0.000578   \n",
       "3980    0.003099        0.000959    0.000851       0.000772      0.000683   \n",
       "3981    0.003036        0.000939    0.001357       0.001588      0.000883   \n",
       "\n",
       "      antioxidant  antiprotozoal  antiviral  apoptosis_stimulant  \\\n",
       "0        0.005169       0.003434   0.001811             0.003851   \n",
       "1        0.000799       0.000524   0.000475             0.000565   \n",
       "2        0.000000       0.000000   0.000000             0.000000   \n",
       "3        0.009678       0.002593   0.002135             0.002229   \n",
       "4        0.011994       0.004579   0.002551             0.002106   \n",
       "...           ...            ...        ...                  ...   \n",
       "3977     0.001182       0.000280   0.000282             0.004240   \n",
       "3978     0.005570       0.007030   0.004691             0.001853   \n",
       "3979     0.004905       0.004056   0.002849             0.001696   \n",
       "3980     0.011441       0.003134   0.001812             0.001025   \n",
       "3981     0.006695       0.003316   0.002237             0.002123   \n",
       "\n",
       "      aromatase_inhibitor  atm_kinase_inhibitor  \\\n",
       "0                0.003899              0.000272   \n",
       "1                0.000256              0.001457   \n",
       "2                0.000000              0.000000   \n",
       "3                0.001662              0.001201   \n",
       "4                0.003661              0.000321   \n",
       "...                   ...                   ...   \n",
       "3977             0.000442              0.000559   \n",
       "3978             0.004146              0.000311   \n",
       "3979             0.003336              0.000319   \n",
       "3980             0.003141              0.000457   \n",
       "3981             0.002620              0.000450   \n",
       "\n",
       "      atp-sensitive_potassium_channel_antagonist  atp_synthase_inhibitor  \\\n",
       "0                                       0.000469                0.000442   \n",
       "1                                       0.000353                0.000081   \n",
       "2                                       0.000000                0.000000   \n",
       "3                                       0.001019                0.000413   \n",
       "4                                       0.000632                0.000605   \n",
       "...                                          ...                     ...   \n",
       "3977                                    0.001354                0.000102   \n",
       "3978                                    0.000672                0.000553   \n",
       "3979                                    0.000517                0.000450   \n",
       "3980                                    0.000541                0.000392   \n",
       "3981                                    0.000596                0.000198   \n",
       "\n",
       "      atpase_inhibitor  atr_kinase_inhibitor  aurora_kinase_inhibitor  \\\n",
       "0             0.002867              0.000394                 0.000542   \n",
       "1             0.000986              0.005781                 0.015054   \n",
       "2             0.000000              0.000000                 0.000000   \n",
       "3             0.003789              0.000389                 0.000867   \n",
       "4             0.001829              0.000255                 0.000267   \n",
       "...                ...                   ...                      ...   \n",
       "3977          0.026530              0.004456                 0.112350   \n",
       "3978          0.004622              0.000225                 0.000288   \n",
       "3979          0.001839              0.000153                 0.000158   \n",
       "3980          0.001528              0.000380                 0.000230   \n",
       "3981          0.005757              0.000547                 0.002128   \n",
       "\n",
       "      autotaxin_inhibitor  bacterial_30s_ribosomal_subunit_inhibitor  \\\n",
       "0                0.000394                                   0.007653   \n",
       "1                0.004853                                   0.000347   \n",
       "2                0.000000                                   0.000000   \n",
       "3                0.001585                                   0.002809   \n",
       "4                0.000523                                   0.008012   \n",
       "...                   ...                                        ...   \n",
       "3977             0.000345                                   0.000924   \n",
       "3978             0.000451                                   0.007945   \n",
       "3979             0.000470                                   0.005684   \n",
       "3980             0.001156                                   0.003302   \n",
       "3981             0.000477                                   0.002634   \n",
       "\n",
       "      bacterial_50s_ribosomal_subunit_inhibitor  bacterial_antifolate  \\\n",
       "0                                      0.008446              0.001736   \n",
       "1                                      0.000330              0.000138   \n",
       "2                                      0.000000              0.000000   \n",
       "3                                      0.002080              0.002321   \n",
       "4                                      0.007066              0.003777   \n",
       "...                                         ...                   ...   \n",
       "3977                                   0.000306              0.000233   \n",
       "3978                                   0.005015              0.003276   \n",
       "3979                                   0.004120              0.004111   \n",
       "3980                                   0.004512              0.003261   \n",
       "3981                                   0.002539              0.001148   \n",
       "\n",
       "      bacterial_cell_wall_synthesis_inhibitor  bacterial_dna_gyrase_inhibitor  \\\n",
       "0                                    0.013596                        0.009836   \n",
       "1                                    0.000514                        0.000306   \n",
       "2                                    0.000000                        0.000000   \n",
       "3                                    0.016997                        0.003280   \n",
       "4                                    0.008518                        0.016233   \n",
       "...                                       ...                             ...   \n",
       "3977                                 0.000334                        0.000278   \n",
       "3978                                 0.021463                        0.009683   \n",
       "3979                                 0.009140                        0.009442   \n",
       "3980                                 0.012147                        0.008262   \n",
       "3981                                 0.008738                        0.004263   \n",
       "\n",
       "      bacterial_dna_inhibitor  bacterial_membrane_integrity_inhibitor  \\\n",
       "0                    0.011519                                0.000530   \n",
       "1                    0.000271                                0.000226   \n",
       "2                    0.000000                                0.000000   \n",
       "3                    0.004086                                0.000802   \n",
       "4                    0.005678                                0.000739   \n",
       "...                       ...                                     ...   \n",
       "3977                 0.000786                                0.000377   \n",
       "3978                 0.009197                                0.000673   \n",
       "3979                 0.006411                                0.000573   \n",
       "3980                 0.006529                                0.000338   \n",
       "3981                 0.009720                                0.000532   \n",
       "\n",
       "      bcl_inhibitor  bcr-abl_inhibitor  benzodiazepine_receptor_agonist  \\\n",
       "0          0.003160           0.000747                         0.002201   \n",
       "1          0.000035           0.000414                         0.005909   \n",
       "2          0.000000           0.000000                         0.000000   \n",
       "3          0.000564           0.000413                         0.003143   \n",
       "4          0.002531           0.000448                         0.002191   \n",
       "...             ...                ...                              ...   \n",
       "3977       0.000788           0.027340                         0.000733   \n",
       "3978       0.001779           0.001548                         0.004179   \n",
       "3979       0.000739           0.000455                         0.003529   \n",
       "3980       0.000392           0.000218                         0.003747   \n",
       "3981       0.001707           0.000961                         0.002975   \n",
       "\n",
       "      beta_amyloid_inhibitor  bromodomain_inhibitor  btk_inhibitor  \\\n",
       "0                   0.002377               0.003517       0.000446   \n",
       "1                   0.000485               0.002174       0.019541   \n",
       "2                   0.000000               0.000000       0.000000   \n",
       "3                   0.002377               0.000924       0.001171   \n",
       "4                   0.002944               0.001371       0.000502   \n",
       "...                      ...                    ...            ...   \n",
       "3977                0.000824               0.001447       0.000701   \n",
       "3978                0.001369               0.000993       0.000555   \n",
       "3979                0.001526               0.000853       0.000271   \n",
       "3980                0.002380               0.000692       0.000717   \n",
       "3981                0.002023               0.001891       0.000427   \n",
       "\n",
       "      calcineurin_inhibitor  calcium_channel_blocker  \\\n",
       "0                  0.001137                 0.012876   \n",
       "1                  0.000170                 0.004683   \n",
       "2                  0.000000                 0.000000   \n",
       "3                  0.002001                 0.048854   \n",
       "4                  0.001509                 0.007546   \n",
       "...                     ...                      ...   \n",
       "3977               0.000259                 0.011043   \n",
       "3978               0.001325                 0.006917   \n",
       "3979               0.000831                 0.004699   \n",
       "3980               0.000780                 0.013127   \n",
       "3981               0.000907                 0.008714   \n",
       "\n",
       "      cannabinoid_receptor_agonist  cannabinoid_receptor_antagonist  \\\n",
       "0                         0.001543                         0.001953   \n",
       "1                         0.002241                         0.002229   \n",
       "2                         0.000000                         0.000000   \n",
       "3                         0.001139                         0.003584   \n",
       "4                         0.001203                         0.002401   \n",
       "...                            ...                              ...   \n",
       "3977                      0.000568                         0.000296   \n",
       "3978                      0.002940                         0.002610   \n",
       "3979                      0.003258                         0.003150   \n",
       "3980                      0.001778                         0.003482   \n",
       "3981                      0.001795                         0.001281   \n",
       "\n",
       "      carbonic_anhydrase_inhibitor  casein_kinase_inhibitor  \\\n",
       "0                         0.002775                 0.001574   \n",
       "1                         0.001174                 0.010068   \n",
       "2                         0.000000                 0.000000   \n",
       "3                         0.001792                 0.001834   \n",
       "4                         0.003067                 0.002496   \n",
       "...                            ...                      ...   \n",
       "3977                      0.000955                 0.000512   \n",
       "3978                      0.002739                 0.002489   \n",
       "3979                      0.002928                 0.001416   \n",
       "3980                      0.001562                 0.001542   \n",
       "3981                      0.001870                 0.001354   \n",
       "\n",
       "      caspase_activator  catechol_o_methyltransferase_inhibitor  \\\n",
       "0              0.001388                                0.002185   \n",
       "1              0.000234                                0.000370   \n",
       "2              0.000000                                0.000000   \n",
       "3              0.001164                                0.001026   \n",
       "4              0.001906                                0.002497   \n",
       "...                 ...                                     ...   \n",
       "3977           0.000342                                0.000515   \n",
       "3978           0.001920                                0.001308   \n",
       "3979           0.002774                                0.001710   \n",
       "3980           0.001374                                0.001332   \n",
       "3981           0.001594                                0.001351   \n",
       "\n",
       "      cc_chemokine_receptor_antagonist  cck_receptor_antagonist  \\\n",
       "0                             0.007353                 0.001292   \n",
       "1                             0.001361                 0.000638   \n",
       "2                             0.000000                 0.000000   \n",
       "3                             0.014560                 0.001262   \n",
       "4                             0.005801                 0.001836   \n",
       "...                                ...                      ...   \n",
       "3977                          0.000526                 0.000338   \n",
       "3978                          0.004449                 0.002223   \n",
       "3979                          0.002476                 0.002189   \n",
       "3980                          0.011153                 0.001578   \n",
       "3981                          0.006223                 0.000845   \n",
       "\n",
       "      cdk_inhibitor  chelating_agent  chk_inhibitor  chloride_channel_blocker  \\\n",
       "0          0.000716         0.004460       0.000595                  0.003431   \n",
       "1          0.002626         0.001050       0.001627                  0.000247   \n",
       "2          0.000000         0.000000       0.000000                  0.000000   \n",
       "3          0.001451         0.005781       0.000178                  0.005940   \n",
       "4          0.002323         0.004169       0.000304                  0.005523   \n",
       "...             ...              ...            ...                       ...   \n",
       "3977       0.000318         0.000681       0.009438                  0.000142   \n",
       "3978       0.001311         0.004745       0.000444                  0.003417   \n",
       "3979       0.000630         0.002885       0.000289                  0.004204   \n",
       "3980       0.000922         0.003729       0.000267                  0.002234   \n",
       "3981       0.000470         0.004263       0.000646                  0.001643   \n",
       "\n",
       "      cholesterol_inhibitor  cholinergic_receptor_antagonist  \\\n",
       "0                  0.003965                         0.005240   \n",
       "1                  0.001364                         0.000272   \n",
       "2                  0.000000                         0.000000   \n",
       "3                  0.003168                         0.005235   \n",
       "4                  0.003958                         0.004037   \n",
       "...                     ...                              ...   \n",
       "3977               0.000582                         0.000192   \n",
       "3978               0.002221                         0.002851   \n",
       "3979               0.002260                         0.002787   \n",
       "3980               0.001806                         0.004889   \n",
       "3981               0.003691                         0.002537   \n",
       "\n",
       "      coagulation_factor_inhibitor  corticosteroid_agonist  \\\n",
       "0                         0.000605                0.001087   \n",
       "1                         0.000703                0.000230   \n",
       "2                         0.000000                0.000000   \n",
       "3                         0.000814                0.000428   \n",
       "4                         0.000570                0.002186   \n",
       "...                            ...                     ...   \n",
       "3977                      0.000604                0.000742   \n",
       "3978                      0.000872                0.001717   \n",
       "3979                      0.000755                0.001470   \n",
       "3980                      0.000553                0.001471   \n",
       "3981                      0.000476                0.001032   \n",
       "\n",
       "      cyclooxygenase_inhibitor  cytochrome_p450_inhibitor  \\\n",
       "0                     0.052696                   0.007150   \n",
       "1                     0.000501                   0.000791   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.023050                   0.005602   \n",
       "4                     0.020639                   0.004589   \n",
       "...                        ...                        ...   \n",
       "3977                  0.000788                   0.000571   \n",
       "3978                  0.021324                   0.007050   \n",
       "3979                  0.021199                   0.005352   \n",
       "3980                  0.015376                   0.003261   \n",
       "3981                  0.044420                   0.006874   \n",
       "\n",
       "      dihydrofolate_reductase_inhibitor  dipeptidyl_peptidase_inhibitor  \\\n",
       "0                              0.001032                        0.001795   \n",
       "1                              0.000273                        0.001321   \n",
       "2                              0.000000                        0.000000   \n",
       "3                              0.000965                        0.000713   \n",
       "4                              0.001943                        0.002734   \n",
       "...                                 ...                             ...   \n",
       "3977                           0.000484                        0.000532   \n",
       "3978                           0.001401                        0.002708   \n",
       "3979                           0.002591                        0.004250   \n",
       "3980                           0.001469                        0.001268   \n",
       "3981                           0.000635                        0.001319   \n",
       "\n",
       "      diuretic  dna_alkylating_agent  dna_inhibitor  \\\n",
       "0     0.000819              0.004329       0.024621   \n",
       "1     0.000247              0.000190       0.000167   \n",
       "2     0.000000              0.000000       0.000000   \n",
       "3     0.000571              0.001940       0.014186   \n",
       "4     0.001122              0.003982       0.017368   \n",
       "...        ...                   ...            ...   \n",
       "3977  0.000223              0.008127       0.008068   \n",
       "3978  0.002042              0.002515       0.012318   \n",
       "3979  0.001294              0.001490       0.011886   \n",
       "3980  0.000614              0.001910       0.019773   \n",
       "3981  0.000777              0.008165       0.014725   \n",
       "\n",
       "      dopamine_receptor_agonist  dopamine_receptor_antagonist  egfr_inhibitor  \\\n",
       "0                      0.005350                      0.015176        0.000550   \n",
       "1                      0.000648                      0.001248        0.003940   \n",
       "2                      0.000000                      0.000000        0.000000   \n",
       "3                      0.011093                      0.066280        0.006345   \n",
       "4                      0.006458                      0.014142        0.000693   \n",
       "...                         ...                           ...             ...   \n",
       "3977                   0.013776                      0.001421        0.001307   \n",
       "3978                   0.005760                      0.013443        0.000443   \n",
       "3979                   0.005975                      0.015154        0.000654   \n",
       "3980                   0.007605                      0.024820        0.000520   \n",
       "3981                   0.007521                      0.019921        0.000456   \n",
       "\n",
       "      elastase_inhibitor  erbb2_inhibitor  estrogen_receptor_agonist  \\\n",
       "0               0.000854         0.000370                   0.019696   \n",
       "1               0.000624         0.000489                   0.000287   \n",
       "2               0.000000         0.000000                   0.000000   \n",
       "3               0.000769         0.000547                   0.005413   \n",
       "4               0.001187         0.000519                   0.008818   \n",
       "...                  ...              ...                        ...   \n",
       "3977            0.000604         0.000635                   0.002282   \n",
       "3978            0.000874         0.000442                   0.009761   \n",
       "3979            0.000787         0.000469                   0.004709   \n",
       "3980            0.000700         0.000441                   0.003308   \n",
       "3981            0.001059         0.000373                   0.007748   \n",
       "\n",
       "      estrogen_receptor_antagonist  faah_inhibitor  \\\n",
       "0                         0.001674        0.001951   \n",
       "1                         0.002122        0.002710   \n",
       "2                         0.000000        0.000000   \n",
       "3                         0.003819        0.001955   \n",
       "4                         0.000815        0.000871   \n",
       "...                            ...             ...   \n",
       "3977                      0.000953        0.000344   \n",
       "3978                      0.001326        0.000540   \n",
       "3979                      0.001175        0.000473   \n",
       "3980                      0.001307        0.001045   \n",
       "3981                      0.002114        0.001085   \n",
       "\n",
       "      farnesyltransferase_inhibitor  fatty_acid_receptor_agonist  \\\n",
       "0                          0.000622                     0.001820   \n",
       "1                          0.000265                     0.001438   \n",
       "2                          0.000000                     0.000000   \n",
       "3                          0.001278                     0.000795   \n",
       "4                          0.000547                     0.002055   \n",
       "...                             ...                          ...   \n",
       "3977                       0.001170                     0.000384   \n",
       "3978                       0.000309                     0.001746   \n",
       "3979                       0.000225                     0.001722   \n",
       "3980                       0.000232                     0.002009   \n",
       "3981                       0.000424                     0.001071   \n",
       "\n",
       "      fgfr_inhibitor  flt3_inhibitor  focal_adhesion_kinase_inhibitor  \\\n",
       "0           0.000462        0.001003                         0.000308   \n",
       "1           0.004290        0.002475                         0.001186   \n",
       "2           0.000000        0.000000                         0.000000   \n",
       "3           0.003424        0.001676                         0.000980   \n",
       "4           0.000519        0.001344                         0.000528   \n",
       "...              ...             ...                              ...   \n",
       "3977        0.000163        0.020919                         0.000369   \n",
       "3978        0.000710        0.000538                         0.000332   \n",
       "3979        0.000383        0.000247                         0.000197   \n",
       "3980        0.000587        0.000456                         0.000378   \n",
       "3981        0.000501        0.001083                         0.000178   \n",
       "\n",
       "      free_radical_scavenger  fungal_squalene_epoxidase_inhibitor  \\\n",
       "0                   0.001562                             0.000949   \n",
       "1                   0.000298                             0.000167   \n",
       "2                   0.000000                             0.000000   \n",
       "3                   0.001040                             0.000737   \n",
       "4                   0.001420                             0.000420   \n",
       "...                      ...                                  ...   \n",
       "3977                0.000896                             0.000776   \n",
       "3978                0.004068                             0.000475   \n",
       "3979                0.002406                             0.000588   \n",
       "3980                0.001285                             0.000531   \n",
       "3981                0.002143                             0.000608   \n",
       "\n",
       "      gaba_receptor_agonist  gaba_receptor_antagonist  \\\n",
       "0                  0.012978                  0.011431   \n",
       "1                  0.000205                  0.000986   \n",
       "2                  0.000000                  0.000000   \n",
       "3                  0.002479                  0.005998   \n",
       "4                  0.009581                  0.006794   \n",
       "...                     ...                       ...   \n",
       "3977               0.002537                  0.004028   \n",
       "3978               0.004263                  0.013488   \n",
       "3979               0.006736                  0.012508   \n",
       "3980               0.007152                  0.010428   \n",
       "3981               0.008728                  0.019987   \n",
       "\n",
       "      gamma_secretase_inhibitor  glucocorticoid_receptor_agonist  \\\n",
       "0                      0.000323                         0.001190   \n",
       "1                      0.005579                         0.000143   \n",
       "2                      0.000000                         0.000000   \n",
       "3                      0.000916                         0.001096   \n",
       "4                      0.000225                         0.001784   \n",
       "...                         ...                              ...   \n",
       "3977                   0.002316                         0.000592   \n",
       "3978                   0.000263                         0.000951   \n",
       "3979                   0.000446                         0.001182   \n",
       "3980                   0.000495                         0.002168   \n",
       "3981                   0.001372                         0.001540   \n",
       "\n",
       "      glutamate_inhibitor  glutamate_receptor_agonist  \\\n",
       "0                0.003128                    0.009080   \n",
       "1                0.000886                    0.000330   \n",
       "2                0.000000                    0.000000   \n",
       "3                0.000660                    0.003633   \n",
       "4                0.002400                    0.011555   \n",
       "...                   ...                         ...   \n",
       "3977             0.001126                    0.000370   \n",
       "3978             0.001177                    0.006727   \n",
       "3979             0.000775                    0.006218   \n",
       "3980             0.000801                    0.004140   \n",
       "3981             0.001042                    0.005013   \n",
       "\n",
       "      glutamate_receptor_antagonist  gonadotropin_receptor_agonist  \\\n",
       "0                          0.036030                       0.001540   \n",
       "1                          0.001766                       0.000403   \n",
       "2                          0.000000                       0.000000   \n",
       "3                          0.018996                       0.002088   \n",
       "4                          0.026131                       0.003004   \n",
       "...                             ...                            ...   \n",
       "3977                       0.001610                       0.000182   \n",
       "3978                       0.023382                       0.001756   \n",
       "3979                       0.021841                       0.001457   \n",
       "3980                       0.030124                       0.002109   \n",
       "3981                       0.026471                       0.001292   \n",
       "\n",
       "      gsk_inhibitor  hcv_inhibitor  hdac_inhibitor  \\\n",
       "0          0.000741       0.004323        0.001015   \n",
       "1          0.001451       0.007205        0.000240   \n",
       "2          0.000000       0.000000        0.000000   \n",
       "3          0.001005       0.004311        0.002048   \n",
       "4          0.001540       0.003617        0.000946   \n",
       "...             ...            ...             ...   \n",
       "3977       0.000160       0.003072        0.000380   \n",
       "3978       0.001390       0.002360        0.001413   \n",
       "3979       0.000542       0.002817        0.000771   \n",
       "3980       0.000470       0.003356        0.000762   \n",
       "3981       0.000725       0.003818        0.000700   \n",
       "\n",
       "      histamine_receptor_agonist  histamine_receptor_antagonist  \\\n",
       "0                       0.005648                       0.012301   \n",
       "1                       0.000136                       0.002149   \n",
       "2                       0.000000                       0.000000   \n",
       "3                       0.004851                       0.039271   \n",
       "4                       0.003308                       0.010849   \n",
       "...                          ...                            ...   \n",
       "3977                    0.000785                       0.000826   \n",
       "3978                    0.008076                       0.012291   \n",
       "3979                    0.004924                       0.009207   \n",
       "3980                    0.002342                       0.016248   \n",
       "3981                    0.006995                       0.012351   \n",
       "\n",
       "      histone_lysine_demethylase_inhibitor  \\\n",
       "0                                 0.000174   \n",
       "1                                 0.002810   \n",
       "2                                 0.000000   \n",
       "3                                 0.005564   \n",
       "4                                 0.000245   \n",
       "...                                    ...   \n",
       "3977                              0.000165   \n",
       "3978                              0.000468   \n",
       "3979                              0.000483   \n",
       "3980                              0.001386   \n",
       "3981                              0.000483   \n",
       "\n",
       "      histone_lysine_methyltransferase_inhibitor  hiv_inhibitor  \\\n",
       "0                                       0.000915       0.004357   \n",
       "1                                       0.013439       0.000715   \n",
       "2                                       0.000000       0.000000   \n",
       "3                                       0.011026       0.003247   \n",
       "4                                       0.000531       0.003954   \n",
       "...                                          ...            ...   \n",
       "3977                                    0.002877       0.000337   \n",
       "3978                                    0.001010       0.005612   \n",
       "3979                                    0.000745       0.005464   \n",
       "3980                                    0.001230       0.004592   \n",
       "3981                                    0.001905       0.002462   \n",
       "\n",
       "      hmgcr_inhibitor  hsp_inhibitor  igf-1_inhibitor  ikk_inhibitor  \\\n",
       "0            0.000852       0.000643         0.000425       0.001623   \n",
       "1            0.000093       0.000334         0.030998       0.000744   \n",
       "2            0.000000       0.000000         0.000000       0.000000   \n",
       "3            0.002145       0.000493         0.003704       0.000462   \n",
       "4            0.000787       0.001246         0.000208       0.001782   \n",
       "...               ...            ...              ...            ...   \n",
       "3977         0.000200       0.000320         0.002541       0.000891   \n",
       "3978         0.000452       0.001454         0.000289       0.002121   \n",
       "3979         0.000332       0.000719         0.000169       0.000623   \n",
       "3980         0.000800       0.000603         0.000264       0.000798   \n",
       "3981         0.000718       0.000960         0.000552       0.000762   \n",
       "\n",
       "      imidazoline_receptor_agonist  immunosuppressant  insulin_secretagogue  \\\n",
       "0                         0.003258           0.002744              0.001950   \n",
       "1                         0.000739           0.000660              0.000779   \n",
       "2                         0.000000           0.000000              0.000000   \n",
       "3                         0.002748           0.003733              0.003033   \n",
       "4                         0.002778           0.003757              0.002264   \n",
       "...                            ...                ...                   ...   \n",
       "3977                      0.000800           0.004073              0.000340   \n",
       "3978                      0.002834           0.000907              0.001477   \n",
       "3979                      0.001512           0.001086              0.001266   \n",
       "3980                      0.001854           0.002701              0.001940   \n",
       "3981                      0.005626           0.001986              0.001577   \n",
       "\n",
       "      insulin_sensitizer  integrin_inhibitor  jak_inhibitor  kit_inhibitor  \\\n",
       "0               0.000843            0.002089       0.001816       0.000922   \n",
       "1               0.002399            0.021335       0.010279       0.000572   \n",
       "2               0.000000            0.000000       0.000000       0.000000   \n",
       "3               0.002587            0.002689       0.000693       0.000745   \n",
       "4               0.001479            0.002255       0.001315       0.001408   \n",
       "...                  ...                 ...            ...            ...   \n",
       "3977            0.000096            0.000710       0.023981       0.000540   \n",
       "3978            0.000745            0.003114       0.000611       0.002467   \n",
       "3979            0.000506            0.002197       0.000121       0.000610   \n",
       "3980            0.002372            0.002486       0.000372       0.000650   \n",
       "3981            0.000589            0.002341       0.002426       0.000690   \n",
       "\n",
       "      laxative  leukotriene_inhibitor  leukotriene_receptor_antagonist  \\\n",
       "0     0.000701               0.000973                         0.002980   \n",
       "1     0.000284               0.000337                         0.002816   \n",
       "2     0.000000               0.000000                         0.000000   \n",
       "3     0.000631               0.000668                         0.002831   \n",
       "4     0.000979               0.001045                         0.004102   \n",
       "...        ...                    ...                              ...   \n",
       "3977  0.000631               0.000332                         0.003961   \n",
       "3978  0.001408               0.000939                         0.003527   \n",
       "3979  0.000842               0.000888                         0.003596   \n",
       "3980  0.000663               0.000742                         0.003645   \n",
       "3981  0.000647               0.000748                         0.005790   \n",
       "\n",
       "      lipase_inhibitor  lipoxygenase_inhibitor  lxr_agonist  mdm_inhibitor  \\\n",
       "0             0.001694                0.003322     0.000515       0.000400   \n",
       "1             0.000459                0.004560     0.000408       0.000317   \n",
       "2             0.000000                0.000000     0.000000       0.000000   \n",
       "3             0.001998                0.002334     0.001419       0.000259   \n",
       "4             0.001514                0.003115     0.000457       0.000211   \n",
       "...                ...                     ...          ...            ...   \n",
       "3977          0.001170                0.001145     0.000422       0.001088   \n",
       "3978          0.001086                0.001741     0.000335       0.000300   \n",
       "3979          0.001147                0.001372     0.000268       0.000177   \n",
       "3980          0.001348                0.000850     0.000478       0.000264   \n",
       "3981          0.001429                0.003621     0.000543       0.000388   \n",
       "\n",
       "      mek_inhibitor  membrane_integrity_inhibitor  \\\n",
       "0          0.000456                      0.007170   \n",
       "1          0.000165                      0.000177   \n",
       "2          0.000000                      0.000000   \n",
       "3          0.001979                      0.003534   \n",
       "4          0.000512                      0.009862   \n",
       "...             ...                           ...   \n",
       "3977       0.000812                      0.000353   \n",
       "3978       0.000537                      0.003026   \n",
       "3979       0.000457                      0.005852   \n",
       "3980       0.000331                      0.007375   \n",
       "3981       0.001029                      0.004543   \n",
       "\n",
       "      mineralocorticoid_receptor_antagonist  \\\n",
       "0                                  0.002044   \n",
       "1                                  0.000295   \n",
       "2                                  0.000000   \n",
       "3                                  0.004616   \n",
       "4                                  0.001811   \n",
       "...                                     ...   \n",
       "3977                               0.000837   \n",
       "3978                               0.000662   \n",
       "3979                               0.000666   \n",
       "3980                               0.001439   \n",
       "3981                               0.001776   \n",
       "\n",
       "      monoacylglycerol_lipase_inhibitor  monoamine_oxidase_inhibitor  \\\n",
       "0                              0.001770                     0.007061   \n",
       "1                              0.000389                     0.000489   \n",
       "2                              0.000000                     0.000000   \n",
       "3                              0.000728                     0.006163   \n",
       "4                              0.001132                     0.004851   \n",
       "...                                 ...                          ...   \n",
       "3977                           0.001021                     0.000594   \n",
       "3978                           0.000927                     0.006742   \n",
       "3979                           0.000981                     0.006621   \n",
       "3980                           0.000484                     0.005889   \n",
       "3981                           0.001023                     0.009080   \n",
       "\n",
       "      monopolar_spindle_1_kinase_inhibitor  mtor_inhibitor  mucolytic_agent  \\\n",
       "0                                 0.000404        0.001095         0.003628   \n",
       "1                                 0.000466        0.000317         0.000376   \n",
       "2                                 0.000000        0.000000         0.000000   \n",
       "3                                 0.000452        0.001903         0.005070   \n",
       "4                                 0.000271        0.001736         0.003760   \n",
       "...                                    ...             ...              ...   \n",
       "3977                              0.001671        0.000193         0.000673   \n",
       "3978                              0.000456        0.002171         0.004577   \n",
       "3979                              0.000480        0.001142         0.004674   \n",
       "3980                              0.000389        0.002292         0.003032   \n",
       "3981                              0.000333        0.000788         0.003091   \n",
       "\n",
       "      neuropeptide_receptor_antagonist  nfkb_inhibitor  \\\n",
       "0                             0.000858        0.003420   \n",
       "1                             0.000498        0.000308   \n",
       "2                             0.000000        0.000000   \n",
       "3                             0.004663        0.003138   \n",
       "4                             0.001129        0.002243   \n",
       "...                                ...             ...   \n",
       "3977                          0.000312        0.000426   \n",
       "3978                          0.001778        0.003066   \n",
       "3979                          0.002410        0.002760   \n",
       "3980                          0.003015        0.001595   \n",
       "3981                          0.001644        0.007340   \n",
       "\n",
       "      nicotinic_receptor_agonist  nitric_oxide_donor  \\\n",
       "0                       0.001174            0.001255   \n",
       "1                       0.000474            0.000200   \n",
       "2                       0.000000            0.000000   \n",
       "3                       0.000428            0.002355   \n",
       "4                       0.000941            0.002936   \n",
       "...                          ...                 ...   \n",
       "3977                    0.000665            0.000082   \n",
       "3978                    0.001153            0.003468   \n",
       "3979                    0.000696            0.003855   \n",
       "3980                    0.000850            0.003409   \n",
       "3981                    0.001176            0.001561   \n",
       "\n",
       "      nitric_oxide_production_inhibitor  nitric_oxide_synthase_inhibitor  \\\n",
       "0                              0.000721                         0.001154   \n",
       "1                              0.000407                         0.000485   \n",
       "2                              0.000000                         0.000000   \n",
       "3                              0.001642                         0.002314   \n",
       "4                              0.000596                         0.002233   \n",
       "...                                 ...                              ...   \n",
       "3977                           0.000427                         0.000383   \n",
       "3978                           0.000709                         0.002925   \n",
       "3979                           0.000460                         0.003170   \n",
       "3980                           0.000739                         0.001918   \n",
       "3981                           0.000683                         0.001376   \n",
       "\n",
       "      norepinephrine_reuptake_inhibitor  nrf2_activator  \\\n",
       "0                              0.000640        0.000963   \n",
       "1                              0.000365        0.000132   \n",
       "2                              0.000000        0.000000   \n",
       "3                              0.000661        0.000386   \n",
       "4                              0.000885        0.000894   \n",
       "...                                 ...             ...   \n",
       "3977                           0.000287        0.000474   \n",
       "3978                           0.001397        0.001036   \n",
       "3979                           0.001285        0.001037   \n",
       "3980                           0.000716        0.000529   \n",
       "3981                           0.000629        0.000967   \n",
       "\n",
       "      opioid_receptor_agonist  opioid_receptor_antagonist  \\\n",
       "0                    0.003281                    0.005644   \n",
       "1                    0.000484                    0.000565   \n",
       "2                    0.000000                    0.000000   \n",
       "3                    0.005066                    0.005935   \n",
       "4                    0.002795                    0.005402   \n",
       "...                       ...                         ...   \n",
       "3977                 0.003011                    0.000762   \n",
       "3978                 0.006172                    0.002390   \n",
       "3979                 0.003580                    0.004013   \n",
       "3980                 0.004009                    0.003745   \n",
       "3981                 0.003125                    0.004558   \n",
       "\n",
       "      orexin_receptor_antagonist  p38_mapk_inhibitor  \\\n",
       "0                       0.002031            0.000231   \n",
       "1                       0.000533            0.001878   \n",
       "2                       0.000000            0.000000   \n",
       "3                       0.002103            0.005259   \n",
       "4                       0.003969            0.000484   \n",
       "...                          ...                 ...   \n",
       "3977                    0.001201            0.000257   \n",
       "3978                    0.004518            0.000379   \n",
       "3979                    0.004584            0.000587   \n",
       "3980                    0.002802            0.000624   \n",
       "3981                    0.002610            0.000362   \n",
       "\n",
       "      p-glycoprotein_inhibitor  parp_inhibitor  pdgfr_inhibitor  \\\n",
       "0                     0.000783        0.001539         0.000727   \n",
       "1                     0.003098        0.005631         0.000920   \n",
       "2                     0.000000        0.000000         0.000000   \n",
       "3                     0.000826        0.000914         0.000537   \n",
       "4                     0.000985        0.001415         0.000803   \n",
       "...                        ...             ...              ...   \n",
       "3977                  0.000257        0.001328         0.000520   \n",
       "3978                  0.001770        0.001921         0.001313   \n",
       "3979                  0.001497        0.001013         0.000249   \n",
       "3980                  0.001468        0.001577         0.000691   \n",
       "3981                  0.000554        0.000845         0.000689   \n",
       "\n",
       "      pdk_inhibitor  phosphodiesterase_inhibitor  phospholipase_inhibitor  \\\n",
       "0          0.001424                     0.020555                 0.002439   \n",
       "1          0.001433                     0.012509                 0.000343   \n",
       "2          0.000000                     0.000000                 0.000000   \n",
       "3          0.000680                     0.010987                 0.003677   \n",
       "4          0.001159                     0.009492                 0.002604   \n",
       "...             ...                          ...                      ...   \n",
       "3977       0.000877                     0.002559                 0.000372   \n",
       "3978       0.003470                     0.021214                 0.001825   \n",
       "3979       0.001074                     0.009528                 0.001729   \n",
       "3980       0.001304                     0.016132                 0.001372   \n",
       "3981       0.001327                     0.016059                 0.002807   \n",
       "\n",
       "      pi3k_inhibitor  pkc_inhibitor  potassium_channel_activator  \\\n",
       "0           0.001441       0.000693                     0.003296   \n",
       "1           0.011387       0.002791                     0.001076   \n",
       "2           0.000000       0.000000                     0.000000   \n",
       "3           0.008718       0.002291                     0.006303   \n",
       "4           0.000783       0.001826                     0.004839   \n",
       "...              ...            ...                          ...   \n",
       "3977        0.003751       0.000178                     0.000528   \n",
       "3978        0.001716       0.001258                     0.004564   \n",
       "3979        0.000602       0.001049                     0.003334   \n",
       "3980        0.001379       0.002178                     0.009681   \n",
       "3981        0.001087       0.000477                     0.003290   \n",
       "\n",
       "      potassium_channel_antagonist  ppar_receptor_agonist  \\\n",
       "0                         0.006456               0.001795   \n",
       "1                         0.001309               0.006865   \n",
       "2                         0.000000               0.000000   \n",
       "3                         0.010304               0.003329   \n",
       "4                         0.005244               0.001527   \n",
       "...                            ...                    ...   \n",
       "3977                      0.000951               0.000159   \n",
       "3978                      0.004047               0.001629   \n",
       "3979                      0.004356               0.001089   \n",
       "3980                      0.003748               0.003540   \n",
       "3981                      0.009768               0.001529   \n",
       "\n",
       "      ppar_receptor_antagonist  progesterone_receptor_agonist  \\\n",
       "0                     0.001467                       0.017517   \n",
       "1                     0.006427                       0.000084   \n",
       "2                     0.000000                       0.000000   \n",
       "3                     0.002457                       0.001401   \n",
       "4                     0.001295                       0.006696   \n",
       "...                        ...                            ...   \n",
       "3977                  0.000256                       0.000535   \n",
       "3978                  0.001023                       0.003432   \n",
       "3979                  0.000935                       0.004136   \n",
       "3980                  0.001691                       0.005457   \n",
       "3981                  0.001106                       0.003008   \n",
       "\n",
       "      progesterone_receptor_antagonist  prostaglandin_inhibitor  \\\n",
       "0                             0.001200                 0.004194   \n",
       "1                             0.011556                 0.000217   \n",
       "2                             0.000000                 0.000000   \n",
       "3                             0.000719                 0.003013   \n",
       "4                             0.000761                 0.003987   \n",
       "...                                ...                      ...   \n",
       "3977                          0.000806                 0.000263   \n",
       "3978                          0.000499                 0.003030   \n",
       "3979                          0.000438                 0.002544   \n",
       "3980                          0.000886                 0.002936   \n",
       "3981                          0.000676                 0.002225   \n",
       "\n",
       "      prostanoid_receptor_antagonist  proteasome_inhibitor  \\\n",
       "0                           0.009744              0.000352   \n",
       "1                           0.000378              0.000279   \n",
       "2                           0.000000              0.000000   \n",
       "3                           0.002441              0.000298   \n",
       "4                           0.003920              0.000328   \n",
       "...                              ...                   ...   \n",
       "3977                        0.000332              0.000153   \n",
       "3978                        0.006735              0.000490   \n",
       "3979                        0.005523              0.000493   \n",
       "3980                        0.004592              0.000194   \n",
       "3981                        0.007876              0.000685   \n",
       "\n",
       "      protein_kinase_inhibitor  protein_phosphatase_inhibitor  \\\n",
       "0                     0.003584                       0.000469   \n",
       "1                     0.000264                       0.000407   \n",
       "2                     0.000000                       0.000000   \n",
       "3                     0.004591                       0.000384   \n",
       "4                     0.002751                       0.000665   \n",
       "...                        ...                            ...   \n",
       "3977                  0.001033                       0.000348   \n",
       "3978                  0.001290                       0.000727   \n",
       "3979                  0.001378                       0.000873   \n",
       "3980                  0.001848                       0.000656   \n",
       "3981                  0.002883                       0.000494   \n",
       "\n",
       "      protein_synthesis_inhibitor  protein_tyrosine_kinase_inhibitor  \\\n",
       "0                        0.004242                           0.000767   \n",
       "1                        0.000260                           0.000675   \n",
       "2                        0.000000                           0.000000   \n",
       "3                        0.011595                           0.002030   \n",
       "4                        0.004391                           0.001283   \n",
       "...                           ...                                ...   \n",
       "3977                     0.001365                           0.001390   \n",
       "3978                     0.002607                           0.001938   \n",
       "3979                     0.001876                           0.001005   \n",
       "3980                     0.005006                           0.000821   \n",
       "3981                     0.003205                           0.000914   \n",
       "\n",
       "      radiopaque_medium  raf_inhibitor  ras_gtpase_inhibitor  \\\n",
       "0              0.005486       0.000387              0.000785   \n",
       "1              0.000201       0.000034              0.001328   \n",
       "2              0.000000       0.000000              0.000000   \n",
       "3              0.002942       0.000382              0.001232   \n",
       "4              0.011775       0.000490              0.000718   \n",
       "...                 ...            ...                   ...   \n",
       "3977           0.000220       0.000054              0.000224   \n",
       "3978           0.007983       0.001307              0.000715   \n",
       "3979           0.008010       0.000308              0.000807   \n",
       "3980           0.009628       0.000630              0.000608   \n",
       "3981           0.003906       0.000216              0.000911   \n",
       "\n",
       "      retinoid_receptor_agonist  retinoid_receptor_antagonist  \\\n",
       "0                      0.001928                      0.000792   \n",
       "1                      0.000666                      0.000622   \n",
       "2                      0.000000                      0.000000   \n",
       "3                      0.000260                      0.001187   \n",
       "4                      0.000586                      0.001204   \n",
       "...                         ...                           ...   \n",
       "3977                   0.002513                      0.000338   \n",
       "3978                   0.001351                      0.000992   \n",
       "3979                   0.004282                      0.000648   \n",
       "3980                   0.000569                      0.000777   \n",
       "3981                   0.001002                      0.000558   \n",
       "\n",
       "      rho_associated_kinase_inhibitor  ribonucleoside_reductase_inhibitor  \\\n",
       "0                            0.000599                            0.001416   \n",
       "1                            0.067902                            0.000192   \n",
       "2                            0.000000                            0.000000   \n",
       "3                            0.001124                            0.001369   \n",
       "4                            0.002023                            0.001881   \n",
       "...                               ...                                 ...   \n",
       "3977                         0.000290                            0.000270   \n",
       "3978                         0.001374                            0.001754   \n",
       "3979                         0.000675                            0.002722   \n",
       "3980                         0.001260                            0.001745   \n",
       "3981                         0.000361                            0.001428   \n",
       "\n",
       "      rna_polymerase_inhibitor  serotonin_receptor_agonist  \\\n",
       "0                     0.003589                    0.016048   \n",
       "1                     0.001678                    0.008922   \n",
       "2                     0.000000                    0.000000   \n",
       "3                     0.001299                    0.020440   \n",
       "4                     0.002261                    0.007480   \n",
       "...                        ...                         ...   \n",
       "3977                  0.000375                    0.010463   \n",
       "3978                  0.002237                    0.008997   \n",
       "3979                  0.000973                    0.008691   \n",
       "3980                  0.000839                    0.016432   \n",
       "3981                  0.003872                    0.022006   \n",
       "\n",
       "      serotonin_receptor_antagonist  serotonin_reuptake_inhibitor  \\\n",
       "0                          0.011812                      0.003126   \n",
       "1                          0.000809                      0.000118   \n",
       "2                          0.000000                      0.000000   \n",
       "3                          0.057859                      0.002541   \n",
       "4                          0.008366                      0.003449   \n",
       "...                             ...                           ...   \n",
       "3977                       0.000726                      0.001298   \n",
       "3978                       0.014665                      0.003384   \n",
       "3979                       0.024664                      0.002454   \n",
       "3980                       0.011804                      0.002719   \n",
       "3981                       0.018463                      0.004341   \n",
       "\n",
       "      sigma_receptor_agonist  sigma_receptor_antagonist  \\\n",
       "0                   0.004764                   0.001264   \n",
       "1                   0.000246                   0.002415   \n",
       "2                   0.000000                   0.000000   \n",
       "3                   0.002460                   0.006485   \n",
       "4                   0.003925                   0.001187   \n",
       "...                      ...                        ...   \n",
       "3977                0.000356                   0.000433   \n",
       "3978                0.003170                   0.001147   \n",
       "3979                0.002471                   0.001391   \n",
       "3980                0.001488                   0.001967   \n",
       "3981                0.003409                   0.001944   \n",
       "\n",
       "      smoothened_receptor_antagonist  sodium_channel_inhibitor  \\\n",
       "0                           0.001527                  0.030887   \n",
       "1                           0.001445                  0.005446   \n",
       "2                           0.000000                  0.000000   \n",
       "3                           0.004952                  0.009968   \n",
       "4                           0.001471                  0.013573   \n",
       "...                              ...                       ...   \n",
       "3977                        0.000534                  0.002853   \n",
       "3978                        0.001079                  0.020350   \n",
       "3979                        0.000558                  0.017391   \n",
       "3980                        0.001770                  0.009422   \n",
       "3981                        0.001773                  0.022216   \n",
       "\n",
       "      sphingosine_receptor_agonist  src_inhibitor   steroid  syk_inhibitor  \\\n",
       "0                         0.002939       0.000563  0.000738       0.000485   \n",
       "1                         0.000073       0.034004  0.000195       0.002079   \n",
       "2                         0.000000       0.000000  0.000000       0.000000   \n",
       "3                         0.002968       0.001298  0.001536       0.000258   \n",
       "4                         0.003263       0.000486  0.001203       0.000487   \n",
       "...                            ...            ...       ...            ...   \n",
       "3977                      0.000347       0.010709  0.000152       0.000613   \n",
       "3978                      0.001017       0.001764  0.000759       0.000911   \n",
       "3979                      0.001020       0.000730  0.000566       0.000325   \n",
       "3980                      0.001754       0.000494  0.000899       0.000415   \n",
       "3981                      0.001536       0.001853  0.000526       0.000500   \n",
       "\n",
       "      tachykinin_antagonist  tgf-beta_receptor_inhibitor  thrombin_inhibitor  \\\n",
       "0                  0.002748                     0.000248            0.000830   \n",
       "1                  0.001032                     0.001112            0.000635   \n",
       "2                  0.000000                     0.000000            0.000000   \n",
       "3                  0.005307                     0.000492            0.001458   \n",
       "4                  0.002057                     0.000530            0.000977   \n",
       "...                     ...                          ...                 ...   \n",
       "3977               0.000537                     0.000479            0.000395   \n",
       "3978               0.003127                     0.000372            0.001386   \n",
       "3979               0.002621                     0.000529            0.002731   \n",
       "3980               0.005527                     0.000670            0.002045   \n",
       "3981               0.002696                     0.000529            0.001005   \n",
       "\n",
       "      thymidylate_synthase_inhibitor  tlr_agonist  tlr_antagonist  \\\n",
       "0                           0.002091     0.001653        0.000935   \n",
       "1                           0.000124     0.000429        0.000750   \n",
       "2                           0.000000     0.000000        0.000000   \n",
       "3                           0.001032     0.002637        0.000900   \n",
       "4                           0.002908     0.002416        0.000896   \n",
       "...                              ...          ...             ...   \n",
       "3977                        0.000141     0.000301        0.000202   \n",
       "3978                        0.002797     0.003401        0.000974   \n",
       "3979                        0.004601     0.002824        0.000741   \n",
       "3980                        0.002718     0.002031        0.000841   \n",
       "3981                        0.001234     0.001497        0.000792   \n",
       "\n",
       "      tnf_inhibitor  topoisomerase_inhibitor  \\\n",
       "0          0.002066                 0.001014   \n",
       "1          0.000490                 0.000983   \n",
       "2          0.000000                 0.000000   \n",
       "3          0.001868                 0.000528   \n",
       "4          0.002270                 0.000752   \n",
       "...             ...                      ...   \n",
       "3977       0.001482                 0.000201   \n",
       "3978       0.002413                 0.001438   \n",
       "3979       0.001882                 0.001117   \n",
       "3980       0.001060                 0.001517   \n",
       "3981       0.003524                 0.000839   \n",
       "\n",
       "      transient_receptor_potential_channel_antagonist  \\\n",
       "0                                            0.000913   \n",
       "1                                            0.001629   \n",
       "2                                            0.000000   \n",
       "3                                            0.001135   \n",
       "4                                            0.000934   \n",
       "...                                               ...   \n",
       "3977                                         0.000555   \n",
       "3978                                         0.002095   \n",
       "3979                                         0.001364   \n",
       "3980                                         0.000670   \n",
       "3981                                         0.000813   \n",
       "\n",
       "      tropomyosin_receptor_kinase_inhibitor  trpv_agonist  trpv_antagonist  \\\n",
       "0                                  0.001604      0.000953         0.003348   \n",
       "1                                  0.000351      0.000083         0.002157   \n",
       "2                                  0.000000      0.000000         0.000000   \n",
       "3                                  0.000696      0.000791         0.001772   \n",
       "4                                  0.001501      0.000728         0.003340   \n",
       "...                                     ...           ...              ...   \n",
       "3977                               0.000534      0.018441         0.002029   \n",
       "3978                               0.000880      0.000682         0.002549   \n",
       "3979                               0.000580      0.000471         0.002182   \n",
       "3980                               0.000606      0.000489         0.003491   \n",
       "3981                               0.001163      0.002568         0.001842   \n",
       "\n",
       "      tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "0              0.001803                   0.000712   \n",
       "1              0.000171                   0.010801   \n",
       "2              0.000000                   0.000000   \n",
       "3              0.004583                   0.020798   \n",
       "4              0.000954                   0.001529   \n",
       "...                 ...                        ...   \n",
       "3977           0.483528                   0.008911   \n",
       "3978           0.001605                   0.001178   \n",
       "3979           0.000862                   0.000837   \n",
       "3980           0.001422                   0.002253   \n",
       "3981           0.012824                   0.001031   \n",
       "\n",
       "      ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n",
       "0                                  0.000977         0.000812   0.002556   \n",
       "1                                  0.000274         0.003126   0.000582   \n",
       "2                                  0.000000         0.000000   0.000000   \n",
       "3                                  0.000363         0.007036   0.002800   \n",
       "4                                  0.000979         0.001632   0.001792   \n",
       "...                                     ...              ...        ...   \n",
       "3977                               0.001142         0.002032   0.000703   \n",
       "3978                               0.001170         0.002731   0.001397   \n",
       "3979                               0.000719         0.000979   0.001079   \n",
       "3980                               0.000499         0.000981   0.003025   \n",
       "3981                               0.001096         0.001398   0.002549   \n",
       "\n",
       "      vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0                       0.001730       0.001381  \n",
       "1                       0.000212       0.002201  \n",
       "2                       0.000000       0.000000  \n",
       "3                       0.001171       0.003052  \n",
       "4                       0.000510       0.001407  \n",
       "...                          ...            ...  \n",
       "3977                    0.000990       0.000264  \n",
       "3978                    0.000392       0.000999  \n",
       "3979                    0.000153       0.002041  \n",
       "3980                    0.000326       0.003771  \n",
       "3981                    0.000571       0.001204  \n",
       "\n",
       "[3982 rows x 207 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25065"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028907,
     "end_time": "2020-10-26T08:36:27.882367",
     "exception": false,
     "start_time": "2020-10-26T08:36:27.853460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# This is an Updated version of my previous public kernel <https://www.kaggle.com/kushal1506/moa-pytorch-0-01859-rankgauss-pca-nn>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**train_targets_nonscored.csv本身就是额外的训练数据（与要预测的特征无关，仅是为了帮助提升模型性能），本notebook没有用额外数据，且最小，所以先看这一个？**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028398,
     "end_time": "2020-10-26T08:36:27.939323",
     "exception": false,
     "start_time": "2020-10-26T08:36:27.910925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Updates -\n",
    "\n",
    "* Implementing Feature Engineering \n",
    "* Implementing Label Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0 导入库与读取数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-26T08:36:28.058044Z",
     "iopub.status.busy": "2020-10-26T08:36:28.057389Z",
     "iopub.status.idle": "2020-10-26T08:36:29.061106Z",
     "shell.execute_reply": "2020-10-26T08:36:29.059801Z"
    },
    "papermill": {
     "duration": 1.036625,
     "end_time": "2020-10-26T08:36:29.061237",
     "exception": false,
     "start_time": "2020-10-26T08:36:28.024612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "'''\n",
    "\n",
    "sys.path.append('../input/iterativestratification')\n",
    "'''\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-26T08:36:29.127571Z",
     "iopub.status.busy": "2020-10-26T08:36:29.126618Z",
     "iopub.status.idle": "2020-10-26T08:36:30.836269Z",
     "shell.execute_reply": "2020-10-26T08:36:30.835715Z"
    },
    "papermill": {
     "duration": 1.746235,
     "end_time": "2020-10-26T08:36:30.836408",
     "exception": false,
     "start_time": "2020-10-26T08:36:29.090173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA,FactorAnalysis\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:36:30.897703Z",
     "iopub.status.busy": "2020-10-26T08:36:30.896739Z",
     "iopub.status.idle": "2020-10-26T08:36:30.899950Z",
     "shell.execute_reply": "2020-10-26T08:36:30.899428Z"
    },
    "papermill": {
     "duration": 0.034811,
     "end_time": "2020-10-26T08:36:30.900076",
     "exception": false,
     "start_time": "2020-10-26T08:36:30.865265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-26T08:36:30.960787Z",
     "iopub.status.busy": "2020-10-26T08:36:30.960170Z",
     "iopub.status.idle": "2020-10-26T08:36:30.967563Z",
     "shell.execute_reply": "2020-10-26T08:36:30.968017Z"
    },
    "papermill": {
     "duration": 0.040718,
     "end_time": "2020-10-26T08:36:30.968157",
     "exception": false,
     "start_time": "2020-10-26T08:36:30.927439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nos.listdir('../input/lish-moa')\\n\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "os.listdir('../input/lish-moa')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-26T08:36:31.032634Z",
     "iopub.status.busy": "2020-10-26T08:36:31.031938Z",
     "iopub.status.idle": "2020-10-26T08:36:40.729277Z",
     "shell.execute_reply": "2020-10-26T08:36:40.727985Z"
    },
    "papermill": {
     "duration": 9.732581,
     "end_time": "2020-10-26T08:36:40.729401",
     "exception": false,
     "start_time": "2020-10-26T08:36:30.996820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "'''\n",
    "base_path = 'E:\\AAAMyCodes\\myjupyter\\kaggle\\Mechanisms of Action (MoA) Prediction\\data'\n",
    "train_features = pd.read_csv(base_path+'/train_features.csv')\n",
    "train_targets_scored = pd.read_csv(base_path+'/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(base_path+'/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv(base_path+'/test_features.csv')\n",
    "sample_submission = pd.read_csv(base_path+'/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 数据预处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 数据观察"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 由数据描述，很自然分离出GENES与CELLS两大类特征，注意这里返回的是list类型\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**观察原始数据**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plot_example_list = ['g-0', 'g-1', 'g-2', 'c-0', 'c-1', 'c-2']\n",
    "plt.figure(figsize=(10,10))\n",
    "for i, plot_example in enumerate(plot_example_list):\n",
    "    plt.subplot(4,3,(i+1))\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    plt.title('train_features_'+plot_example)\n",
    "    plt.hist(train_features[plot_example])\n",
    "\n",
    "    plt.subplot(4,3,(i+1+6))\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    plt.title('test_features_'+plot_example)\n",
    "    plt.hist(test_features[plot_example])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 12 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 将前四个特征之外的特征转换为正态分布（train、test分别转换为正态分布）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#RankGauss\n",
    "# str的list相加其实就是拼接\n",
    "# 这里面有些操作应该有更简洁的写法，但是这种属于同一种功能的不同写法，就不改了，浪费时间，大致理解其意思即可\n",
    "for col in (GENES + CELLS):\n",
    "\n",
    "    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
    "    # 设置train_features\n",
    "    vec_len = len(train_features[col].values)\n",
    "    vec_len_test = len(test_features[col].values)\n",
    "    raw_vec = train_features[col].values.reshape(vec_len, 1) # 可是train_features[col].values本身就是这个形状啊\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    # 设置test_features\n",
    "    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**观察QuantileTransformer转换后的数据分布**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 12 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_example_list = ['g-0', 'g-1', 'g-2', 'c-0', 'c-1', 'c-2']\n",
    "plt.figure(figsize=(10,10))\n",
    "for i, plot_example in enumerate(plot_example_list):\n",
    "    plt.subplot(4,3,(i+1))\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    plt.title('train_features_'+plot_example)\n",
    "    plt.hist(train_features[plot_example])\n",
    "\n",
    "    plt.subplot(4,3,(i+1+6))\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    plt.title('test_features_'+plot_example)\n",
    "    plt.hist(test_features[plot_example])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:36:40.865485Z",
     "iopub.status.busy": "2020-10-26T08:36:40.864744Z",
     "iopub.status.idle": "2020-10-26T08:36:50.328820Z",
     "shell.execute_reply": "2020-10-26T08:36:50.328209Z"
    },
    "papermill": {
     "duration": 9.504492,
     "end_time": "2020-10-26T08:36:50.328951",
     "exception": false,
     "start_time": "2020-10-26T08:36:40.824459",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**统一设置所有用到的随机数**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:36:50.394386Z",
     "iopub.status.busy": "2020-10-26T08:36:50.393625Z",
     "iopub.status.idle": "2020-10-26T08:36:50.400128Z",
     "shell.execute_reply": "2020-10-26T08:36:50.399585Z"
    },
    "papermill": {
     "duration": 0.041901,
     "end_time": "2020-10-26T08:36:50.400227",
     "exception": false,
     "start_time": "2020-10-26T08:36:50.358326",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3 对genes与cells特征进行pca降维"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print('genes维数',train_features[GENES].shape)\n",
    "print('cells维数',train_features[CELLS].shape)\n",
    "# 可以看出有772列"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genes维数 (23814, 772)\n",
      "cells维数 (23814, 100)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "GENES因子分析"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# GENES\n",
    "# 因子分析\n",
    "n_comp = 90  #<--Update # 因子分析的因子数（降低为90列）\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])]) # 纵向拼接（仅是为了统一处理列，后面又分开）\n",
    "data2 = (FactorAnalysis(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "# 为新获得的dataframe添加列名\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# 将pca后的特征与原特征横向拼接\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:36:50.471352Z",
     "iopub.status.busy": "2020-10-26T08:36:50.470259Z",
     "iopub.status.idle": "2020-10-26T08:36:59.597632Z",
     "shell.execute_reply": "2020-10-26T08:36:59.596644Z"
    },
    "papermill": {
     "duration": 9.168918,
     "end_time": "2020-10-26T08:36:59.597753",
     "exception": false,
     "start_time": "2020-10-26T08:36:50.428835",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "cells因子分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:36:59.670117Z",
     "iopub.status.busy": "2020-10-26T08:36:59.668759Z",
     "iopub.status.idle": "2020-10-26T08:37:02.140870Z",
     "shell.execute_reply": "2020-10-26T08:37:02.140324Z"
    },
    "papermill": {
     "duration": 2.514494,
     "end_time": "2020-10-26T08:37:02.141002",
     "exception": false,
     "start_time": "2020-10-26T08:36:59.626508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CELLS\n",
    "n_comp = 50  #<--Update\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "data2 = (FactorAnalysis(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:37:02.205896Z",
     "iopub.status.busy": "2020-10-26T08:37:02.205248Z",
     "iopub.status.idle": "2020-10-26T08:37:02.208252Z",
     "shell.execute_reply": "2020-10-26T08:37:02.208799Z"
    },
    "papermill": {
     "duration": 0.038213,
     "end_time": "2020-10-26T08:37:02.208908",
     "exception": false,
     "start_time": "2020-10-26T08:37:02.170695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "降维并拼接原特征后的特征维数 (23814, 1016)\n"
     ]
    }
   ],
   "source": [
    "print('降维并拼接原特征后的特征维数',train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 将前四个特征之外的特征转换为正态分布（train、test拼接转换为正态分布）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:37:02.278370Z",
     "iopub.status.busy": "2020-10-26T08:37:02.277725Z",
     "iopub.status.idle": "2020-10-26T08:37:12.223668Z",
     "shell.execute_reply": "2020-10-26T08:37:12.222687Z"
    },
    "papermill": {
     "duration": 9.985423,
     "end_time": "2020-10-26T08:37:12.223788",
     "exception": false,
     "start_time": "2020-10-26T08:37:02.238365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(23814, 1016)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "#var_thresh = VarianceThreshold(0.8)  #<-- Update\n",
    "var_thresh = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
    "\n",
    "data = train_features.append(test_features) # 纵向拼接\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:]) # 仅处理第四列之外的特征\n",
    "\n",
    "# 将处理过之后的数据拆分\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "\n",
    "# 重组train_features\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "# 重组test_features\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "\n",
    "train_features.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**观察处理之后的数据**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# plot_example_list = ['g-0', 'g-1', 'g-2', 'c-0', 'c-1', 'c-2']\n",
    "# plt.figure(figsize=(10,10))\n",
    "# for i, plot_example in enumerate(plot_example_list):\n",
    "#     plt.subplot(4,3,(i+1))\n",
    "#     plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "#     plt.title('train_features_'+plot_example)\n",
    "#     plt.hist(train_features[plot_example])\n",
    "#\n",
    "#     plt.subplot(4,3,(i+1+6))\n",
    "#     plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "#     plt.title('test_features_'+plot_example)\n",
    "#     plt.hist(test_features[plot_example])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.5 根据gene与cell特征对样本进行聚类\n",
    "**注意这里将train与test一同用于KMeans的fit（为了保证聚类标准的一致性？）**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**dataframe添加KMeans聚类结果**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:37:12.303672Z",
     "iopub.status.busy": "2020-10-26T08:37:12.302883Z",
     "iopub.status.idle": "2020-10-26T08:39:00.811295Z",
     "shell.execute_reply": "2020-10-26T08:39:00.810658Z"
    },
    "papermill": {
     "duration": 108.55383,
     "end_time": "2020-10-26T08:39:00.811428",
     "exception": false,
     "start_time": "2020-10-26T08:37:12.257598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def fe_cluster(train, test, n_clusters_g = 45, n_clusters_c = 15, SEED = 123):\n",
    "\n",
    "    # 返回的是int值列表\n",
    "    features_g = list(train.columns[4:776]) #4:776这个要自己观察dataframe数据去找\n",
    "    features_c = list(train.columns[776:876])\n",
    "    \n",
    "    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n",
    "        # features是int列表，包含了要进行聚类的gene/cell的列索引\n",
    "        train_ = train[features].copy()\n",
    "        test_ = test[features].copy()\n",
    "        data = pd.concat([train_, test_], axis = 0)\n",
    "        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data) # 这里为什么要将train与test的数据一同fit？（为了保证类别划分的标准一致？）\n",
    "        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]] # 添加一个新列，这个列标记了样本按gene/cell进行的分类结果\n",
    "        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n",
    "        train = pd.get_dummies(train, columns = [f'clusters_{kind}']) # 将类别节点（原来为[0, n_clusters)的整数）转换为哑结点(1,0,0,0……0这种形式)\n",
    "        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "        return train, test\n",
    "    \n",
    "    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n",
    "    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c) #  注意运行create_cluster是对train与test添加新列，并不会覆盖老数据，所以这种写法没关系\n",
    "    return train, test\n",
    "\n",
    "train_features ,test_features=fe_cluster(train_features,test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.6 dataframe添加新的数据特征的列，删除无用的行"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:00.890461Z",
     "iopub.status.busy": "2020-10-26T08:39:00.880743Z",
     "iopub.status.idle": "2020-10-26T08:39:07.240441Z",
     "shell.execute_reply": "2020-10-26T08:39:07.241782Z"
    },
    "papermill": {
     "duration": 6.399712,
     "end_time": "2020-10-26T08:39:07.242000",
     "exception": false,
     "start_time": "2020-10-26T08:39:00.842288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = list(train.columns[4:776])\n",
    "    features_c = list(train.columns[776:876])\n",
    "    \n",
    "    for df in train, test:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1)\n",
    "        df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1)\n",
    "        df['g_kurt'] = df[features_g].kurtosis(axis = 1) # 峰度\n",
    "        df['g_skew'] = df[features_g].skew(axis = 1) # 偏斜度\n",
    "\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1)\n",
    "        df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1)\n",
    "        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n",
    "        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "train_features,test_features=fe_stats(train_features,test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**删除'cp_type'=='ctl_vehicle'的行，并重置索引以使得索引连续化**\n",
    "因为这样的行没有moa，也就是对分类没有帮助，所以要删除"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:07.343313Z",
     "iopub.status.busy": "2020-10-26T08:39:07.342378Z",
     "iopub.status.idle": "2020-10-26T08:39:08.042735Z",
     "shell.execute_reply": "2020-10-26T08:39:08.043691Z"
    },
    "papermill": {
     "duration": 0.754339,
     "end_time": "2020-10-26T08:39:08.043853",
     "exception": false,
     "start_time": "2020-10-26T08:39:07.289514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge 函数通过一个或多个键将数据集的行连接起来。\n",
    "# 场景：针对同一个主键存在的两张包含不同特征的表，通过主键的链接，将两张表进行合并。合并之后，两张表的行数不增加，列数是两张表的列数之和。\n",
    "train = train_features.merge(train_targets_scored, on='sig_id') # 这个拼接的目的是一同丢弃cp_type'=='ctl_vehicle的行（出于代码书写简洁考虑）\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True) # drop=True为重新添加连续索引之后，将老索引删除\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target = train[train_targets_scored.columns] # 一同丢弃相应行后，再分离出target（注意这里的train中仍然包含着target）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:08.166933Z",
     "iopub.status.busy": "2020-10-26T08:39:08.165528Z",
     "iopub.status.idle": "2020-10-26T08:39:08.190374Z",
     "shell.execute_reply": "2020-10-26T08:39:08.189788Z"
    },
    "papermill": {
     "duration": 0.114967,
     "end_time": "2020-10-26T08:39:08.190495",
     "exception": false,
     "start_time": "2020-10-26T08:39:08.075528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cp_type现在没用了，删除\n",
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:08.259445Z",
     "iopub.status.busy": "2020-10-26T08:39:08.258342Z",
     "iopub.status.idle": "2020-10-26T08:39:08.299113Z",
     "shell.execute_reply": "2020-10-26T08:39:08.299760Z"
    },
    "papermill": {
     "duration": 0.077415,
     "end_time": "2020-10-26T08:39:08.299920",
     "exception": false,
     "start_time": "2020-10-26T08:39:08.222505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             sig_id cp_time cp_dose         0         1         2         3  \\\n0      id_000644bb2      24      D1  1.147013  0.901837 -0.418317 -0.961328   \n1      id_000779bfc      72      D1  0.128837  0.676615  0.274303  0.090277   \n2      id_000a6266a      48      D1  0.790059  0.940310  1.427325 -0.121422   \n3      id_0015fd391      48      D1 -0.729887 -0.277122 -0.441238  0.766535   \n4      id_001626bd3      72      D2 -0.444200 -0.481230  0.974570  0.977613   \n...             ...     ...     ...       ...       ...       ...       ...   \n21943  id_fff8c2444      72      D1  0.247854 -1.231353  0.221655 -0.354349   \n21944  id_fffb1ceed      24      D2  0.217631 -0.026850 -0.237436 -0.787443   \n21945  id_fffb70c0c      24      D2 -1.911045  0.581948 -0.588666  1.303231   \n21946  id_fffcb9e7c      24      D1  0.826571  0.411264  0.433236  0.307423   \n21947  id_ffffdd77b      72      D1 -1.245408  1.566327 -0.269846  1.091825   \n\n              4         5         6  ...  \\\n0     -0.254748 -1.021168 -1.368753  ...   \n1      1.208714  0.688985  0.316750  ...   \n2     -0.001534  1.494980  0.238795  ...   \n3      2.327042 -0.862822 -2.304780  ...   \n4      1.468226 -0.874366 -0.372617  ...   \n...         ...       ...       ...  ...   \n21943 -0.332003  0.570609 -0.150057  ...   \n21944 -0.677697  0.919339  0.743390  ...   \n21945 -1.008858  0.851933 -0.302789  ...   \n21946  1.075457 -0.024437  0.051466  ...   \n21947 -0.515807 -2.085725 -1.626974  ...   \n\n       tropomyosin_receptor_kinase_inhibitor  trpv_agonist  trpv_antagonist  \\\n0                                          0             0                0   \n1                                          0             0                0   \n2                                          0             0                0   \n3                                          0             0                0   \n4                                          0             0                0   \n...                                      ...           ...              ...   \n21943                                      0             0                0   \n21944                                      0             0                0   \n21945                                      0             0                0   \n21946                                      0             0                0   \n21947                                      0             0                0   \n\n       tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n0                      0                          0   \n1                      0                          0   \n2                      0                          0   \n3                      0                          0   \n4                      0                          0   \n...                  ...                        ...   \n21943                  0                          0   \n21944                  0                          0   \n21945                  0                          0   \n21946                  0                          0   \n21947                  0                          0   \n\n       ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n0                                          0                0          0   \n1                                          0                0          0   \n2                                          0                0          0   \n3                                          0                0          0   \n4                                          0                0          0   \n...                                      ...              ...        ...   \n21943                                      0                0          0   \n21944                                      0                0          0   \n21945                                      0                0          0   \n21946                                      0                0          0   \n21947                                      0                0          0   \n\n       vitamin_d_receptor_agonist  wnt_inhibitor  \n0                               0              0  \n1                               0              0  \n2                               0              0  \n3                               0              0  \n4                               0              0  \n...                           ...            ...  \n21943                           0              0  \n21944                           0              0  \n21945                           0              0  \n21946                           0              0  \n21947                           0              0  \n\n[21948 rows x 1296 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>...</th>\n      <th>tropomyosin_receptor_kinase_inhibitor</th>\n      <th>trpv_agonist</th>\n      <th>trpv_antagonist</th>\n      <th>tubulin_inhibitor</th>\n      <th>tyrosine_kinase_inhibitor</th>\n      <th>ubiquitin_specific_protease_inhibitor</th>\n      <th>vegfr_inhibitor</th>\n      <th>vitamin_b</th>\n      <th>vitamin_d_receptor_agonist</th>\n      <th>wnt_inhibitor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>1.147013</td>\n      <td>0.901837</td>\n      <td>-0.418317</td>\n      <td>-0.961328</td>\n      <td>-0.254748</td>\n      <td>-1.021168</td>\n      <td>-1.368753</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_000779bfc</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.128837</td>\n      <td>0.676615</td>\n      <td>0.274303</td>\n      <td>0.090277</td>\n      <td>1.208714</td>\n      <td>0.688985</td>\n      <td>0.316750</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_000a6266a</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>0.790059</td>\n      <td>0.940310</td>\n      <td>1.427325</td>\n      <td>-0.121422</td>\n      <td>-0.001534</td>\n      <td>1.494980</td>\n      <td>0.238795</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_0015fd391</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>-0.729887</td>\n      <td>-0.277122</td>\n      <td>-0.441238</td>\n      <td>0.766535</td>\n      <td>2.327042</td>\n      <td>-0.862822</td>\n      <td>-2.304780</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_001626bd3</td>\n      <td>72</td>\n      <td>D2</td>\n      <td>-0.444200</td>\n      <td>-0.481230</td>\n      <td>0.974570</td>\n      <td>0.977613</td>\n      <td>1.468226</td>\n      <td>-0.874366</td>\n      <td>-0.372617</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21943</th>\n      <td>id_fff8c2444</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.247854</td>\n      <td>-1.231353</td>\n      <td>0.221655</td>\n      <td>-0.354349</td>\n      <td>-0.332003</td>\n      <td>0.570609</td>\n      <td>-0.150057</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21944</th>\n      <td>id_fffb1ceed</td>\n      <td>24</td>\n      <td>D2</td>\n      <td>0.217631</td>\n      <td>-0.026850</td>\n      <td>-0.237436</td>\n      <td>-0.787443</td>\n      <td>-0.677697</td>\n      <td>0.919339</td>\n      <td>0.743390</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21945</th>\n      <td>id_fffb70c0c</td>\n      <td>24</td>\n      <td>D2</td>\n      <td>-1.911045</td>\n      <td>0.581948</td>\n      <td>-0.588666</td>\n      <td>1.303231</td>\n      <td>-1.008858</td>\n      <td>0.851933</td>\n      <td>-0.302789</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21946</th>\n      <td>id_fffcb9e7c</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>0.826571</td>\n      <td>0.411264</td>\n      <td>0.433236</td>\n      <td>0.307423</td>\n      <td>1.075457</td>\n      <td>-0.024437</td>\n      <td>0.051466</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21947</th>\n      <td>id_ffffdd77b</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>-1.245408</td>\n      <td>1.566327</td>\n      <td>-0.269846</td>\n      <td>1.091825</td>\n      <td>-0.515807</td>\n      <td>-2.085725</td>\n      <td>-1.626974</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>21948 rows × 1296 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:08.390726Z",
     "iopub.status.busy": "2020-10-26T08:39:08.388642Z",
     "iopub.status.idle": "2020-10-26T08:39:08.391499Z",
     "shell.execute_reply": "2020-10-26T08:39:08.392173Z"
    },
    "papermill": {
     "duration": 0.05766,
     "end_time": "2020-10-26T08:39:08.392333",
     "exception": false,
     "start_time": "2020-10-26T08:39:08.334673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 获得target的各列名\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:08.468771Z",
     "iopub.status.busy": "2020-10-26T08:39:08.467485Z",
     "iopub.status.idle": "2020-10-26T08:39:11.828456Z",
     "shell.execute_reply": "2020-10-26T08:39:11.827670Z"
    },
    "papermill": {
     "duration": 3.40076,
     "end_time": "2020-10-26T08:39:11.828591",
     "exception": false,
     "start_time": "2020-10-26T08:39:08.427831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             sig_id cp_time cp_dose         0         1         2         3  \\\n0      id_000644bb2      24      D1  1.147013  0.901837 -0.418317 -0.961328   \n1      id_000779bfc      72      D1  0.128837  0.676615  0.274303  0.090277   \n2      id_000a6266a      48      D1  0.790059  0.940310  1.427325 -0.121422   \n3      id_0015fd391      48      D1 -0.729887 -0.277122 -0.441238  0.766535   \n4      id_001626bd3      72      D2 -0.444200 -0.481230  0.974570  0.977613   \n...             ...     ...     ...       ...       ...       ...       ...   \n21943  id_fff8c2444      72      D1  0.247854 -1.231353  0.221655 -0.354349   \n21944  id_fffb1ceed      24      D2  0.217631 -0.026850 -0.237436 -0.787443   \n21945  id_fffb70c0c      24      D2 -1.911045  0.581948 -0.588666  1.303231   \n21946  id_fffcb9e7c      24      D1  0.826571  0.411264  0.433236  0.307423   \n21947  id_ffffdd77b      72      D1 -1.245408  1.566327 -0.269846  1.091825   \n\n              4         5         6  ...  trpv_agonist  trpv_antagonist  \\\n0     -0.254748 -1.021168 -1.368753  ...             0                0   \n1      1.208714  0.688985  0.316750  ...             0                0   \n2     -0.001534  1.494980  0.238795  ...             0                0   \n3      2.327042 -0.862822 -2.304780  ...             0                0   \n4      1.468226 -0.874366 -0.372617  ...             0                0   \n...         ...       ...       ...  ...           ...              ...   \n21943 -0.332003  0.570609 -0.150057  ...             0                0   \n21944 -0.677697  0.919339  0.743390  ...             0                0   \n21945 -1.008858  0.851933 -0.302789  ...             0                0   \n21946  1.075457 -0.024437  0.051466  ...             0                0   \n21947 -0.515807 -2.085725 -1.626974  ...             0                0   \n\n       tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n0                      0                          0   \n1                      0                          0   \n2                      0                          0   \n3                      0                          0   \n4                      0                          0   \n...                  ...                        ...   \n21943                  0                          0   \n21944                  0                          0   \n21945                  0                          0   \n21946                  0                          0   \n21947                  0                          0   \n\n       ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n0                                          0                0          0   \n1                                          0                0          0   \n2                                          0                0          0   \n3                                          0                0          0   \n4                                          0                0          0   \n...                                      ...              ...        ...   \n21943                                      0                0          0   \n21944                                      0                0          0   \n21945                                      0                0          0   \n21946                                      0                0          0   \n21947                                      0                0          0   \n\n       vitamin_d_receptor_agonist  wnt_inhibitor  kfold  \n0                               0              0      0  \n1                               0              0      2  \n2                               0              0      1  \n3                               0              0      2  \n4                               0              0      2  \n...                           ...            ...    ...  \n21943                           0              0      0  \n21944                           0              0      4  \n21945                           0              0      0  \n21946                           0              0      1  \n21947                           0              0      2  \n\n[21948 rows x 1297 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>...</th>\n      <th>trpv_agonist</th>\n      <th>trpv_antagonist</th>\n      <th>tubulin_inhibitor</th>\n      <th>tyrosine_kinase_inhibitor</th>\n      <th>ubiquitin_specific_protease_inhibitor</th>\n      <th>vegfr_inhibitor</th>\n      <th>vitamin_b</th>\n      <th>vitamin_d_receptor_agonist</th>\n      <th>wnt_inhibitor</th>\n      <th>kfold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>1.147013</td>\n      <td>0.901837</td>\n      <td>-0.418317</td>\n      <td>-0.961328</td>\n      <td>-0.254748</td>\n      <td>-1.021168</td>\n      <td>-1.368753</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_000779bfc</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.128837</td>\n      <td>0.676615</td>\n      <td>0.274303</td>\n      <td>0.090277</td>\n      <td>1.208714</td>\n      <td>0.688985</td>\n      <td>0.316750</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_000a6266a</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>0.790059</td>\n      <td>0.940310</td>\n      <td>1.427325</td>\n      <td>-0.121422</td>\n      <td>-0.001534</td>\n      <td>1.494980</td>\n      <td>0.238795</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_0015fd391</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>-0.729887</td>\n      <td>-0.277122</td>\n      <td>-0.441238</td>\n      <td>0.766535</td>\n      <td>2.327042</td>\n      <td>-0.862822</td>\n      <td>-2.304780</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_001626bd3</td>\n      <td>72</td>\n      <td>D2</td>\n      <td>-0.444200</td>\n      <td>-0.481230</td>\n      <td>0.974570</td>\n      <td>0.977613</td>\n      <td>1.468226</td>\n      <td>-0.874366</td>\n      <td>-0.372617</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21943</th>\n      <td>id_fff8c2444</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.247854</td>\n      <td>-1.231353</td>\n      <td>0.221655</td>\n      <td>-0.354349</td>\n      <td>-0.332003</td>\n      <td>0.570609</td>\n      <td>-0.150057</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21944</th>\n      <td>id_fffb1ceed</td>\n      <td>24</td>\n      <td>D2</td>\n      <td>0.217631</td>\n      <td>-0.026850</td>\n      <td>-0.237436</td>\n      <td>-0.787443</td>\n      <td>-0.677697</td>\n      <td>0.919339</td>\n      <td>0.743390</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>21945</th>\n      <td>id_fffb70c0c</td>\n      <td>24</td>\n      <td>D2</td>\n      <td>-1.911045</td>\n      <td>0.581948</td>\n      <td>-0.588666</td>\n      <td>1.303231</td>\n      <td>-1.008858</td>\n      <td>0.851933</td>\n      <td>-0.302789</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21946</th>\n      <td>id_fffcb9e7c</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>0.826571</td>\n      <td>0.411264</td>\n      <td>0.433236</td>\n      <td>0.307423</td>\n      <td>1.075457</td>\n      <td>-0.024437</td>\n      <td>0.051466</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21947</th>\n      <td>id_ffffdd77b</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>-1.245408</td>\n      <td>1.566327</td>\n      <td>-0.269846</td>\n      <td>1.091825</td>\n      <td>-0.515807</td>\n      <td>-2.085725</td>\n      <td>-1.626974</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>21948 rows × 1297 columns</p>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = train.copy()\n",
    "\n",
    "# MultilabelStratifiedKFold是用于多标签分层的K折交叉验证\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5)\n",
    "\n",
    "# 不清楚为啥非要用类型转换\n",
    "# v_idx返回的是第f+1次k折交叉验证的验证集索引\n",
    "# 至于这个x其实对函数计算并没有帮助，只不过根据官网介绍sklearn的非分层抽样要x，分层抽样算法上不用x，但为了兼容，还是要了x，本库为了与sklearn兼容，也要x了\n",
    "for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n",
    "    folds.loc[v_idx, 'kfold'] = int(f) # 添加一个新的列来记录下第f+1轮的验证集索引（与此对应的，第f+1轮，该列数值不是f的样本，即是训练集样本）\n",
    "\n",
    "folds['kfold'] = folds['kfold'].astype(int)\n",
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:11.927490Z",
     "iopub.status.busy": "2020-10-26T08:39:11.926271Z",
     "iopub.status.idle": "2020-10-26T08:39:11.933967Z",
     "shell.execute_reply": "2020-10-26T08:39:11.934853Z"
    },
    "papermill": {
     "duration": 0.068729,
     "end_time": "2020-10-26T08:39:11.935063",
     "exception": false,
     "start_time": "2020-10-26T08:39:11.866334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 1296)\n",
      "(21948, 1297)\n",
      "(3624, 1090)\n",
      "(21948, 207)\n",
      "(3982, 207)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target.shape)\n",
    "print(sample_submission.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**保存关键数据，方便py文件中调试**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def my_save_data(data, data_name):\n",
    "    # 储存数据为txt文件到当前文件夹中\n",
    "    pd.DataFrame(data).to_csv(os.getcwd()+'//'+data_name+'.txt', index=False)\n",
    "\n",
    "# 保存folds为csv文件\n",
    "data_name_list = ['folds', 'target_cols', 'test', 'target', 'train']\n",
    "for i, data in enumerate([folds, target_cols, test, target, train]):\n",
    "    my_save_data(data=data, data_name=data_name_list[i])\n",
    "\n",
    "# temp=pd.read_csv('folds.txt') # 从txt文件读取datframe类型数据的写法\n",
    "# temp=pd.read_csv('target_cols.txt').values.T[0].tolist() # 从txt文件读取list类型数据的写法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import utils\n",
    "data_name_list = ['folds', 'target_cols', 'test', 'target', 'train']\n",
    "for i, data in enumerate([folds, target_cols, test, target, train]):\n",
    "    utils.save_data(data=data, file_name=data_name_list[i])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040196,
     "end_time": "2020-10-26T08:39:12.024192",
     "exception": false,
     "start_time": "2020-10-26T08:39:11.983996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2 Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:12.114860Z",
     "iopub.status.busy": "2020-10-26T08:39:12.113971Z",
     "iopub.status.idle": "2020-10-26T08:39:12.116773Z",
     "shell.execute_reply": "2020-10-26T08:39:12.117318Z"
    },
    "papermill": {
     "duration": 0.055662,
     "end_time": "2020-10-26T08:39:12.117447",
     "exception": false,
     "start_time": "2020-10-26T08:39:12.061785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pytorch自定义数据结构类至少含有以下三个函数\n",
    "class MoADataset:\n",
    "    # 定义MoADataset数据结构\n",
    "    # torch.utils.data.DataLoader类中需要dataset数据结构\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:12.217876Z",
     "iopub.status.busy": "2020-10-26T08:39:12.216924Z",
     "iopub.status.idle": "2020-10-26T08:39:12.219466Z",
     "shell.execute_reply": "2020-10-26T08:39:12.219997Z"
    },
    "papermill": {
     "duration": 0.06305,
     "end_time": "2020-10-26T08:39:12.220135",
     "exception": false,
     "start_time": "2020-10-26T08:39:12.157085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:12.302912Z",
     "iopub.status.busy": "2020-10-26T08:39:12.302021Z",
     "iopub.status.idle": "2020-10-26T08:39:12.305050Z",
     "shell.execute_reply": "2020-10-26T08:39:12.304504Z"
    },
    "papermill": {
     "duration": 0.051229,
     "end_time": "2020-10-26T08:39:12.305156",
     "exception": false,
     "start_time": "2020-10-26T08:39:12.253927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    # @语法装饰器，相当于_smooth = staticmethod(_smooth)\n",
    "    # @staticmethod是一类特殊的用法，静态方法_smooth属于 SmoothBCEwLogits class,但是 Pizza类和实例都可以调用该方法：\n",
    "    # SmoothBCEwLogits._smooth is SmoothBCEwLogits()._smooth\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        # 断言可以在条件不满足程序运行的情况下直接返回错误, assert expression 等价于：\n",
    "        # if not expression:\n",
    "        #     raise AssertionError\n",
    "        assert 0 <= smoothing < 1\n",
    "        # Python 中的 with 语句用于异常处理，封装了 try…except…finally 编码范式，提高了易用性\n",
    "        # torch.no_grad()为上下文管理器， 就是在一个类里，实现了__enter__和__exit__的方法，这个类的实例就是一个上下文管理器。\n",
    "        # 在torch.no_grad()上下文管理器中，禁止使用梯度计算\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:12.385864Z",
     "iopub.status.busy": "2020-10-26T08:39:12.381125Z",
     "iopub.status.idle": "2020-10-26T08:39:12.388897Z",
     "shell.execute_reply": "2020-10-26T08:39:12.388353Z"
    },
    "papermill": {
     "duration": 0.050116,
     "end_time": "2020-10-26T08:39:12.389001",
     "exception": false,
     "start_time": "2020-10-26T08:39:12.338885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):      # <-- Update\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.dense2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:12.462553Z",
     "iopub.status.busy": "2020-10-26T08:39:12.461742Z",
     "iopub.status.idle": "2020-10-26T08:39:12.465168Z",
     "shell.execute_reply": "2020-10-26T08:39:12.464585Z"
    },
    "papermill": {
     "duration": 0.042185,
     "end_time": "2020-10-26T08:39:12.465279",
     "exception": false,
     "start_time": "2020-10-26T08:39:12.423094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 将'cp_time', 'cp_dose'列转换为哑结点（得到的数据，将原来的列删除，将后来得到的列放到最后）\n",
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:12.543178Z",
     "iopub.status.busy": "2020-10-26T08:39:12.541860Z",
     "iopub.status.idle": "2020-10-26T08:39:12.723622Z",
     "shell.execute_reply": "2020-10-26T08:39:12.724190Z"
    },
    "papermill": {
     "duration": 0.224338,
     "end_time": "2020-10-26T08:39:12.724355",
     "exception": false,
     "start_time": "2020-10-26T08:39:12.500017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1092"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [c for c in process_data(folds).columns if c not in target_cols] # 提取非目标列\n",
    "feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]  # 将费目标列中的这两列删去\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "['5-alpha_reductase_inhibitor',\n '11-beta-hsd1_inhibitor',\n 'acat_inhibitor',\n 'acetylcholine_receptor_agonist',\n 'acetylcholine_receptor_antagonist',\n 'acetylcholinesterase_inhibitor',\n 'adenosine_receptor_agonist',\n 'adenosine_receptor_antagonist',\n 'adenylyl_cyclase_activator',\n 'adrenergic_receptor_agonist',\n 'adrenergic_receptor_antagonist',\n 'akt_inhibitor',\n 'aldehyde_dehydrogenase_inhibitor',\n 'alk_inhibitor',\n 'ampk_activator',\n 'analgesic',\n 'androgen_receptor_agonist',\n 'androgen_receptor_antagonist',\n 'anesthetic_-_local',\n 'angiogenesis_inhibitor',\n 'angiotensin_receptor_antagonist',\n 'anti-inflammatory',\n 'antiarrhythmic',\n 'antibiotic',\n 'anticonvulsant',\n 'antifungal',\n 'antihistamine',\n 'antimalarial',\n 'antioxidant',\n 'antiprotozoal',\n 'antiviral',\n 'apoptosis_stimulant',\n 'aromatase_inhibitor',\n 'atm_kinase_inhibitor',\n 'atp-sensitive_potassium_channel_antagonist',\n 'atp_synthase_inhibitor',\n 'atpase_inhibitor',\n 'atr_kinase_inhibitor',\n 'aurora_kinase_inhibitor',\n 'autotaxin_inhibitor',\n 'bacterial_30s_ribosomal_subunit_inhibitor',\n 'bacterial_50s_ribosomal_subunit_inhibitor',\n 'bacterial_antifolate',\n 'bacterial_cell_wall_synthesis_inhibitor',\n 'bacterial_dna_gyrase_inhibitor',\n 'bacterial_dna_inhibitor',\n 'bacterial_membrane_integrity_inhibitor',\n 'bcl_inhibitor',\n 'bcr-abl_inhibitor',\n 'benzodiazepine_receptor_agonist',\n 'beta_amyloid_inhibitor',\n 'bromodomain_inhibitor',\n 'btk_inhibitor',\n 'calcineurin_inhibitor',\n 'calcium_channel_blocker',\n 'cannabinoid_receptor_agonist',\n 'cannabinoid_receptor_antagonist',\n 'carbonic_anhydrase_inhibitor',\n 'casein_kinase_inhibitor',\n 'caspase_activator',\n 'catechol_o_methyltransferase_inhibitor',\n 'cc_chemokine_receptor_antagonist',\n 'cck_receptor_antagonist',\n 'cdk_inhibitor',\n 'chelating_agent',\n 'chk_inhibitor',\n 'chloride_channel_blocker',\n 'cholesterol_inhibitor',\n 'cholinergic_receptor_antagonist',\n 'coagulation_factor_inhibitor',\n 'corticosteroid_agonist',\n 'cyclooxygenase_inhibitor',\n 'cytochrome_p450_inhibitor',\n 'dihydrofolate_reductase_inhibitor',\n 'dipeptidyl_peptidase_inhibitor',\n 'diuretic',\n 'dna_alkylating_agent',\n 'dna_inhibitor',\n 'dopamine_receptor_agonist',\n 'dopamine_receptor_antagonist',\n 'egfr_inhibitor',\n 'elastase_inhibitor',\n 'erbb2_inhibitor',\n 'estrogen_receptor_agonist',\n 'estrogen_receptor_antagonist',\n 'faah_inhibitor',\n 'farnesyltransferase_inhibitor',\n 'fatty_acid_receptor_agonist',\n 'fgfr_inhibitor',\n 'flt3_inhibitor',\n 'focal_adhesion_kinase_inhibitor',\n 'free_radical_scavenger',\n 'fungal_squalene_epoxidase_inhibitor',\n 'gaba_receptor_agonist',\n 'gaba_receptor_antagonist',\n 'gamma_secretase_inhibitor',\n 'glucocorticoid_receptor_agonist',\n 'glutamate_inhibitor',\n 'glutamate_receptor_agonist',\n 'glutamate_receptor_antagonist',\n 'gonadotropin_receptor_agonist',\n 'gsk_inhibitor',\n 'hcv_inhibitor',\n 'hdac_inhibitor',\n 'histamine_receptor_agonist',\n 'histamine_receptor_antagonist',\n 'histone_lysine_demethylase_inhibitor',\n 'histone_lysine_methyltransferase_inhibitor',\n 'hiv_inhibitor',\n 'hmgcr_inhibitor',\n 'hsp_inhibitor',\n 'igf-1_inhibitor',\n 'ikk_inhibitor',\n 'imidazoline_receptor_agonist',\n 'immunosuppressant',\n 'insulin_secretagogue',\n 'insulin_sensitizer',\n 'integrin_inhibitor',\n 'jak_inhibitor',\n 'kit_inhibitor',\n 'laxative',\n 'leukotriene_inhibitor',\n 'leukotriene_receptor_antagonist',\n 'lipase_inhibitor',\n 'lipoxygenase_inhibitor',\n 'lxr_agonist',\n 'mdm_inhibitor',\n 'mek_inhibitor',\n 'membrane_integrity_inhibitor',\n 'mineralocorticoid_receptor_antagonist',\n 'monoacylglycerol_lipase_inhibitor',\n 'monoamine_oxidase_inhibitor',\n 'monopolar_spindle_1_kinase_inhibitor',\n 'mtor_inhibitor',\n 'mucolytic_agent',\n 'neuropeptide_receptor_antagonist',\n 'nfkb_inhibitor',\n 'nicotinic_receptor_agonist',\n 'nitric_oxide_donor',\n 'nitric_oxide_production_inhibitor',\n 'nitric_oxide_synthase_inhibitor',\n 'norepinephrine_reuptake_inhibitor',\n 'nrf2_activator',\n 'opioid_receptor_agonist',\n 'opioid_receptor_antagonist',\n 'orexin_receptor_antagonist',\n 'p38_mapk_inhibitor',\n 'p-glycoprotein_inhibitor',\n 'parp_inhibitor',\n 'pdgfr_inhibitor',\n 'pdk_inhibitor',\n 'phosphodiesterase_inhibitor',\n 'phospholipase_inhibitor',\n 'pi3k_inhibitor',\n 'pkc_inhibitor',\n 'potassium_channel_activator',\n 'potassium_channel_antagonist',\n 'ppar_receptor_agonist',\n 'ppar_receptor_antagonist',\n 'progesterone_receptor_agonist',\n 'progesterone_receptor_antagonist',\n 'prostaglandin_inhibitor',\n 'prostanoid_receptor_antagonist',\n 'proteasome_inhibitor',\n 'protein_kinase_inhibitor',\n 'protein_phosphatase_inhibitor',\n 'protein_synthesis_inhibitor',\n 'protein_tyrosine_kinase_inhibitor',\n 'radiopaque_medium',\n 'raf_inhibitor',\n 'ras_gtpase_inhibitor',\n 'retinoid_receptor_agonist',\n 'retinoid_receptor_antagonist',\n 'rho_associated_kinase_inhibitor',\n 'ribonucleoside_reductase_inhibitor',\n 'rna_polymerase_inhibitor',\n 'serotonin_receptor_agonist',\n 'serotonin_receptor_antagonist',\n 'serotonin_reuptake_inhibitor',\n 'sigma_receptor_agonist',\n 'sigma_receptor_antagonist',\n 'smoothened_receptor_antagonist',\n 'sodium_channel_inhibitor',\n 'sphingosine_receptor_agonist',\n 'src_inhibitor',\n 'steroid',\n 'syk_inhibitor',\n 'tachykinin_antagonist',\n 'tgf-beta_receptor_inhibitor',\n 'thrombin_inhibitor',\n 'thymidylate_synthase_inhibitor',\n 'tlr_agonist',\n 'tlr_antagonist',\n 'tnf_inhibitor',\n 'topoisomerase_inhibitor',\n 'transient_receptor_potential_channel_antagonist',\n 'tropomyosin_receptor_kinase_inhibitor',\n 'trpv_agonist',\n 'trpv_antagonist',\n 'tubulin_inhibitor',\n 'tyrosine_kinase_inhibitor',\n 'ubiquitin_specific_protease_inhibitor',\n 'vegfr_inhibitor',\n 'vitamin_b',\n 'vitamin_d_receptor_agonist',\n 'wnt_inhibitor']"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:13.179676Z",
     "iopub.status.busy": "2020-10-26T08:39:13.178890Z",
     "iopub.status.idle": "2020-10-26T08:39:13.187147Z",
     "shell.execute_reply": "2020-10-26T08:39:13.186439Z"
    },
    "papermill": {
     "duration": 0.426442,
     "end_time": "2020-10-26T08:39:13.187284",
     "exception": false,
     "start_time": "2020-10-26T08:39:12.760842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 5e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 5            #<-- Update\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = False\n",
    "\n",
    "num_features=len(feature_cols)\n",
    "num_targets=len(target_cols)\n",
    "hidden_size=2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:13.285914Z",
     "iopub.status.busy": "2020-10-26T08:39:13.283993Z",
     "iopub.status.idle": "2020-10-26T08:39:13.286675Z",
     "shell.execute_reply": "2020-10-26T08:39:13.287204Z"
    },
    "papermill": {
     "duration": 0.063751,
     "end_time": "2020-10-26T08:39:13.287348",
     "exception": false,
     "start_time": "2020-10-26T08:39:13.223597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training(fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "\n",
    "    # 将'cp_time', 'cp_dose'列转换为哑结点（得到的数据，将原来的列删除，将后来得到的列放到最后）\n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "\n",
    "    # 根据kfold列，获得训练集与验证集索引\n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "\n",
    "    # 根据索引训练集验证集索引获得相应dataframe数据，并重置索引\n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    # 划分出训练集与验证集的特征与目标\n",
    "    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "\n",
    "    # 通过特征与目标，实例化自定义dataset类\n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "\n",
    "    # 利用自定义dataset类，实例化DataLoader类\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=WEIGHT_DECAY)\n",
    "    # tortch.optim.lr_scheduler.OneCycleLR类，一种学习率策略\n",
    "    # The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate.\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "    \n",
    "    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "        print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"SEED: {seed} ,FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:13.366713Z",
     "iopub.status.busy": "2020-10-26T08:39:13.364796Z",
     "iopub.status.idle": "2020-10-26T08:39:13.367435Z",
     "shell.execute_reply": "2020-10-26T08:39:13.367957Z"
    },
    "papermill": {
     "duration": 0.046137,
     "end_time": "2020-10-26T08:39:13.368103",
     "exception": false,
     "start_time": "2020-10-26T08:39:13.321966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "    # 注意这个NFOLDS和之前MultilabelStratifiedKFold的轮次相同（前面run_training函数也有体现）\n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:39:13.449119Z",
     "iopub.status.busy": "2020-10-26T08:39:13.448059Z",
     "iopub.status.idle": "2020-10-26T08:59:18.860452Z",
     "shell.execute_reply": "2020-10-26T08:59:18.859804Z"
    },
    "papermill": {
     "duration": 1205.456514,
     "end_time": "2020-10-26T08:59:18.860571",
     "exception": false,
     "start_time": "2020-10-26T08:39:13.404057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 940, FOLD: 0, EPOCH: 0, train_loss: 0.4709183501241648\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 0, valid_loss: 0.02435992184494223\n",
      "SEED: 940, FOLD: 0, EPOCH: 1, train_loss: 0.02370495936306922\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 1, valid_loss: 0.018982262590101787\n",
      "SEED: 940, FOLD: 0, EPOCH: 2, train_loss: 0.022423562053860962\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 2, valid_loss: 0.19759794239486966\n",
      "SEED: 940, FOLD: 0, EPOCH: 3, train_loss: 0.02696398992523335\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 3, valid_loss: 0.0197769112352814\n",
      "SEED: 940, FOLD: 0, EPOCH: 4, train_loss: 0.02219309817081776\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 4, valid_loss: 0.018827627652457782\n",
      "SEED: 940, FOLD: 0, EPOCH: 5, train_loss: 0.02147800004298704\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 5, valid_loss: 0.017931388478193963\n",
      "SEED: 940, FOLD: 0, EPOCH: 6, train_loss: 0.021078649692345356\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 6, valid_loss: 0.017856099749250072\n",
      "SEED: 940, FOLD: 0, EPOCH: 7, train_loss: 0.02083339995664099\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 7, valid_loss: 0.01763602096055235\n",
      "SEED: 940, FOLD: 0, EPOCH: 8, train_loss: 0.020561822501105675\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 8, valid_loss: 0.017541030049324037\n",
      "SEED: 940, FOLD: 0, EPOCH: 9, train_loss: 0.02037095907481684\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 9, valid_loss: 0.017464678628104073\n",
      "SEED: 940, FOLD: 0, EPOCH: 10, train_loss: 0.020362722161023514\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 10, valid_loss: 0.017246866918035914\n",
      "SEED: 940, FOLD: 0, EPOCH: 11, train_loss: 0.020304735358534515\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 11, valid_loss: 0.017145251190023764\n",
      "SEED: 940, FOLD: 0, EPOCH: 12, train_loss: 0.020105619374932587\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 12, valid_loss: 0.017052982508071832\n",
      "SEED: 940, FOLD: 0, EPOCH: 13, train_loss: 0.019908011540014675\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 13, valid_loss: 0.017068619041570594\n",
      "SEED: 940, FOLD: 0, EPOCH: 14, train_loss: 0.01981486528571965\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 14, valid_loss: 0.016981784706669195\n",
      "SEED: 940, FOLD: 0, EPOCH: 15, train_loss: 0.019628590998658234\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 15, valid_loss: 0.016936446486839226\n",
      "SEED: 940, FOLD: 0, EPOCH: 16, train_loss: 0.019413116334033184\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 16, valid_loss: 0.016612107466374124\n",
      "SEED: 940, FOLD: 0, EPOCH: 17, train_loss: 0.019089549874373966\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 17, valid_loss: 0.016522292739578656\n",
      "SEED: 940, FOLD: 0, EPOCH: 18, train_loss: 0.018798445837329262\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 18, valid_loss: 0.016432406300944943\n",
      "SEED: 940, FOLD: 0, EPOCH: 19, train_loss: 0.018438818554083507\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 19, valid_loss: 0.016274431188191687\n",
      "SEED: 940, FOLD: 0, EPOCH: 20, train_loss: 0.017839221735957308\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 20, valid_loss: 0.01619435160287789\n",
      "SEED: 940, FOLD: 0, EPOCH: 21, train_loss: 0.0172061738288165\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 21, valid_loss: 0.016091544872948102\n",
      "SEED: 940, FOLD: 0, EPOCH: 22, train_loss: 0.01647981522820782\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 22, valid_loss: 0.016064705247325558\n",
      "SEED: 940, FOLD: 0, EPOCH: 23, train_loss: 0.0159023096649975\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 23, valid_loss: 0.01607203685811588\n",
      "SEED: 940, FOLD: 0, EPOCH: 24, train_loss: 0.015608426561390144\n",
      "SEED: 940 ,FOLD: 0, EPOCH: 24, valid_loss: 0.01608249129993575\n",
      "SEED: 940, FOLD: 1, EPOCH: 0, train_loss: 0.4704194545367922\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 0, valid_loss: 0.024419602911387173\n",
      "SEED: 940, FOLD: 1, EPOCH: 1, train_loss: 0.023973107648392517\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 1, valid_loss: 0.019003621063062123\n",
      "SEED: 940, FOLD: 1, EPOCH: 2, train_loss: 0.022013392758326256\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 2, valid_loss: 0.017801297988210407\n",
      "SEED: 940, FOLD: 1, EPOCH: 3, train_loss: 0.020970523181924786\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 3, valid_loss: 0.017705428201173032\n",
      "SEED: 940, FOLD: 1, EPOCH: 4, train_loss: 0.020382598771349243\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 4, valid_loss: 0.025367588150714125\n",
      "SEED: 940, FOLD: 1, EPOCH: 5, train_loss: 0.02027046704745811\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 5, valid_loss: 0.0172042134350964\n",
      "SEED: 940, FOLD: 1, EPOCH: 6, train_loss: 0.02016705773986768\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 6, valid_loss: 0.017744787835649083\n",
      "SEED: 940, FOLD: 1, EPOCH: 7, train_loss: 0.02023502339379511\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 7, valid_loss: 0.017199788029704774\n",
      "SEED: 940, FOLD: 1, EPOCH: 8, train_loss: 0.020216056159225063\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 8, valid_loss: 0.017734692458595547\n",
      "SEED: 940, FOLD: 1, EPOCH: 9, train_loss: 0.02017978573406952\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 9, valid_loss: 0.016986782023949282\n",
      "SEED: 940, FOLD: 1, EPOCH: 10, train_loss: 0.020127625407084175\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 10, valid_loss: 0.017367938293942384\n",
      "SEED: 940, FOLD: 1, EPOCH: 11, train_loss: 0.02010986847344084\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 11, valid_loss: 0.01732804070093802\n",
      "SEED: 940, FOLD: 1, EPOCH: 12, train_loss: 0.019976887001615505\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 12, valid_loss: 0.017075599410704204\n",
      "SEED: 940, FOLD: 1, EPOCH: 13, train_loss: 0.019876638899786747\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 13, valid_loss: 0.016892397297280176\n",
      "SEED: 940, FOLD: 1, EPOCH: 14, train_loss: 0.019767254914926445\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 14, valid_loss: 0.016769281269184182\n",
      "SEED: 940, FOLD: 1, EPOCH: 15, train_loss: 0.01957765573878651\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 15, valid_loss: 0.016678175223725184\n",
      "SEED: 940, FOLD: 1, EPOCH: 16, train_loss: 0.019342100666161034\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 16, valid_loss: 0.016611262623752867\n",
      "SEED: 940, FOLD: 1, EPOCH: 17, train_loss: 0.019115935430686543\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 17, valid_loss: 0.016548083855637483\n",
      "SEED: 940, FOLD: 1, EPOCH: 18, train_loss: 0.01878276167680388\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 18, valid_loss: 0.016398483674441064\n",
      "SEED: 940, FOLD: 1, EPOCH: 19, train_loss: 0.018359165604941656\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 19, valid_loss: 0.016208718184913908\n",
      "SEED: 940, FOLD: 1, EPOCH: 20, train_loss: 0.017885826521323644\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 20, valid_loss: 0.016137864493897983\n",
      "SEED: 940, FOLD: 1, EPOCH: 21, train_loss: 0.017341827926482412\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 21, valid_loss: 0.01616766910467829\n",
      "SEED: 940, FOLD: 1, EPOCH: 22, train_loss: 0.01678569973918839\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 22, valid_loss: 0.0160795393000756\n",
      "SEED: 940, FOLD: 1, EPOCH: 23, train_loss: 0.016322451002517904\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 23, valid_loss: 0.016107541136443616\n",
      "SEED: 940, FOLD: 1, EPOCH: 24, train_loss: 0.016109651069332293\n",
      "SEED: 940 ,FOLD: 1, EPOCH: 24, valid_loss: 0.01611186470836401\n",
      "SEED: 940, FOLD: 2, EPOCH: 0, train_loss: 0.47202956671084184\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 0, valid_loss: 0.023940478691032954\n",
      "SEED: 940, FOLD: 2, EPOCH: 1, train_loss: 0.02379286386396574\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 1, valid_loss: 0.02007284840302808\n",
      "SEED: 940, FOLD: 2, EPOCH: 2, train_loss: 0.022349546083073685\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 2, valid_loss: 0.018017038250608103\n",
      "SEED: 940, FOLD: 2, EPOCH: 3, train_loss: 0.02080060507211348\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 3, valid_loss: 0.017325899404074464\n",
      "SEED: 940, FOLD: 2, EPOCH: 4, train_loss: 0.020333806228270565\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 4, valid_loss: 0.017914795636066367\n",
      "SEED: 940, FOLD: 2, EPOCH: 5, train_loss: 0.020303647303818794\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 5, valid_loss: 0.017916719801723956\n",
      "SEED: 940, FOLD: 2, EPOCH: 6, train_loss: 0.02025979367233273\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 6, valid_loss: 0.017362317629158498\n",
      "SEED: 940, FOLD: 2, EPOCH: 7, train_loss: 0.020153914132843846\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 7, valid_loss: 0.017382977663406303\n",
      "SEED: 940, FOLD: 2, EPOCH: 8, train_loss: 0.020214773404101532\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 8, valid_loss: 0.017248035834303923\n",
      "SEED: 940, FOLD: 2, EPOCH: 9, train_loss: 0.02020863262747509\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 9, valid_loss: 0.017458978375153884\n",
      "SEED: 940, FOLD: 2, EPOCH: 10, train_loss: 0.02018754809177008\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 10, valid_loss: 0.01715407432722194\n",
      "SEED: 940, FOLD: 2, EPOCH: 11, train_loss: 0.020051922946088555\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 11, valid_loss: 0.01734750544918435\n",
      "SEED: 940, FOLD: 2, EPOCH: 12, train_loss: 0.01999377619907044\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 12, valid_loss: 0.016962853419993606\n",
      "SEED: 940, FOLD: 2, EPOCH: 13, train_loss: 0.019889417279889618\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 13, valid_loss: 0.016798579639622144\n",
      "SEED: 940, FOLD: 2, EPOCH: 14, train_loss: 0.019846785206185734\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 14, valid_loss: 0.01682485443140779\n",
      "SEED: 940, FOLD: 2, EPOCH: 15, train_loss: 0.019639391533058624\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 15, valid_loss: 0.016836187988519668\n",
      "SEED: 940, FOLD: 2, EPOCH: 16, train_loss: 0.019410286058226357\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 16, valid_loss: 0.016701341686504226\n",
      "SEED: 940, FOLD: 2, EPOCH: 17, train_loss: 0.019167104061099068\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 17, valid_loss: 0.01647159672741379\n",
      "SEED: 940, FOLD: 2, EPOCH: 18, train_loss: 0.018766956727789795\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 18, valid_loss: 0.016334400565496513\n",
      "SEED: 940, FOLD: 2, EPOCH: 19, train_loss: 0.018369029147847406\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 19, valid_loss: 0.0161956430545875\n",
      "SEED: 940, FOLD: 2, EPOCH: 20, train_loss: 0.017896279780383127\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 20, valid_loss: 0.0161197604611516\n",
      "SEED: 940, FOLD: 2, EPOCH: 21, train_loss: 0.017319442684073812\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 21, valid_loss: 0.016043363511562346\n",
      "SEED: 940, FOLD: 2, EPOCH: 22, train_loss: 0.01674472865905019\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 22, valid_loss: 0.01600961498916149\n",
      "SEED: 940, FOLD: 2, EPOCH: 23, train_loss: 0.01635407809627013\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 23, valid_loss: 0.015979373322001524\n",
      "SEED: 940, FOLD: 2, EPOCH: 24, train_loss: 0.016088109733401867\n",
      "SEED: 940 ,FOLD: 2, EPOCH: 24, valid_loss: 0.015996911775852954\n",
      "SEED: 940, FOLD: 3, EPOCH: 0, train_loss: 0.4709181446123166\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 0, valid_loss: 0.024337845402104513\n",
      "SEED: 940, FOLD: 3, EPOCH: 1, train_loss: 0.023582245371695877\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 1, valid_loss: 0.019250670873693056\n",
      "SEED: 940, FOLD: 3, EPOCH: 2, train_loss: 0.022089666376511257\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 2, valid_loss: 0.021840843398656162\n",
      "SEED: 940, FOLD: 3, EPOCH: 3, train_loss: 0.02085269747328931\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 3, valid_loss: 0.018215994244175297\n",
      "SEED: 940, FOLD: 3, EPOCH: 4, train_loss: 0.02106206746690947\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 4, valid_loss: 0.017550770273166042\n",
      "SEED: 940, FOLD: 3, EPOCH: 5, train_loss: 0.020320898259355537\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 5, valid_loss: 0.017523198548172202\n",
      "SEED: 940, FOLD: 3, EPOCH: 6, train_loss: 0.020200871552030247\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 6, valid_loss: 0.017381146736443042\n",
      "SEED: 940, FOLD: 3, EPOCH: 7, train_loss: 0.020196342489857605\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 7, valid_loss: 0.01741053964942694\n",
      "SEED: 940, FOLD: 3, EPOCH: 8, train_loss: 0.02016543557840413\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 8, valid_loss: 0.017352142930030824\n",
      "SEED: 940, FOLD: 3, EPOCH: 9, train_loss: 0.020139723219841286\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 9, valid_loss: 0.017860083867396626\n",
      "SEED: 940, FOLD: 3, EPOCH: 10, train_loss: 0.020089799632736736\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 10, valid_loss: 0.017198699260396615\n",
      "SEED: 940, FOLD: 3, EPOCH: 11, train_loss: 0.020036629522624222\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 11, valid_loss: 0.01727150066622666\n",
      "SEED: 940, FOLD: 3, EPOCH: 12, train_loss: 0.01991306702889826\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 12, valid_loss: 0.017020337735967977\n",
      "SEED: 940, FOLD: 3, EPOCH: 13, train_loss: 0.019852216328507748\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 13, valid_loss: 0.017014455316322192\n",
      "SEED: 940, FOLD: 3, EPOCH: 14, train_loss: 0.01965634880479479\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 14, valid_loss: 0.01687942732657705\n",
      "SEED: 940, FOLD: 3, EPOCH: 15, train_loss: 0.01946148303323898\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 15, valid_loss: 0.01692817115357944\n",
      "SEED: 940, FOLD: 3, EPOCH: 16, train_loss: 0.019190356189358063\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 16, valid_loss: 0.01678420387740646\n",
      "SEED: 940, FOLD: 3, EPOCH: 17, train_loss: 0.019004849366087845\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 17, valid_loss: 0.016646772623062134\n",
      "SEED: 940, FOLD: 3, EPOCH: 18, train_loss: 0.018662202131489048\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 18, valid_loss: 0.016541255584784916\n",
      "SEED: 940, FOLD: 3, EPOCH: 19, train_loss: 0.01821468894680341\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 19, valid_loss: 0.016384318683828628\n",
      "SEED: 940, FOLD: 3, EPOCH: 20, train_loss: 0.01767343755784458\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 20, valid_loss: 0.01630908450377839\n",
      "SEED: 940, FOLD: 3, EPOCH: 21, train_loss: 0.017033579307135897\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 21, valid_loss: 0.01633129385965211\n",
      "SEED: 940, FOLD: 3, EPOCH: 22, train_loss: 0.0164423826272073\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 22, valid_loss: 0.016258980174149784\n",
      "SEED: 940, FOLD: 3, EPOCH: 23, train_loss: 0.015974095057480146\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 23, valid_loss: 0.016228105686604977\n",
      "SEED: 940, FOLD: 3, EPOCH: 24, train_loss: 0.015726723552991945\n",
      "SEED: 940 ,FOLD: 3, EPOCH: 24, valid_loss: 0.01623250895312854\n",
      "SEED: 940, FOLD: 4, EPOCH: 0, train_loss: 0.4710008591224534\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 0, valid_loss: 0.023787930554577282\n",
      "SEED: 940, FOLD: 4, EPOCH: 1, train_loss: 0.02376572839051917\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 1, valid_loss: 0.019008999743631908\n",
      "SEED: 940, FOLD: 4, EPOCH: 2, train_loss: 0.022143217559525932\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 2, valid_loss: 0.020839010551571845\n",
      "SEED: 940, FOLD: 4, EPOCH: 3, train_loss: 0.02094468547274237\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 3, valid_loss: 0.017402057509337154\n",
      "SEED: 940, FOLD: 4, EPOCH: 4, train_loss: 0.02029474678894748\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 4, valid_loss: 0.017446813599339552\n",
      "SEED: 940, FOLD: 4, EPOCH: 5, train_loss: 0.020172014832496643\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 5, valid_loss: 0.017411379995090622\n",
      "SEED: 940, FOLD: 4, EPOCH: 6, train_loss: 0.0201703407464252\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 6, valid_loss: 0.017301107623747418\n",
      "SEED: 940, FOLD: 4, EPOCH: 7, train_loss: 0.02017594825314439\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 7, valid_loss: 0.017151138692029885\n",
      "SEED: 940, FOLD: 4, EPOCH: 8, train_loss: 0.02018253340561321\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 8, valid_loss: 0.017169712989458016\n",
      "SEED: 940, FOLD: 4, EPOCH: 9, train_loss: 0.020105974802720375\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 9, valid_loss: 0.017321920607771192\n",
      "SEED: 940, FOLD: 4, EPOCH: 10, train_loss: 0.020151815809093525\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 10, valid_loss: 0.01703192458621093\n",
      "SEED: 940, FOLD: 4, EPOCH: 11, train_loss: 0.0200646897588951\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 11, valid_loss: 0.01692863614963634\n",
      "SEED: 940, FOLD: 4, EPOCH: 12, train_loss: 0.02002432079904753\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 12, valid_loss: 0.016884873355073587\n",
      "SEED: 940, FOLD: 4, EPOCH: 13, train_loss: 0.01980974226245198\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 13, valid_loss: 0.01676419973373413\n",
      "SEED: 940, FOLD: 4, EPOCH: 14, train_loss: 0.019757117684660614\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 14, valid_loss: 0.016818313699747834\n",
      "SEED: 940, FOLD: 4, EPOCH: 15, train_loss: 0.019492745831392815\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 15, valid_loss: 0.016644010825880937\n",
      "SEED: 940, FOLD: 4, EPOCH: 16, train_loss: 0.019391647364566292\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 16, valid_loss: 0.016530682519078254\n",
      "SEED: 940, FOLD: 4, EPOCH: 17, train_loss: 0.019155940486361153\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 17, valid_loss: 0.0164642120046275\n",
      "SEED: 940, FOLD: 4, EPOCH: 18, train_loss: 0.01870253551211478\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 18, valid_loss: 0.016341626564306874\n",
      "SEED: 940, FOLD: 4, EPOCH: 19, train_loss: 0.0183280516496819\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 19, valid_loss: 0.016081061347254684\n",
      "SEED: 940, FOLD: 4, EPOCH: 20, train_loss: 0.017800892649245434\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 20, valid_loss: 0.016094994225672312\n",
      "SEED: 940, FOLD: 4, EPOCH: 21, train_loss: 0.017250229084891253\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 21, valid_loss: 0.01602255670087678\n",
      "SEED: 940, FOLD: 4, EPOCH: 22, train_loss: 0.01668483103234051\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 22, valid_loss: 0.015968648796635013\n",
      "SEED: 940, FOLD: 4, EPOCH: 23, train_loss: 0.016187968150969\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 23, valid_loss: 0.01601396825696741\n",
      "SEED: 940, FOLD: 4, EPOCH: 24, train_loss: 0.016003150411921997\n",
      "SEED: 940 ,FOLD: 4, EPOCH: 24, valid_loss: 0.0160243865900806\n",
      "SEED: 1513, FOLD: 0, EPOCH: 0, train_loss: 0.47125363256782293\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 0, valid_loss: 0.023492583792124475\n",
      "SEED: 1513, FOLD: 0, EPOCH: 1, train_loss: 0.02368433105394892\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 1, valid_loss: 0.018845211767724582\n",
      "SEED: 1513, FOLD: 0, EPOCH: 2, train_loss: 0.021815786853540634\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 2, valid_loss: 0.0176871299211468\n",
      "SEED: 1513, FOLD: 0, EPOCH: 3, train_loss: 0.0214578446365245\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 3, valid_loss: 0.017439738235303332\n",
      "SEED: 1513, FOLD: 0, EPOCH: 4, train_loss: 0.020361897022719833\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 4, valid_loss: 0.01704080993575709\n",
      "SEED: 1513, FOLD: 0, EPOCH: 5, train_loss: 0.020137995400506516\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 5, valid_loss: 0.017419675551354884\n",
      "SEED: 1513, FOLD: 0, EPOCH: 6, train_loss: 0.020203199075615925\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 6, valid_loss: 0.017026042991450854\n",
      "SEED: 1513, FOLD: 0, EPOCH: 7, train_loss: 0.02025750188993803\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 7, valid_loss: 0.017296269509409157\n",
      "SEED: 1513, FOLD: 0, EPOCH: 8, train_loss: 0.020109508928937325\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 8, valid_loss: 0.01729679804827486\n",
      "SEED: 1513, FOLD: 0, EPOCH: 9, train_loss: 0.020124138677087816\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 9, valid_loss: 0.017243025824427605\n",
      "SEED: 1513, FOLD: 0, EPOCH: 10, train_loss: 0.02009579768755298\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 10, valid_loss: 0.01710557376167604\n",
      "SEED: 1513, FOLD: 0, EPOCH: 11, train_loss: 0.01998265489153024\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 11, valid_loss: 0.01711919443415744\n",
      "SEED: 1513, FOLD: 0, EPOCH: 12, train_loss: 0.020001619068932705\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 12, valid_loss: 0.0169887940798487\n",
      "SEED: 1513, FOLD: 0, EPOCH: 13, train_loss: 0.01986074994277695\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 13, valid_loss: 0.01677069291472435\n",
      "SEED: 1513, FOLD: 0, EPOCH: 14, train_loss: 0.019688173588635265\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 14, valid_loss: 0.016729571191327914\n",
      "SEED: 1513, FOLD: 0, EPOCH: 15, train_loss: 0.01952582948665688\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 15, valid_loss: 0.016789526918104716\n",
      "SEED: 1513, FOLD: 0, EPOCH: 16, train_loss: 0.01936363420732643\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 16, valid_loss: 0.01663347849888461\n",
      "SEED: 1513, FOLD: 0, EPOCH: 17, train_loss: 0.019055943139761253\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 17, valid_loss: 0.016474015824496747\n",
      "SEED: 1513, FOLD: 0, EPOCH: 18, train_loss: 0.018691091042389904\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 18, valid_loss: 0.016394938475319316\n",
      "SEED: 1513, FOLD: 0, EPOCH: 19, train_loss: 0.018282876484959885\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 19, valid_loss: 0.01623474218483482\n",
      "SEED: 1513, FOLD: 0, EPOCH: 20, train_loss: 0.017760360289527023\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 20, valid_loss: 0.016164782084524633\n",
      "SEED: 1513, FOLD: 0, EPOCH: 21, train_loss: 0.017235552946078606\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 21, valid_loss: 0.01609251219779253\n",
      "SEED: 1513, FOLD: 0, EPOCH: 22, train_loss: 0.016674826299582703\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 22, valid_loss: 0.016062770916947296\n",
      "SEED: 1513, FOLD: 0, EPOCH: 23, train_loss: 0.016259265375202118\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 23, valid_loss: 0.016048640199005603\n",
      "SEED: 1513, FOLD: 0, EPOCH: 24, train_loss: 0.01596769526519853\n",
      "SEED: 1513 ,FOLD: 0, EPOCH: 24, valid_loss: 0.01606174348188298\n",
      "SEED: 1513, FOLD: 1, EPOCH: 0, train_loss: 0.47118743646727956\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 0, valid_loss: 0.024029024903263363\n",
      "SEED: 1513, FOLD: 1, EPOCH: 1, train_loss: 0.023709273816126843\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 1, valid_loss: 0.020089982556445257\n",
      "SEED: 1513, FOLD: 1, EPOCH: 2, train_loss: 0.0220016831856059\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 2, valid_loss: 0.019051498440759523\n",
      "SEED: 1513, FOLD: 1, EPOCH: 3, train_loss: 0.022161536755553192\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 3, valid_loss: 0.018081257093165604\n",
      "SEED: 1513, FOLD: 1, EPOCH: 4, train_loss: 0.02067199994580469\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 4, valid_loss: 0.0181653497208442\n",
      "SEED: 1513, FOLD: 1, EPOCH: 5, train_loss: 0.020336838142163513\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 5, valid_loss: 0.01758133109126772\n",
      "SEED: 1513, FOLD: 1, EPOCH: 6, train_loss: 0.020262343485070312\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 6, valid_loss: 0.017367703387779848\n",
      "SEED: 1513, FOLD: 1, EPOCH: 7, train_loss: 0.0202368320627273\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 7, valid_loss: 0.017166340856679847\n",
      "SEED: 1513, FOLD: 1, EPOCH: 8, train_loss: 0.020256798849373623\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 8, valid_loss: 0.017416519884552274\n",
      "SEED: 1513, FOLD: 1, EPOCH: 9, train_loss: 0.020166489390143448\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 9, valid_loss: 0.01714357914669173\n",
      "SEED: 1513, FOLD: 1, EPOCH: 10, train_loss: 0.020092208788770695\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 10, valid_loss: 0.017149682103523186\n",
      "SEED: 1513, FOLD: 1, EPOCH: 11, train_loss: 0.020086473771843357\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 11, valid_loss: 0.017101635092071125\n",
      "SEED: 1513, FOLD: 1, EPOCH: 12, train_loss: 0.019993345064205536\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 12, valid_loss: 0.01699997450092009\n",
      "SEED: 1513, FOLD: 1, EPOCH: 13, train_loss: 0.019891449493234573\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 13, valid_loss: 0.016875293771071092\n",
      "SEED: 1513, FOLD: 1, EPOCH: 14, train_loss: 0.019741076382174007\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 14, valid_loss: 0.016875737054007394\n",
      "SEED: 1513, FOLD: 1, EPOCH: 15, train_loss: 0.019602229251809742\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 15, valid_loss: 0.01682264357805252\n",
      "SEED: 1513, FOLD: 1, EPOCH: 16, train_loss: 0.01941767359233421\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 16, valid_loss: 0.01672101438577686\n",
      "SEED: 1513, FOLD: 1, EPOCH: 17, train_loss: 0.019117572898234146\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 17, valid_loss: 0.016486127275441374\n",
      "SEED: 1513, FOLD: 1, EPOCH: 18, train_loss: 0.01877651015377563\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 18, valid_loss: 0.016420897229441574\n",
      "SEED: 1513, FOLD: 1, EPOCH: 19, train_loss: 0.01840900811974121\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 19, valid_loss: 0.016313683401261058\n",
      "SEED: 1513, FOLD: 1, EPOCH: 20, train_loss: 0.017952329904326925\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 20, valid_loss: 0.016152850006307874\n",
      "SEED: 1513, FOLD: 1, EPOCH: 21, train_loss: 0.017322214090845722\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 21, valid_loss: 0.016099566646984646\n",
      "SEED: 1513, FOLD: 1, EPOCH: 22, train_loss: 0.01674352904808694\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 22, valid_loss: 0.016034812347165178\n",
      "SEED: 1513, FOLD: 1, EPOCH: 23, train_loss: 0.016260621617075758\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 23, valid_loss: 0.01610408185848168\n",
      "SEED: 1513, FOLD: 1, EPOCH: 24, train_loss: 0.01598386573807701\n",
      "SEED: 1513 ,FOLD: 1, EPOCH: 24, valid_loss: 0.01612091814833028\n",
      "SEED: 1513, FOLD: 2, EPOCH: 0, train_loss: 0.4713596302772994\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 0, valid_loss: 0.023695967559303557\n",
      "SEED: 1513, FOLD: 2, EPOCH: 1, train_loss: 0.023837491343526737\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 1, valid_loss: 0.018996097786085946\n",
      "SEED: 1513, FOLD: 2, EPOCH: 2, train_loss: 0.022179296334692532\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 2, valid_loss: 0.018050930462777615\n",
      "SEED: 1513, FOLD: 2, EPOCH: 3, train_loss: 0.020724769668194695\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 3, valid_loss: 0.01795265312705721\n",
      "SEED: 1513, FOLD: 2, EPOCH: 4, train_loss: 0.020289421000558396\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 4, valid_loss: 0.017102247928934437\n",
      "SEED: 1513, FOLD: 2, EPOCH: 5, train_loss: 0.020405070113854996\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 5, valid_loss: 0.017291204844202313\n",
      "SEED: 1513, FOLD: 2, EPOCH: 6, train_loss: 0.020185003785983376\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 6, valid_loss: 0.017407210118004255\n",
      "SEED: 1513, FOLD: 2, EPOCH: 7, train_loss: 0.020243770213446755\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 7, valid_loss: 0.017125762945839337\n",
      "SEED: 1513, FOLD: 2, EPOCH: 8, train_loss: 0.020246024877912758\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 8, valid_loss: 0.017329884852681842\n",
      "SEED: 1513, FOLD: 2, EPOCH: 9, train_loss: 0.020157736132218353\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 9, valid_loss: 0.017261507362127303\n",
      "SEED: 1513, FOLD: 2, EPOCH: 10, train_loss: 0.020250429466798687\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 10, valid_loss: 0.01712622799511467\n",
      "SEED: 1513, FOLD: 2, EPOCH: 11, train_loss: 0.02002674871650727\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 11, valid_loss: 0.01708409400390727\n",
      "SEED: 1513, FOLD: 2, EPOCH: 12, train_loss: 0.02000696680414072\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 12, valid_loss: 0.017066254626427378\n",
      "SEED: 1513, FOLD: 2, EPOCH: 13, train_loss: 0.01990442570514869\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 13, valid_loss: 0.01697398227240358\n",
      "SEED: 1513, FOLD: 2, EPOCH: 14, train_loss: 0.019771467327423718\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 14, valid_loss: 0.016882460564374924\n",
      "SEED: 1513, FOLD: 2, EPOCH: 15, train_loss: 0.019665962314584118\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 15, valid_loss: 0.016860455673720156\n",
      "SEED: 1513, FOLD: 2, EPOCH: 16, train_loss: 0.019440455942590168\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 16, valid_loss: 0.01657537112810782\n",
      "SEED: 1513, FOLD: 2, EPOCH: 17, train_loss: 0.01911494863368031\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 17, valid_loss: 0.01645082538681371\n",
      "SEED: 1513, FOLD: 2, EPOCH: 18, train_loss: 0.018834147818278576\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 18, valid_loss: 0.016334868648222516\n",
      "SEED: 1513, FOLD: 2, EPOCH: 19, train_loss: 0.01843342362511633\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 19, valid_loss: 0.016189550129430633\n",
      "SEED: 1513, FOLD: 2, EPOCH: 20, train_loss: 0.017991116709089365\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 20, valid_loss: 0.01609887257218361\n",
      "SEED: 1513, FOLD: 2, EPOCH: 21, train_loss: 0.017414691453070744\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 21, valid_loss: 0.016086678047265324\n",
      "SEED: 1513, FOLD: 2, EPOCH: 22, train_loss: 0.016932212322464456\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 22, valid_loss: 0.01604015909667526\n",
      "SEED: 1513, FOLD: 2, EPOCH: 23, train_loss: 0.016488117071381515\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 23, valid_loss: 0.016005351740334715\n",
      "SEED: 1513, FOLD: 2, EPOCH: 24, train_loss: 0.016318716115547693\n",
      "SEED: 1513 ,FOLD: 2, EPOCH: 24, valid_loss: 0.016032393276691436\n",
      "SEED: 1513, FOLD: 3, EPOCH: 0, train_loss: 0.47040000505259505\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 0, valid_loss: 0.023958445233958108\n",
      "SEED: 1513, FOLD: 3, EPOCH: 1, train_loss: 0.023824243671328262\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 1, valid_loss: 0.019037474585430963\n",
      "SEED: 1513, FOLD: 3, EPOCH: 2, train_loss: 0.02178921423636485\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 2, valid_loss: 0.017911249878151077\n",
      "SEED: 1513, FOLD: 3, EPOCH: 3, train_loss: 0.020695171966824844\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 3, valid_loss: 0.10483988555414336\n",
      "SEED: 1513, FOLD: 3, EPOCH: 4, train_loss: 0.02095442406995141\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 4, valid_loss: 0.01733371495668377\n",
      "SEED: 1513, FOLD: 3, EPOCH: 5, train_loss: 0.020263435912952907\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 5, valid_loss: 0.01752538023782628\n",
      "SEED: 1513, FOLD: 3, EPOCH: 6, train_loss: 0.02016678455191246\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 6, valid_loss: 0.01773419851171119\n",
      "SEED: 1513, FOLD: 3, EPOCH: 7, train_loss: 0.020303199634603832\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 7, valid_loss: 0.01749210927103247\n",
      "SEED: 1513, FOLD: 3, EPOCH: 8, train_loss: 0.020166066245756287\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 8, valid_loss: 0.017802650667726995\n",
      "SEED: 1513, FOLD: 3, EPOCH: 9, train_loss: 0.020123499756058056\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 9, valid_loss: 0.01732165262635265\n",
      "SEED: 1513, FOLD: 3, EPOCH: 10, train_loss: 0.020118599190660145\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 10, valid_loss: 0.017435373153005328\n",
      "SEED: 1513, FOLD: 3, EPOCH: 11, train_loss: 0.020060824689225876\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 11, valid_loss: 0.017145061838839737\n",
      "SEED: 1513, FOLD: 3, EPOCH: 12, train_loss: 0.01992913172242866\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 12, valid_loss: 0.017230042735380785\n",
      "SEED: 1513, FOLD: 3, EPOCH: 13, train_loss: 0.01985332041816867\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 13, valid_loss: 0.017098084118749413\n",
      "SEED: 1513, FOLD: 3, EPOCH: 14, train_loss: 0.019692253415891224\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 14, valid_loss: 0.01695863881281444\n",
      "SEED: 1513, FOLD: 3, EPOCH: 15, train_loss: 0.019558613890431065\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 15, valid_loss: 0.016929261306566853\n",
      "SEED: 1513, FOLD: 3, EPOCH: 16, train_loss: 0.019396705336976742\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 16, valid_loss: 0.01675087484930243\n",
      "SEED: 1513, FOLD: 3, EPOCH: 17, train_loss: 0.019046755750542103\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 17, valid_loss: 0.01670112418276923\n",
      "SEED: 1513, FOLD: 3, EPOCH: 18, train_loss: 0.01870542192372723\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 18, valid_loss: 0.016524406815213815\n",
      "SEED: 1513, FOLD: 3, EPOCH: 19, train_loss: 0.0182927460477188\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 19, valid_loss: 0.01639805039657014\n",
      "SEED: 1513, FOLD: 3, EPOCH: 20, train_loss: 0.017787238515481568\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 20, valid_loss: 0.016296707838773728\n",
      "SEED: 1513, FOLD: 3, EPOCH: 21, train_loss: 0.017228991819032723\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 21, valid_loss: 0.016293300821312837\n",
      "SEED: 1513, FOLD: 3, EPOCH: 22, train_loss: 0.016641238117185625\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 22, valid_loss: 0.01624275339501245\n",
      "SEED: 1513, FOLD: 3, EPOCH: 23, train_loss: 0.01617601386986781\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 23, valid_loss: 0.016257001832127572\n",
      "SEED: 1513, FOLD: 3, EPOCH: 24, train_loss: 0.015957641597513273\n",
      "SEED: 1513 ,FOLD: 3, EPOCH: 24, valid_loss: 0.016257886375699724\n",
      "SEED: 1513, FOLD: 4, EPOCH: 0, train_loss: 0.4710116867866853\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 0, valid_loss: 0.02402935613478933\n",
      "SEED: 1513, FOLD: 4, EPOCH: 1, train_loss: 0.023730167727647484\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 1, valid_loss: 0.020032302662730216\n",
      "SEED: 1513, FOLD: 4, EPOCH: 2, train_loss: 0.0245835448520771\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 2, valid_loss: 0.018439497160060065\n",
      "SEED: 1513, FOLD: 4, EPOCH: 3, train_loss: 0.021529114751172237\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 3, valid_loss: 0.018260293028184344\n",
      "SEED: 1513, FOLD: 4, EPOCH: 4, train_loss: 0.02072424380837575\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 4, valid_loss: 0.017697812536997454\n",
      "SEED: 1513, FOLD: 4, EPOCH: 5, train_loss: 0.02047488870828048\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 5, valid_loss: 0.01771022882312536\n",
      "SEED: 1513, FOLD: 4, EPOCH: 6, train_loss: 0.020367061381862648\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 6, valid_loss: 0.01759661625006369\n",
      "SEED: 1513, FOLD: 4, EPOCH: 7, train_loss: 0.020332528035277905\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 7, valid_loss: 0.017576816252299718\n",
      "SEED: 1513, FOLD: 4, EPOCH: 8, train_loss: 0.02029513261294451\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 8, valid_loss: 0.01706155642334904\n",
      "SEED: 1513, FOLD: 4, EPOCH: 9, train_loss: 0.020316670243830784\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 9, valid_loss: 0.017180954877819333\n",
      "SEED: 1513, FOLD: 4, EPOCH: 10, train_loss: 0.020207878242692223\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 10, valid_loss: 0.017130478764218943\n",
      "SEED: 1513, FOLD: 4, EPOCH: 11, train_loss: 0.020088756065545738\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 11, valid_loss: 0.017063875789088863\n",
      "SEED: 1513, FOLD: 4, EPOCH: 12, train_loss: 0.020107130331081757\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 12, valid_loss: 0.017017669656447004\n",
      "SEED: 1513, FOLD: 4, EPOCH: 13, train_loss: 0.019963780566510082\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 13, valid_loss: 0.016894959099590778\n",
      "SEED: 1513, FOLD: 4, EPOCH: 14, train_loss: 0.01976140376612328\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 14, valid_loss: 0.016767828672059944\n",
      "SEED: 1513, FOLD: 4, EPOCH: 15, train_loss: 0.01965697113748478\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 15, valid_loss: 0.016748090593942572\n",
      "SEED: 1513, FOLD: 4, EPOCH: 16, train_loss: 0.019444103919617508\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 16, valid_loss: 0.016693296922104698\n",
      "SEED: 1513, FOLD: 4, EPOCH: 17, train_loss: 0.019162666997399883\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 17, valid_loss: 0.016409442307693617\n",
      "SEED: 1513, FOLD: 4, EPOCH: 18, train_loss: 0.018777618778572567\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 18, valid_loss: 0.016328161795224463\n",
      "SEED: 1513, FOLD: 4, EPOCH: 19, train_loss: 0.018413838458018028\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 19, valid_loss: 0.016238148829766683\n",
      "SEED: 1513, FOLD: 4, EPOCH: 20, train_loss: 0.017924359797135643\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 20, valid_loss: 0.016133791128439563\n",
      "SEED: 1513, FOLD: 4, EPOCH: 21, train_loss: 0.01734365593722981\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 21, valid_loss: 0.016093743246580874\n",
      "SEED: 1513, FOLD: 4, EPOCH: 22, train_loss: 0.016805552445568035\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 22, valid_loss: 0.016020325730953897\n",
      "SEED: 1513, FOLD: 4, EPOCH: 23, train_loss: 0.016281486799319584\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 23, valid_loss: 0.016021542996168137\n",
      "SEED: 1513, FOLD: 4, EPOCH: 24, train_loss: 0.016042049920213394\n",
      "SEED: 1513 ,FOLD: 4, EPOCH: 24, valid_loss: 0.01607345457055739\n",
      "SEED: 1269, FOLD: 0, EPOCH: 0, train_loss: 0.4728840248038371\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 0, valid_loss: 0.023521910927125387\n",
      "SEED: 1269, FOLD: 0, EPOCH: 1, train_loss: 0.023884289047640304\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 1, valid_loss: 0.020065215123551233\n",
      "SEED: 1269, FOLD: 0, EPOCH: 2, train_loss: 0.02189656383479419\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 2, valid_loss: 0.017655061717544284\n",
      "SEED: 1269, FOLD: 0, EPOCH: 3, train_loss: 0.020818431956180626\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 3, valid_loss: 0.017467023299208708\n",
      "SEED: 1269, FOLD: 0, EPOCH: 4, train_loss: 0.020218027971576954\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 4, valid_loss: 0.01736848277172872\n",
      "SEED: 1269, FOLD: 0, EPOCH: 5, train_loss: 0.020166700719383316\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 5, valid_loss: 0.017404947695987567\n",
      "SEED: 1269, FOLD: 0, EPOCH: 6, train_loss: 0.020207877540825935\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 6, valid_loss: 0.017242833838931153\n",
      "SEED: 1269, FOLD: 0, EPOCH: 7, train_loss: 0.02015572160050489\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 7, valid_loss: 0.017443588748574258\n",
      "SEED: 1269, FOLD: 0, EPOCH: 8, train_loss: 0.020108377034573452\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 8, valid_loss: 0.017150634633643285\n",
      "SEED: 1269, FOLD: 0, EPOCH: 9, train_loss: 0.02013473531258279\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 9, valid_loss: 0.017256459008370126\n",
      "SEED: 1269, FOLD: 0, EPOCH: 10, train_loss: 0.020094003298900265\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 10, valid_loss: 0.017262434081307478\n",
      "SEED: 1269, FOLD: 0, EPOCH: 11, train_loss: 0.020026972875053037\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 11, valid_loss: 0.017095396029097692\n",
      "SEED: 1269, FOLD: 0, EPOCH: 12, train_loss: 0.01997453649190889\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 12, valid_loss: 0.016932915124510017\n",
      "SEED: 1269, FOLD: 0, EPOCH: 13, train_loss: 0.019740894018415955\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 13, valid_loss: 0.01705012856317418\n",
      "SEED: 1269, FOLD: 0, EPOCH: 14, train_loss: 0.019695557491934818\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 14, valid_loss: 0.016967637916760786\n",
      "SEED: 1269, FOLD: 0, EPOCH: 15, train_loss: 0.019532223596521046\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 15, valid_loss: 0.01675854085811547\n",
      "SEED: 1269, FOLD: 0, EPOCH: 16, train_loss: 0.01935939972653337\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 16, valid_loss: 0.01660781292510884\n",
      "SEED: 1269, FOLD: 0, EPOCH: 17, train_loss: 0.019067192004750603\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 17, valid_loss: 0.016447657107242514\n",
      "SEED: 1269, FOLD: 0, EPOCH: 18, train_loss: 0.018628639877652346\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 18, valid_loss: 0.016288014075585774\n",
      "SEED: 1269, FOLD: 0, EPOCH: 19, train_loss: 0.018277068008277296\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 19, valid_loss: 0.016207998512046677\n",
      "SEED: 1269, FOLD: 0, EPOCH: 20, train_loss: 0.0177340232902139\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 20, valid_loss: 0.01612547431141138\n",
      "SEED: 1269, FOLD: 0, EPOCH: 21, train_loss: 0.017214974368234045\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 21, valid_loss: 0.016035373163010392\n",
      "SEED: 1269, FOLD: 0, EPOCH: 22, train_loss: 0.016582131999936224\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 22, valid_loss: 0.016049506195953914\n",
      "SEED: 1269, FOLD: 0, EPOCH: 23, train_loss: 0.016104152372133904\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 23, valid_loss: 0.01605696225804942\n",
      "SEED: 1269, FOLD: 0, EPOCH: 24, train_loss: 0.015912793637455805\n",
      "SEED: 1269 ,FOLD: 0, EPOCH: 24, valid_loss: 0.016075974250478405\n",
      "SEED: 1269, FOLD: 1, EPOCH: 0, train_loss: 0.4716413123263181\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 0, valid_loss: 0.023843867384961675\n",
      "SEED: 1269, FOLD: 1, EPOCH: 1, train_loss: 0.023817737789257713\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 1, valid_loss: 0.018836540249841553\n",
      "SEED: 1269, FOLD: 1, EPOCH: 2, train_loss: 0.022216772147710773\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 2, valid_loss: 0.01779144150870187\n",
      "SEED: 1269, FOLD: 1, EPOCH: 3, train_loss: 0.020771147251345108\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 3, valid_loss: 0.017659240295844418\n",
      "SEED: 1269, FOLD: 1, EPOCH: 4, train_loss: 0.020317746744747612\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 4, valid_loss: 0.01715271111045565\n",
      "SEED: 1269, FOLD: 1, EPOCH: 5, train_loss: 0.02019084821306709\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 5, valid_loss: 0.01784324257501534\n",
      "SEED: 1269, FOLD: 1, EPOCH: 6, train_loss: 0.020186429694834827\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 6, valid_loss: 0.01719182346548353\n",
      "SEED: 1269, FOLD: 1, EPOCH: 7, train_loss: 0.020173227171535076\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 7, valid_loss: 0.017192744011325495\n",
      "SEED: 1269, FOLD: 1, EPOCH: 8, train_loss: 0.02017582942178284\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 8, valid_loss: 0.017291473224759102\n",
      "SEED: 1269, FOLD: 1, EPOCH: 9, train_loss: 0.020174593934654327\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 9, valid_loss: 0.017366680077144078\n",
      "SEED: 1269, FOLD: 1, EPOCH: 10, train_loss: 0.020198165897981846\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 10, valid_loss: 0.017182850944144384\n",
      "SEED: 1269, FOLD: 1, EPOCH: 11, train_loss: 0.020087282821212128\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 11, valid_loss: 0.017011825421026774\n",
      "SEED: 1269, FOLD: 1, EPOCH: 12, train_loss: 0.02001688513310923\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 12, valid_loss: 0.017062103535447803\n",
      "SEED: 1269, FOLD: 1, EPOCH: 13, train_loss: 0.019935586894659893\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 13, valid_loss: 0.017035670977618014\n",
      "SEED: 1269, FOLD: 1, EPOCH: 14, train_loss: 0.01976946140706971\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 14, valid_loss: 0.016785836219787596\n",
      "SEED: 1269, FOLD: 1, EPOCH: 15, train_loss: 0.01954189818892358\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 15, valid_loss: 0.016738831757434777\n",
      "SEED: 1269, FOLD: 1, EPOCH: 16, train_loss: 0.019349581251541775\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 16, valid_loss: 0.0165596837710057\n",
      "SEED: 1269, FOLD: 1, EPOCH: 17, train_loss: 0.019066736331560474\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 17, valid_loss: 0.016452825335519655\n",
      "SEED: 1269, FOLD: 1, EPOCH: 18, train_loss: 0.01874622231538313\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 18, valid_loss: 0.016410169138440064\n",
      "SEED: 1269, FOLD: 1, EPOCH: 19, train_loss: 0.01836261830792047\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 19, valid_loss: 0.016307150200009345\n",
      "SEED: 1269, FOLD: 1, EPOCH: 20, train_loss: 0.01783301624590936\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 20, valid_loss: 0.0161545779556036\n",
      "SEED: 1269, FOLD: 1, EPOCH: 21, train_loss: 0.017306137485834566\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 21, valid_loss: 0.0161078187503985\n",
      "SEED: 1269, FOLD: 1, EPOCH: 22, train_loss: 0.01670414369985245\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 22, valid_loss: 0.016098234323518618\n",
      "SEED: 1269, FOLD: 1, EPOCH: 23, train_loss: 0.016241783168220867\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 23, valid_loss: 0.016097764245101384\n",
      "SEED: 1269, FOLD: 1, EPOCH: 24, train_loss: 0.016019181503603857\n",
      "SEED: 1269 ,FOLD: 1, EPOCH: 24, valid_loss: 0.016126015303390365\n",
      "SEED: 1269, FOLD: 2, EPOCH: 0, train_loss: 0.47104801215987274\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 0, valid_loss: 0.024668026183332716\n",
      "SEED: 1269, FOLD: 2, EPOCH: 1, train_loss: 0.023690537927483303\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 1, valid_loss: 0.019211394020489284\n",
      "SEED: 1269, FOLD: 2, EPOCH: 2, train_loss: 0.021706754166254963\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 2, valid_loss: 0.01956929128084864\n",
      "SEED: 1269, FOLD: 2, EPOCH: 3, train_loss: 0.02069181118808363\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 3, valid_loss: 0.019585675267236573\n",
      "SEED: 1269, FOLD: 2, EPOCH: 4, train_loss: 0.020385451587861862\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 4, valid_loss: 0.01773756758442947\n",
      "SEED: 1269, FOLD: 2, EPOCH: 5, train_loss: 0.020162761562328407\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 5, valid_loss: 0.017344017992062227\n",
      "SEED: 1269, FOLD: 2, EPOCH: 6, train_loss: 0.020115830274163814\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 6, valid_loss: 0.01728732186768736\n",
      "SEED: 1269, FOLD: 2, EPOCH: 7, train_loss: 0.020226958464237228\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 7, valid_loss: 0.017572990538818497\n",
      "SEED: 1269, FOLD: 2, EPOCH: 8, train_loss: 0.02023144023141999\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 8, valid_loss: 0.01722025118236031\n",
      "SEED: 1269, FOLD: 2, EPOCH: 9, train_loss: 0.020157649816162346\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 9, valid_loss: 0.017177108409149305\n",
      "SEED: 1269, FOLD: 2, EPOCH: 10, train_loss: 0.020118602065612442\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 10, valid_loss: 0.01710608388696398\n",
      "SEED: 1269, FOLD: 2, EPOCH: 11, train_loss: 0.020088192628885525\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 11, valid_loss: 0.01744587349572352\n",
      "SEED: 1269, FOLD: 2, EPOCH: 12, train_loss: 0.02003541775047779\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 12, valid_loss: 0.01687227516834225\n",
      "SEED: 1269, FOLD: 2, EPOCH: 13, train_loss: 0.019929652250763298\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 13, valid_loss: 0.016951010642307144\n",
      "SEED: 1269, FOLD: 2, EPOCH: 14, train_loss: 0.019747885807916737\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 14, valid_loss: 0.016803644837013312\n",
      "SEED: 1269, FOLD: 2, EPOCH: 15, train_loss: 0.01958633662349936\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 15, valid_loss: 0.016773409343191555\n",
      "SEED: 1269, FOLD: 2, EPOCH: 16, train_loss: 0.019352657760938873\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 16, valid_loss: 0.016579171349959714\n",
      "SEED: 1269, FOLD: 2, EPOCH: 17, train_loss: 0.019081361807774807\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 17, valid_loss: 0.01639220379292965\n",
      "SEED: 1269, FOLD: 2, EPOCH: 18, train_loss: 0.01875546335688104\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 18, valid_loss: 0.016274014008896692\n",
      "SEED: 1269, FOLD: 2, EPOCH: 19, train_loss: 0.018338875392910795\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 19, valid_loss: 0.016273620591631958\n",
      "SEED: 1269, FOLD: 2, EPOCH: 20, train_loss: 0.017943779303543808\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 20, valid_loss: 0.016135654784739016\n",
      "SEED: 1269, FOLD: 2, EPOCH: 21, train_loss: 0.01738152792002412\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 21, valid_loss: 0.01603361354874713\n",
      "SEED: 1269, FOLD: 2, EPOCH: 22, train_loss: 0.016901000855031652\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 22, valid_loss: 0.016031866334378718\n",
      "SEED: 1269, FOLD: 2, EPOCH: 23, train_loss: 0.016480675486820764\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 23, valid_loss: 0.016021044259624823\n",
      "SEED: 1269, FOLD: 2, EPOCH: 24, train_loss: 0.016287168926572886\n",
      "SEED: 1269 ,FOLD: 2, EPOCH: 24, valid_loss: 0.016018394513853958\n",
      "SEED: 1269, FOLD: 3, EPOCH: 0, train_loss: 0.4716867306202218\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 0, valid_loss: 0.02380271852016449\n",
      "SEED: 1269, FOLD: 3, EPOCH: 1, train_loss: 0.02384367830835391\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 1, valid_loss: 0.019013347955686707\n",
      "SEED: 1269, FOLD: 3, EPOCH: 2, train_loss: 0.021721142600627914\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 2, valid_loss: 0.017750820969896657\n",
      "SEED: 1269, FOLD: 3, EPOCH: 3, train_loss: 0.020925430365014767\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 3, valid_loss: 0.017761293851903507\n",
      "SEED: 1269, FOLD: 3, EPOCH: 4, train_loss: 0.02024529624622369\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 4, valid_loss: 0.017530633535768305\n",
      "SEED: 1269, FOLD: 3, EPOCH: 5, train_loss: 0.02030110100041265\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 5, valid_loss: 0.017579219782991068\n",
      "SEED: 1269, FOLD: 3, EPOCH: 6, train_loss: 0.020133637890651607\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 6, valid_loss: 0.017577916091041906\n",
      "SEED: 1269, FOLD: 3, EPOCH: 7, train_loss: 0.020147287408294884\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 7, valid_loss: 0.01746312627302749\n",
      "SEED: 1269, FOLD: 3, EPOCH: 8, train_loss: 0.02013533189892769\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 8, valid_loss: 0.017531808944685118\n",
      "SEED: 1269, FOLD: 3, EPOCH: 9, train_loss: 0.020045336869045877\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 9, valid_loss: 0.01724783866001027\n",
      "SEED: 1269, FOLD: 3, EPOCH: 10, train_loss: 0.02009749658189822\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 10, valid_loss: 0.017252372125429766\n",
      "SEED: 1269, FOLD: 3, EPOCH: 11, train_loss: 0.020127954730845016\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 11, valid_loss: 0.01795677292559828\n",
      "SEED: 1269, FOLD: 3, EPOCH: 12, train_loss: 0.019939685685803062\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 12, valid_loss: 0.017170091745044504\n",
      "SEED: 1269, FOLD: 3, EPOCH: 13, train_loss: 0.01979962632437979\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 13, valid_loss: 0.017167134689433233\n",
      "SEED: 1269, FOLD: 3, EPOCH: 14, train_loss: 0.01973153355166964\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 14, valid_loss: 0.01702439399170024\n",
      "SEED: 1269, FOLD: 3, EPOCH: 15, train_loss: 0.019523179522998955\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 15, valid_loss: 0.016831884746040617\n",
      "SEED: 1269, FOLD: 3, EPOCH: 16, train_loss: 0.01928628218508717\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 16, valid_loss: 0.016758376306721143\n",
      "SEED: 1269, FOLD: 3, EPOCH: 17, train_loss: 0.019025550642307255\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 17, valid_loss: 0.016759358106979302\n",
      "SEED: 1269, FOLD: 3, EPOCH: 18, train_loss: 0.018772688363611265\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 18, valid_loss: 0.01667521795524018\n",
      "SEED: 1269, FOLD: 3, EPOCH: 19, train_loss: 0.01831807246080775\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 19, valid_loss: 0.016516749374568464\n",
      "SEED: 1269, FOLD: 3, EPOCH: 20, train_loss: 0.017832957869530586\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 20, valid_loss: 0.016339800959186895\n",
      "SEED: 1269, FOLD: 3, EPOCH: 21, train_loss: 0.017314539668892605\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 21, valid_loss: 0.01631846744567156\n",
      "SEED: 1269, FOLD: 3, EPOCH: 22, train_loss: 0.01671822066081391\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 22, valid_loss: 0.016253979903246674\n",
      "SEED: 1269, FOLD: 3, EPOCH: 23, train_loss: 0.01631410713510021\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 23, valid_loss: 0.016251732621874126\n",
      "SEED: 1269, FOLD: 3, EPOCH: 24, train_loss: 0.016091802798589502\n",
      "SEED: 1269 ,FOLD: 3, EPOCH: 24, valid_loss: 0.016266218439808914\n",
      "SEED: 1269, FOLD: 4, EPOCH: 0, train_loss: 0.47170168222130643\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 0, valid_loss: 0.023695143525089536\n",
      "SEED: 1269, FOLD: 4, EPOCH: 1, train_loss: 0.024108877675472828\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 1, valid_loss: 0.019152964811239924\n",
      "SEED: 1269, FOLD: 4, EPOCH: 2, train_loss: 0.021942086558303108\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 2, valid_loss: 0.45441014681543623\n",
      "SEED: 1269, FOLD: 4, EPOCH: 3, train_loss: 0.022631523779768875\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 3, valid_loss: 0.017788649456841606\n",
      "SEED: 1269, FOLD: 4, EPOCH: 4, train_loss: 0.020800893694378327\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 4, valid_loss: 0.017517787058438573\n",
      "SEED: 1269, FOLD: 4, EPOCH: 5, train_loss: 0.020469753679050053\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 5, valid_loss: 0.017330084208931243\n",
      "SEED: 1269, FOLD: 4, EPOCH: 6, train_loss: 0.020447833153108757\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 6, valid_loss: 0.0172305806939091\n",
      "SEED: 1269, FOLD: 4, EPOCH: 7, train_loss: 0.020307186491571476\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 7, valid_loss: 0.017545370997062753\n",
      "SEED: 1269, FOLD: 4, EPOCH: 8, train_loss: 0.020398292353079803\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 8, valid_loss: 0.017298776283860208\n",
      "SEED: 1269, FOLD: 4, EPOCH: 9, train_loss: 0.020321519006097664\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 9, valid_loss: 0.017322232840316636\n",
      "SEED: 1269, FOLD: 4, EPOCH: 10, train_loss: 0.020173573245604832\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 10, valid_loss: 0.01749690281493323\n",
      "SEED: 1269, FOLD: 4, EPOCH: 11, train_loss: 0.020135017611302326\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 11, valid_loss: 0.017180857621133326\n",
      "SEED: 1269, FOLD: 4, EPOCH: 12, train_loss: 0.020101603417508843\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 12, valid_loss: 0.016982935847980635\n",
      "SEED: 1269, FOLD: 4, EPOCH: 13, train_loss: 0.019963118895564392\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 13, valid_loss: 0.0169156959812556\n",
      "SEED: 1269, FOLD: 4, EPOCH: 14, train_loss: 0.019791072909382805\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 14, valid_loss: 0.016909439861774445\n",
      "SEED: 1269, FOLD: 4, EPOCH: 15, train_loss: 0.019669042319815228\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 15, valid_loss: 0.01661600898951292\n",
      "SEED: 1269, FOLD: 4, EPOCH: 16, train_loss: 0.01938749315298122\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 16, valid_loss: 0.016512787714600564\n",
      "SEED: 1269, FOLD: 4, EPOCH: 17, train_loss: 0.019103374764107277\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 17, valid_loss: 0.0163452966936997\n",
      "SEED: 1269, FOLD: 4, EPOCH: 18, train_loss: 0.018795146289672972\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 18, valid_loss: 0.01651532540896109\n",
      "SEED: 1269, FOLD: 4, EPOCH: 19, train_loss: 0.01847380699346895\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 19, valid_loss: 0.016203817379261767\n",
      "SEED: 1269, FOLD: 4, EPOCH: 20, train_loss: 0.017949591208573267\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 20, valid_loss: 0.016046405636838505\n",
      "SEED: 1269, FOLD: 4, EPOCH: 21, train_loss: 0.01740204054268374\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 21, valid_loss: 0.016010785661637782\n",
      "SEED: 1269, FOLD: 4, EPOCH: 22, train_loss: 0.016823516429766365\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 22, valid_loss: 0.015971953102520534\n",
      "SEED: 1269, FOLD: 4, EPOCH: 23, train_loss: 0.01637365617721841\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 23, valid_loss: 0.01601027436554432\n",
      "SEED: 1269, FOLD: 4, EPOCH: 24, train_loss: 0.016182844904993755\n",
      "SEED: 1269 ,FOLD: 4, EPOCH: 24, valid_loss: 0.016008850453155383\n",
      "SEED: 1392, FOLD: 0, EPOCH: 0, train_loss: 0.47138528975293686\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 0, valid_loss: 0.023612693643995694\n",
      "SEED: 1392, FOLD: 0, EPOCH: 1, train_loss: 0.023867718011572742\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 1, valid_loss: 0.018984381641660417\n",
      "SEED: 1392, FOLD: 0, EPOCH: 2, train_loss: 0.02173021816365097\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 2, valid_loss: 0.017608159594237803\n",
      "SEED: 1392, FOLD: 0, EPOCH: 3, train_loss: 0.020526297134009823\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 3, valid_loss: 0.018882407301238604\n",
      "SEED: 1392, FOLD: 0, EPOCH: 4, train_loss: 0.0204540043529393\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 4, valid_loss: 0.01723299108977829\n",
      "SEED: 1392, FOLD: 0, EPOCH: 5, train_loss: 0.020179588172206844\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 5, valid_loss: 0.01722270069377763\n",
      "SEED: 1392, FOLD: 0, EPOCH: 6, train_loss: 0.020204896741695164\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 6, valid_loss: 0.017518024332821368\n",
      "SEED: 1392, FOLD: 0, EPOCH: 7, train_loss: 0.020137756118091984\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 7, valid_loss: 0.017259483757827963\n",
      "SEED: 1392, FOLD: 0, EPOCH: 8, train_loss: 0.020183325610191063\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 8, valid_loss: 0.017220642524106163\n",
      "SEED: 1392, FOLD: 0, EPOCH: 9, train_loss: 0.02013591984691827\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 9, valid_loss: 0.01704398404274668\n",
      "SEED: 1392, FOLD: 0, EPOCH: 10, train_loss: 0.020057573658076748\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 10, valid_loss: 0.017091604854379383\n",
      "SEED: 1392, FOLD: 0, EPOCH: 11, train_loss: 0.02019059463687565\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 11, valid_loss: 0.017231300047465734\n",
      "SEED: 1392, FOLD: 0, EPOCH: 12, train_loss: 0.020028314836647198\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 12, valid_loss: 0.017012988988842282\n",
      "SEED: 1392, FOLD: 0, EPOCH: 13, train_loss: 0.019955986111924267\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 13, valid_loss: 0.01680946783827884\n",
      "SEED: 1392, FOLD: 0, EPOCH: 14, train_loss: 0.019743812283959942\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 14, valid_loss: 0.01675895383315427\n",
      "SEED: 1392, FOLD: 0, EPOCH: 15, train_loss: 0.01953656546285619\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 15, valid_loss: 0.017095374927989074\n",
      "SEED: 1392, FOLD: 0, EPOCH: 16, train_loss: 0.019412601501613422\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 16, valid_loss: 0.01664724288774388\n",
      "SEED: 1392, FOLD: 0, EPOCH: 17, train_loss: 0.01906250010959912\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 17, valid_loss: 0.016487673191087588\n",
      "SEED: 1392, FOLD: 0, EPOCH: 18, train_loss: 0.01879108899637409\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 18, valid_loss: 0.016341403259762695\n",
      "SEED: 1392, FOLD: 0, EPOCH: 19, train_loss: 0.018361732593157154\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 19, valid_loss: 0.016289007450853075\n",
      "SEED: 1392, FOLD: 0, EPOCH: 20, train_loss: 0.017884772135943607\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 20, valid_loss: 0.016162975558212826\n",
      "SEED: 1392, FOLD: 0, EPOCH: 21, train_loss: 0.017335065384057984\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 21, valid_loss: 0.016037082246371678\n",
      "SEED: 1392, FOLD: 0, EPOCH: 22, train_loss: 0.016796992389836174\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 22, valid_loss: 0.01602952405810356\n",
      "SEED: 1392, FOLD: 0, EPOCH: 23, train_loss: 0.016346896161743694\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 23, valid_loss: 0.01604801405753408\n",
      "SEED: 1392, FOLD: 0, EPOCH: 24, train_loss: 0.016147742992725925\n",
      "SEED: 1392 ,FOLD: 0, EPOCH: 24, valid_loss: 0.01600843175713505\n",
      "SEED: 1392, FOLD: 1, EPOCH: 0, train_loss: 0.47229476302754186\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 0, valid_loss: 0.0246561662959201\n",
      "SEED: 1392, FOLD: 1, EPOCH: 1, train_loss: 0.02382402176010436\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 1, valid_loss: 0.018908798109207835\n",
      "SEED: 1392, FOLD: 1, EPOCH: 2, train_loss: 0.022095253947528377\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 2, valid_loss: 0.03356066191835063\n",
      "SEED: 1392, FOLD: 1, EPOCH: 3, train_loss: 0.022124923846643905\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 3, valid_loss: 0.01767585195068802\n",
      "SEED: 1392, FOLD: 1, EPOCH: 4, train_loss: 0.020618844372422798\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 4, valid_loss: 0.017459546242441448\n",
      "SEED: 1392, FOLD: 1, EPOCH: 5, train_loss: 0.020299056207464226\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 5, valid_loss: 0.017322592304221222\n",
      "SEED: 1392, FOLD: 1, EPOCH: 6, train_loss: 0.02022194627510465\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 6, valid_loss: 0.0171794306486845\n",
      "SEED: 1392, FOLD: 1, EPOCH: 7, train_loss: 0.020235421297990757\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 7, valid_loss: 0.017721866576799323\n",
      "SEED: 1392, FOLD: 1, EPOCH: 8, train_loss: 0.020168481664597126\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 8, valid_loss: 0.017316615980650697\n",
      "SEED: 1392, FOLD: 1, EPOCH: 9, train_loss: 0.020154846508217895\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 9, valid_loss: 0.01727448808295386\n",
      "SEED: 1392, FOLD: 1, EPOCH: 10, train_loss: 0.02015176154943048\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 10, valid_loss: 0.0170123731185283\n",
      "SEED: 1392, FOLD: 1, EPOCH: 11, train_loss: 0.0200890615798425\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 11, valid_loss: 0.017182462476193906\n",
      "SEED: 1392, FOLD: 1, EPOCH: 12, train_loss: 0.019960896476455357\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 12, valid_loss: 0.01744033976324967\n",
      "SEED: 1392, FOLD: 1, EPOCH: 13, train_loss: 0.019940642626497192\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 13, valid_loss: 0.01700510425227029\n",
      "SEED: 1392, FOLD: 1, EPOCH: 14, train_loss: 0.019786664311760578\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 14, valid_loss: 0.01703250240534544\n",
      "SEED: 1392, FOLD: 1, EPOCH: 15, train_loss: 0.019600810631569744\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 15, valid_loss: 0.016668973038239138\n",
      "SEED: 1392, FOLD: 1, EPOCH: 16, train_loss: 0.01928237594826066\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 16, valid_loss: 0.016606059643839086\n",
      "SEED: 1392, FOLD: 1, EPOCH: 17, train_loss: 0.019066055325548285\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 17, valid_loss: 0.016484087359692368\n",
      "SEED: 1392, FOLD: 1, EPOCH: 18, train_loss: 0.018750898378050846\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 18, valid_loss: 0.01633698966886316\n",
      "SEED: 1392, FOLD: 1, EPOCH: 19, train_loss: 0.018337801800689835\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 19, valid_loss: 0.01622920533908265\n",
      "SEED: 1392, FOLD: 1, EPOCH: 20, train_loss: 0.017771150451153517\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 20, valid_loss: 0.016170414084834713\n",
      "SEED: 1392, FOLD: 1, EPOCH: 21, train_loss: 0.017142808226787525\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 21, valid_loss: 0.016138765029609203\n",
      "SEED: 1392, FOLD: 1, EPOCH: 22, train_loss: 0.016544938195442806\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 22, valid_loss: 0.016113469057849477\n",
      "SEED: 1392, FOLD: 1, EPOCH: 23, train_loss: 0.01604197999003573\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 23, valid_loss: 0.016094482663486686\n",
      "SEED: 1392, FOLD: 1, EPOCH: 24, train_loss: 0.015826251121588808\n",
      "SEED: 1392 ,FOLD: 1, EPOCH: 24, valid_loss: 0.01613239917371954\n",
      "SEED: 1392, FOLD: 2, EPOCH: 0, train_loss: 0.47132263998028595\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 0, valid_loss: 0.024454229910458838\n",
      "SEED: 1392, FOLD: 2, EPOCH: 1, train_loss: 0.023716484277468662\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 1, valid_loss: 0.018963144905865193\n",
      "SEED: 1392, FOLD: 2, EPOCH: 2, train_loss: 0.021992168996645058\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 2, valid_loss: 0.017937214645956243\n",
      "SEED: 1392, FOLD: 2, EPOCH: 3, train_loss: 0.020651154139119644\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 3, valid_loss: 0.01738593437309776\n",
      "SEED: 1392, FOLD: 2, EPOCH: 4, train_loss: 0.020746449400009453\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 4, valid_loss: 0.017328245166156973\n",
      "SEED: 1392, FOLD: 2, EPOCH: 5, train_loss: 0.020211652366687424\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 5, valid_loss: 0.017370522474603994\n",
      "SEED: 1392, FOLD: 2, EPOCH: 6, train_loss: 0.02027768996692654\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 6, valid_loss: 0.017381458410194943\n",
      "SEED: 1392, FOLD: 2, EPOCH: 7, train_loss: 0.02020209971005502\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 7, valid_loss: 0.017299256926136358\n",
      "SEED: 1392, FOLD: 2, EPOCH: 8, train_loss: 0.02025980440278848\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 8, valid_loss: 0.017177500336297922\n",
      "SEED: 1392, FOLD: 2, EPOCH: 9, train_loss: 0.020136054834701878\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 9, valid_loss: 0.017324957278157985\n",
      "SEED: 1392, FOLD: 2, EPOCH: 10, train_loss: 0.02012016727467594\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 10, valid_loss: 0.017243189125188758\n",
      "SEED: 1392, FOLD: 2, EPOCH: 11, train_loss: 0.020135982704443344\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 11, valid_loss: 0.017014824279717036\n",
      "SEED: 1392, FOLD: 2, EPOCH: 12, train_loss: 0.020028193238312782\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 12, valid_loss: 0.017044111554111754\n",
      "SEED: 1392, FOLD: 2, EPOCH: 13, train_loss: 0.019897705753860268\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 13, valid_loss: 0.016854417882859705\n",
      "SEED: 1392, FOLD: 2, EPOCH: 14, train_loss: 0.01979362632593383\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 14, valid_loss: 0.01681072562932968\n",
      "SEED: 1392, FOLD: 2, EPOCH: 15, train_loss: 0.01965265112348657\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 15, valid_loss: 0.016841492482594082\n",
      "SEED: 1392, FOLD: 2, EPOCH: 16, train_loss: 0.01945016238892424\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 16, valid_loss: 0.01645636803337506\n",
      "SEED: 1392, FOLD: 2, EPOCH: 17, train_loss: 0.019156658401091892\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 17, valid_loss: 0.016400090152663842\n",
      "SEED: 1392, FOLD: 2, EPOCH: 18, train_loss: 0.018801846608951473\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 18, valid_loss: 0.01633773677583252\n",
      "SEED: 1392, FOLD: 2, EPOCH: 19, train_loss: 0.01842195065557093\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 19, valid_loss: 0.016281495536012307\n",
      "SEED: 1392, FOLD: 2, EPOCH: 20, train_loss: 0.017931302813678118\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 20, valid_loss: 0.016079752519726754\n",
      "SEED: 1392, FOLD: 2, EPOCH: 21, train_loss: 0.017419459824652775\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 21, valid_loss: 0.01608265515949045\n",
      "SEED: 1392, FOLD: 2, EPOCH: 22, train_loss: 0.01683374082642621\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 22, valid_loss: 0.01604270072919982\n",
      "SEED: 1392, FOLD: 2, EPOCH: 23, train_loss: 0.016437378813229177\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 23, valid_loss: 0.016041429793196064\n",
      "SEED: 1392, FOLD: 2, EPOCH: 24, train_loss: 0.01621457618539748\n",
      "SEED: 1392 ,FOLD: 2, EPOCH: 24, valid_loss: 0.016016277032239096\n",
      "SEED: 1392, FOLD: 3, EPOCH: 0, train_loss: 0.47163981764806784\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 0, valid_loss: 0.024300992382424218\n",
      "SEED: 1392, FOLD: 3, EPOCH: 1, train_loss: 0.023794829278536465\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 1, valid_loss: 0.019327179555382046\n",
      "SEED: 1392, FOLD: 3, EPOCH: 2, train_loss: 0.02222011200543763\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 2, valid_loss: 0.019152097989405904\n",
      "SEED: 1392, FOLD: 3, EPOCH: 3, train_loss: 0.020686007253285767\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 3, valid_loss: 0.017755603870110853\n",
      "SEED: 1392, FOLD: 3, EPOCH: 4, train_loss: 0.020196668169312718\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 4, valid_loss: 0.017548129414873465\n",
      "SEED: 1392, FOLD: 3, EPOCH: 5, train_loss: 0.02018168725181317\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 5, valid_loss: 0.017468431858079773\n",
      "SEED: 1392, FOLD: 3, EPOCH: 6, train_loss: 0.020129167218355164\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 6, valid_loss: 0.018103746244949953\n",
      "SEED: 1392, FOLD: 3, EPOCH: 7, train_loss: 0.020195570490930393\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 7, valid_loss: 0.017334537234689508\n",
      "SEED: 1392, FOLD: 3, EPOCH: 8, train_loss: 0.020056834795336792\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 8, valid_loss: 0.017296376265585424\n",
      "SEED: 1392, FOLD: 3, EPOCH: 9, train_loss: 0.020130900868579098\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 9, valid_loss: 0.017449406082076686\n",
      "SEED: 1392, FOLD: 3, EPOCH: 10, train_loss: 0.020156697316122227\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 10, valid_loss: 0.017617634391146046\n",
      "SEED: 1392, FOLD: 3, EPOCH: 11, train_loss: 0.02005245921242496\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 11, valid_loss: 0.01725079651389803\n",
      "SEED: 1392, FOLD: 3, EPOCH: 12, train_loss: 0.01999133883341067\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 12, valid_loss: 0.01717237518834216\n",
      "SEED: 1392, FOLD: 3, EPOCH: 13, train_loss: 0.019834032160756382\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 13, valid_loss: 0.017236721329391004\n",
      "SEED: 1392, FOLD: 3, EPOCH: 14, train_loss: 0.019733023559809593\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 14, valid_loss: 0.016934669416929993\n",
      "SEED: 1392, FOLD: 3, EPOCH: 15, train_loss: 0.019493754818171696\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 15, valid_loss: 0.01689286937138864\n",
      "SEED: 1392, FOLD: 3, EPOCH: 16, train_loss: 0.019318073812494244\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 16, valid_loss: 0.01688899751752615\n",
      "SEED: 1392, FOLD: 3, EPOCH: 17, train_loss: 0.01911834068596363\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 17, valid_loss: 0.01657298609082188\n",
      "SEED: 1392, FOLD: 3, EPOCH: 18, train_loss: 0.018731378572250622\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 18, valid_loss: 0.01647619323006698\n",
      "SEED: 1392, FOLD: 3, EPOCH: 19, train_loss: 0.018417726007654615\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 19, valid_loss: 0.01645088688071285\n",
      "SEED: 1392, FOLD: 3, EPOCH: 20, train_loss: 0.017925261216157156\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 20, valid_loss: 0.016287130702819144\n",
      "SEED: 1392, FOLD: 3, EPOCH: 21, train_loss: 0.017375079881183912\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 21, valid_loss: 0.016245388452495846\n",
      "SEED: 1392, FOLD: 3, EPOCH: 22, train_loss: 0.0168502199989946\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 22, valid_loss: 0.01624459813215903\n",
      "SEED: 1392, FOLD: 3, EPOCH: 23, train_loss: 0.016415971115339493\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 23, valid_loss: 0.016202020858015333\n",
      "SEED: 1392, FOLD: 3, EPOCH: 24, train_loss: 0.016212773819764454\n",
      "SEED: 1392 ,FOLD: 3, EPOCH: 24, valid_loss: 0.016190325734870774\n",
      "SEED: 1392, FOLD: 4, EPOCH: 0, train_loss: 0.47128651995697746\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 0, valid_loss: 0.024149864645940915\n",
      "SEED: 1392, FOLD: 4, EPOCH: 1, train_loss: 0.023810982920121456\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 1, valid_loss: 0.01907930358179978\n",
      "SEED: 1392, FOLD: 4, EPOCH: 2, train_loss: 0.02193908227364654\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 2, valid_loss: 0.017681001206593855\n",
      "SEED: 1392, FOLD: 4, EPOCH: 3, train_loss: 0.020676247910528942\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 3, valid_loss: 0.017293299974075384\n",
      "SEED: 1392, FOLD: 4, EPOCH: 4, train_loss: 0.020274605210600555\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 4, valid_loss: 0.017033241183630057\n",
      "SEED: 1392, FOLD: 4, EPOCH: 5, train_loss: 0.020159083540024963\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 5, valid_loss: 0.017307451739907265\n",
      "SEED: 1392, FOLD: 4, EPOCH: 6, train_loss: 0.020209863889908444\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 6, valid_loss: 0.017195868039769784\n",
      "SEED: 1392, FOLD: 4, EPOCH: 7, train_loss: 0.020197638067538323\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 7, valid_loss: 0.01747746808188302\n",
      "SEED: 1392, FOLD: 4, EPOCH: 8, train_loss: 0.02013673815552307\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 8, valid_loss: 0.01791605960045542\n",
      "SEED: 1392, FOLD: 4, EPOCH: 9, train_loss: 0.02020806384583314\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 9, valid_loss: 0.017776905106646673\n",
      "SEED: 1392, FOLD: 4, EPOCH: 10, train_loss: 0.020179215859135857\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 10, valid_loss: 0.017313742983554092\n",
      "SEED: 1392, FOLD: 4, EPOCH: 11, train_loss: 0.020096116213370926\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 11, valid_loss: 0.017027915881148405\n",
      "SEED: 1392, FOLD: 4, EPOCH: 12, train_loss: 0.020017346502214237\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 12, valid_loss: 0.017049730249813624\n",
      "SEED: 1392, FOLD: 4, EPOCH: 13, train_loss: 0.019936006556710472\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 13, valid_loss: 0.016779997306210655\n",
      "SEED: 1392, FOLD: 4, EPOCH: 14, train_loss: 0.019813903418464073\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 14, valid_loss: 0.01686957446592195\n",
      "SEED: 1392, FOLD: 4, EPOCH: 15, train_loss: 0.01955895791289167\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 15, valid_loss: 0.016680785454809666\n",
      "SEED: 1392, FOLD: 4, EPOCH: 16, train_loss: 0.019335925956999046\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 16, valid_loss: 0.016671810458813396\n",
      "SEED: 1392, FOLD: 4, EPOCH: 17, train_loss: 0.019033614033158276\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 17, valid_loss: 0.016503473690577915\n",
      "SEED: 1392, FOLD: 4, EPOCH: 18, train_loss: 0.018748322960691177\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 18, valid_loss: 0.01632308821593012\n",
      "SEED: 1392, FOLD: 4, EPOCH: 19, train_loss: 0.018322042796922768\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 19, valid_loss: 0.016243350745311805\n",
      "SEED: 1392, FOLD: 4, EPOCH: 20, train_loss: 0.017845967339108818\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 20, valid_loss: 0.016168678339038577\n",
      "SEED: 1392, FOLD: 4, EPOCH: 21, train_loss: 0.01735708189021418\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 21, valid_loss: 0.015969852251665934\n",
      "SEED: 1392, FOLD: 4, EPOCH: 22, train_loss: 0.016773669933225367\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 22, valid_loss: 0.015997802120234286\n",
      "SEED: 1392, FOLD: 4, EPOCH: 23, train_loss: 0.016349732079475685\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 23, valid_loss: 0.01600822920777968\n",
      "SEED: 1392, FOLD: 4, EPOCH: 24, train_loss: 0.016165737366384787\n",
      "SEED: 1392 ,FOLD: 4, EPOCH: 24, valid_loss: 0.016017897187599113\n",
      "SEED: 1119, FOLD: 0, EPOCH: 0, train_loss: 0.47078741121821216\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 0, valid_loss: 0.02476025064076696\n",
      "SEED: 1119, FOLD: 0, EPOCH: 1, train_loss: 0.023680937381974167\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 1, valid_loss: 0.01964645885995456\n",
      "SEED: 1119, FOLD: 0, EPOCH: 2, train_loss: 0.02240397858068995\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 2, valid_loss: 0.021232120958822116\n",
      "SEED: 1119, FOLD: 0, EPOCH: 3, train_loss: 0.02139126348808624\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 3, valid_loss: 0.017884069202201706\n",
      "SEED: 1119, FOLD: 0, EPOCH: 4, train_loss: 0.020553548860809078\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 4, valid_loss: 0.017476763895579745\n",
      "SEED: 1119, FOLD: 0, EPOCH: 5, train_loss: 0.020206152245510315\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 5, valid_loss: 0.017224115319550037\n",
      "SEED: 1119, FOLD: 0, EPOCH: 6, train_loss: 0.02020091425789439\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 6, valid_loss: 0.0173733298267637\n",
      "SEED: 1119, FOLD: 0, EPOCH: 7, train_loss: 0.020261885827758175\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 7, valid_loss: 0.01721831200910466\n",
      "SEED: 1119, FOLD: 0, EPOCH: 8, train_loss: 0.020165485086972298\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 8, valid_loss: 0.017278021121663708\n",
      "SEED: 1119, FOLD: 0, EPOCH: 9, train_loss: 0.020209318782756294\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 9, valid_loss: 0.01732654765780483\n",
      "SEED: 1119, FOLD: 0, EPOCH: 10, train_loss: 0.02010589497892753\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 10, valid_loss: 0.017249988072684835\n",
      "SEED: 1119, FOLD: 0, EPOCH: 11, train_loss: 0.020068194825148235\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 11, valid_loss: 0.017065924326223985\n",
      "SEED: 1119, FOLD: 0, EPOCH: 12, train_loss: 0.019987631076271984\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 12, valid_loss: 0.016869527633701052\n",
      "SEED: 1119, FOLD: 0, EPOCH: 13, train_loss: 0.019892819252782974\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 13, valid_loss: 0.017008571779089314\n",
      "SEED: 1119, FOLD: 0, EPOCH: 14, train_loss: 0.0198512673512965\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 14, valid_loss: 0.016872877441346645\n",
      "SEED: 1119, FOLD: 0, EPOCH: 15, train_loss: 0.019657006973157757\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 15, valid_loss: 0.016791193479938165\n",
      "SEED: 1119, FOLD: 0, EPOCH: 16, train_loss: 0.019431073448040348\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 16, valid_loss: 0.0165302184011255\n",
      "SEED: 1119, FOLD: 0, EPOCH: 17, train_loss: 0.019126128216368565\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 17, valid_loss: 0.016625387008701052\n",
      "SEED: 1119, FOLD: 0, EPOCH: 18, train_loss: 0.018782286432342254\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 18, valid_loss: 0.01633075103163719\n",
      "SEED: 1119, FOLD: 0, EPOCH: 19, train_loss: 0.01838855651895637\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 19, valid_loss: 0.016352045722305773\n",
      "SEED: 1119, FOLD: 0, EPOCH: 20, train_loss: 0.01789192072507264\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 20, valid_loss: 0.01614570545830897\n",
      "SEED: 1119, FOLD: 0, EPOCH: 21, train_loss: 0.01736451254185775\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 21, valid_loss: 0.01610353511891195\n",
      "SEED: 1119, FOLD: 0, EPOCH: 22, train_loss: 0.016798898732910555\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 22, valid_loss: 0.016029854517962253\n",
      "SEED: 1119, FOLD: 0, EPOCH: 23, train_loss: 0.016331551151107185\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 23, valid_loss: 0.015994429109351976\n",
      "SEED: 1119, FOLD: 0, EPOCH: 24, train_loss: 0.01612994181451158\n",
      "SEED: 1119 ,FOLD: 0, EPOCH: 24, valid_loss: 0.01607697369264705\n",
      "SEED: 1119, FOLD: 1, EPOCH: 0, train_loss: 0.47099824781543104\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 0, valid_loss: 0.024888851919344493\n",
      "SEED: 1119, FOLD: 1, EPOCH: 1, train_loss: 0.023803210722795433\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 1, valid_loss: 0.019139371971998897\n",
      "SEED: 1119, FOLD: 1, EPOCH: 2, train_loss: 0.02186434733532909\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 2, valid_loss: 0.01880417673715523\n",
      "SEED: 1119, FOLD: 1, EPOCH: 3, train_loss: 0.020705446398452572\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 3, valid_loss: 0.01828270485358579\n",
      "SEED: 1119, FOLD: 1, EPOCH: 4, train_loss: 0.02056613727810158\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 4, valid_loss: 0.017295306654913085\n",
      "SEED: 1119, FOLD: 1, EPOCH: 5, train_loss: 0.020202170908990978\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 5, valid_loss: 0.017529884060578687\n",
      "SEED: 1119, FOLD: 1, EPOCH: 6, train_loss: 0.020200084800413555\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 6, valid_loss: 0.017471235378512313\n",
      "SEED: 1119, FOLD: 1, EPOCH: 7, train_loss: 0.020168326997562595\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 7, valid_loss: 0.01739133766719273\n",
      "SEED: 1119, FOLD: 1, EPOCH: 8, train_loss: 0.020143961925329506\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 8, valid_loss: 0.017247423050659044\n",
      "SEED: 1119, FOLD: 1, EPOCH: 9, train_loss: 0.02016634809906068\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 9, valid_loss: 0.017228393017181327\n",
      "SEED: 1119, FOLD: 1, EPOCH: 10, train_loss: 0.020202973250137722\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 10, valid_loss: 0.01718692210103784\n",
      "SEED: 1119, FOLD: 1, EPOCH: 11, train_loss: 0.020035824266032898\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 11, valid_loss: 0.01703111143516643\n",
      "SEED: 1119, FOLD: 1, EPOCH: 12, train_loss: 0.01995852284565352\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 12, valid_loss: 0.017063664831221105\n",
      "SEED: 1119, FOLD: 1, EPOCH: 13, train_loss: 0.019938687146033928\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 13, valid_loss: 0.01681727830852781\n",
      "SEED: 1119, FOLD: 1, EPOCH: 14, train_loss: 0.01975756735149501\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 14, valid_loss: 0.016848480009606908\n",
      "SEED: 1119, FOLD: 1, EPOCH: 15, train_loss: 0.019632524567777695\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 15, valid_loss: 0.016618780978024005\n",
      "SEED: 1119, FOLD: 1, EPOCH: 16, train_loss: 0.019416275218237137\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 16, valid_loss: 0.016542367610548225\n",
      "SEED: 1119, FOLD: 1, EPOCH: 17, train_loss: 0.01907615197579498\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 17, valid_loss: 0.016470623867852346\n",
      "SEED: 1119, FOLD: 1, EPOCH: 18, train_loss: 0.018768825757222763\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 18, valid_loss: 0.016330286887075218\n",
      "SEED: 1119, FOLD: 1, EPOCH: 19, train_loss: 0.018356460659508255\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 19, valid_loss: 0.016220624132880144\n",
      "SEED: 1119, FOLD: 1, EPOCH: 20, train_loss: 0.017842037299566942\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 20, valid_loss: 0.016131814941763876\n",
      "SEED: 1119, FOLD: 1, EPOCH: 21, train_loss: 0.017325292172693255\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 21, valid_loss: 0.016074039946709362\n",
      "SEED: 1119, FOLD: 1, EPOCH: 22, train_loss: 0.016807684472397617\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 22, valid_loss: 0.01603620076285941\n",
      "SEED: 1119, FOLD: 1, EPOCH: 23, train_loss: 0.016410146434993847\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 23, valid_loss: 0.016077571628349167\n",
      "SEED: 1119, FOLD: 1, EPOCH: 24, train_loss: 0.01617066988694495\n",
      "SEED: 1119 ,FOLD: 1, EPOCH: 24, valid_loss: 0.01604879123291799\n",
      "SEED: 1119, FOLD: 2, EPOCH: 0, train_loss: 0.4716915391196591\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 0, valid_loss: 0.023667123009051595\n",
      "SEED: 1119, FOLD: 2, EPOCH: 1, train_loss: 0.023664293486786926\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 1, valid_loss: 0.01953633970447949\n",
      "SEED: 1119, FOLD: 2, EPOCH: 2, train_loss: 0.02194002949619207\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 2, valid_loss: 0.01956629753112793\n",
      "SEED: 1119, FOLD: 2, EPOCH: 3, train_loss: 0.020721362363817036\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 3, valid_loss: 0.017315724438854627\n",
      "SEED: 1119, FOLD: 2, EPOCH: 4, train_loss: 0.02019428801925286\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 4, valid_loss: 0.01765494304043906\n",
      "SEED: 1119, FOLD: 2, EPOCH: 5, train_loss: 0.020361167324733906\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 5, valid_loss: 0.017141203662114485\n",
      "SEED: 1119, FOLD: 2, EPOCH: 6, train_loss: 0.02012489722582741\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 6, valid_loss: 0.01789473547999348\n",
      "SEED: 1119, FOLD: 2, EPOCH: 7, train_loss: 0.020142095203956833\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 7, valid_loss: 0.01725275032222271\n",
      "SEED: 1119, FOLD: 2, EPOCH: 8, train_loss: 0.020141645753081295\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 8, valid_loss: 0.01716967863695962\n",
      "SEED: 1119, FOLD: 2, EPOCH: 9, train_loss: 0.020174440468890942\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 9, valid_loss: 0.01753185431339911\n",
      "SEED: 1119, FOLD: 2, EPOCH: 10, train_loss: 0.020189084733525913\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 10, valid_loss: 0.017101614230445452\n",
      "SEED: 1119, FOLD: 2, EPOCH: 11, train_loss: 0.02004144334004841\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 11, valid_loss: 0.0170640461945108\n",
      "SEED: 1119, FOLD: 2, EPOCH: 12, train_loss: 0.019980503408157307\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 12, valid_loss: 0.016930017114749975\n",
      "SEED: 1119, FOLD: 2, EPOCH: 13, train_loss: 0.019891710533504036\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 13, valid_loss: 0.016873117270214216\n",
      "SEED: 1119, FOLD: 2, EPOCH: 14, train_loss: 0.019779032798133034\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 14, valid_loss: 0.016692526532070977\n",
      "SEED: 1119, FOLD: 2, EPOCH: 15, train_loss: 0.019493173794361992\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 15, valid_loss: 0.016715641346360957\n",
      "SEED: 1119, FOLD: 2, EPOCH: 16, train_loss: 0.019324756618859112\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 16, valid_loss: 0.016489709514592374\n",
      "SEED: 1119, FOLD: 2, EPOCH: 17, train_loss: 0.019055326469242573\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 17, valid_loss: 0.016554961167275905\n",
      "SEED: 1119, FOLD: 2, EPOCH: 18, train_loss: 0.018712664508949154\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 18, valid_loss: 0.016306697178099835\n",
      "SEED: 1119, FOLD: 2, EPOCH: 19, train_loss: 0.01828464135473621\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 19, valid_loss: 0.016199714078434877\n",
      "SEED: 1119, FOLD: 2, EPOCH: 20, train_loss: 0.017796956124189106\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 20, valid_loss: 0.016114348598888942\n",
      "SEED: 1119, FOLD: 2, EPOCH: 21, train_loss: 0.017159724695796984\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 21, valid_loss: 0.016052967363170214\n",
      "SEED: 1119, FOLD: 2, EPOCH: 22, train_loss: 0.01660815935473943\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 22, valid_loss: 0.01610801863883223\n",
      "SEED: 1119, FOLD: 2, EPOCH: 23, train_loss: 0.016253862779695486\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 23, valid_loss: 0.016031106135674884\n",
      "SEED: 1119, FOLD: 2, EPOCH: 24, train_loss: 0.015957186308999855\n",
      "SEED: 1119 ,FOLD: 2, EPOCH: 24, valid_loss: 0.016045741524015153\n",
      "SEED: 1119, FOLD: 3, EPOCH: 0, train_loss: 0.47045380107896484\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 0, valid_loss: 0.025052213562386377\n",
      "SEED: 1119, FOLD: 3, EPOCH: 1, train_loss: 0.023892351345199604\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 1, valid_loss: 0.019363578728267123\n",
      "SEED: 1119, FOLD: 3, EPOCH: 2, train_loss: 0.02172200129353914\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 2, valid_loss: 0.019283893810851233\n",
      "SEED: 1119, FOLD: 3, EPOCH: 3, train_loss: 0.02075342511407275\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 3, valid_loss: 0.017658657101648195\n",
      "SEED: 1119, FOLD: 3, EPOCH: 4, train_loss: 0.02029319872836704\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 4, valid_loss: 0.017872917678739345\n",
      "SEED: 1119, FOLD: 3, EPOCH: 5, train_loss: 0.02017232515187799\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 5, valid_loss: 0.01751037130930594\n",
      "SEED: 1119, FOLD: 3, EPOCH: 6, train_loss: 0.020177282028548096\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 6, valid_loss: 0.017985298777265207\n",
      "SEED: 1119, FOLD: 3, EPOCH: 7, train_loss: 0.020214133153575054\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 7, valid_loss: 0.01723062435963324\n",
      "SEED: 1119, FOLD: 3, EPOCH: 8, train_loss: 0.020126168845572334\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 8, valid_loss: 0.017260247442339147\n",
      "SEED: 1119, FOLD: 3, EPOCH: 9, train_loss: 0.02009364325499189\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 9, valid_loss: 0.01783063480896609\n",
      "SEED: 1119, FOLD: 3, EPOCH: 10, train_loss: 0.02003753494363332\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 10, valid_loss: 0.01859143796776022\n",
      "SEED: 1119, FOLD: 3, EPOCH: 11, train_loss: 0.02002437599003315\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 11, valid_loss: 0.01726309454866818\n",
      "SEED: 1119, FOLD: 3, EPOCH: 12, train_loss: 0.019943333635835545\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 12, valid_loss: 0.017217406630516054\n",
      "SEED: 1119, FOLD: 3, EPOCH: 13, train_loss: 0.019810853256047634\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 13, valid_loss: 0.0170511145144701\n",
      "SEED: 1119, FOLD: 3, EPOCH: 14, train_loss: 0.01972846427689428\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 14, valid_loss: 0.0168306169499244\n",
      "SEED: 1119, FOLD: 3, EPOCH: 15, train_loss: 0.019484504045027752\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 15, valid_loss: 0.017014136484691075\n",
      "SEED: 1119, FOLD: 3, EPOCH: 16, train_loss: 0.019299137837968876\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 16, valid_loss: 0.016737552545964717\n",
      "SEED: 1119, FOLD: 3, EPOCH: 17, train_loss: 0.018977728942274185\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 17, valid_loss: 0.01671099362096616\n",
      "SEED: 1119, FOLD: 3, EPOCH: 18, train_loss: 0.018617337961019814\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 18, valid_loss: 0.016645792100046363\n",
      "SEED: 1119, FOLD: 3, EPOCH: 19, train_loss: 0.018189984549214874\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 19, valid_loss: 0.016417035806391922\n",
      "SEED: 1119, FOLD: 3, EPOCH: 20, train_loss: 0.01773988352953524\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 20, valid_loss: 0.01633389134492193\n",
      "SEED: 1119, FOLD: 3, EPOCH: 21, train_loss: 0.017114525141221457\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 21, valid_loss: 0.016231293976306915\n",
      "SEED: 1119, FOLD: 3, EPOCH: 22, train_loss: 0.016500177354538355\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 22, valid_loss: 0.01623471951378243\n",
      "SEED: 1119, FOLD: 3, EPOCH: 23, train_loss: 0.016049687399704388\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 23, valid_loss: 0.0162316621946437\n",
      "SEED: 1119, FOLD: 3, EPOCH: 24, train_loss: 0.015821432769028605\n",
      "SEED: 1119 ,FOLD: 3, EPOCH: 24, valid_loss: 0.01623969908271517\n",
      "SEED: 1119, FOLD: 4, EPOCH: 0, train_loss: 0.47133828166440345\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 0, valid_loss: 0.024707865129624095\n",
      "SEED: 1119, FOLD: 4, EPOCH: 1, train_loss: 0.02381796396566906\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 1, valid_loss: 0.019229135130132947\n",
      "SEED: 1119, FOLD: 4, EPOCH: 2, train_loss: 0.021676778334422386\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 2, valid_loss: 0.018359359167516232\n",
      "SEED: 1119, FOLD: 4, EPOCH: 3, train_loss: 0.02086541268542625\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 3, valid_loss: 0.017492987774312498\n",
      "SEED: 1119, FOLD: 4, EPOCH: 4, train_loss: 0.020346260535112327\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 4, valid_loss: 0.01872840023466519\n",
      "SEED: 1119, FOLD: 4, EPOCH: 5, train_loss: 0.020318943669722565\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 5, valid_loss: 0.017135282393012728\n",
      "SEED: 1119, FOLD: 4, EPOCH: 6, train_loss: 0.020149466703119484\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 6, valid_loss: 0.01724755849157061\n",
      "SEED: 1119, FOLD: 4, EPOCH: 7, train_loss: 0.020170312752758247\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 7, valid_loss: 0.017259119264781474\n",
      "SEED: 1119, FOLD: 4, EPOCH: 8, train_loss: 0.020206276813279026\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 8, valid_loss: 0.01734926567545959\n",
      "SEED: 1119, FOLD: 4, EPOCH: 9, train_loss: 0.020220648173404777\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 9, valid_loss: 0.017164781955736026\n",
      "SEED: 1119, FOLD: 4, EPOCH: 10, train_loss: 0.020212830327775166\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 10, valid_loss: 0.0170874364141907\n",
      "SEED: 1119, FOLD: 4, EPOCH: 11, train_loss: 0.020081406661673733\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 11, valid_loss: 0.016985330783895084\n",
      "SEED: 1119, FOLD: 4, EPOCH: 12, train_loss: 0.019944655182568924\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 12, valid_loss: 0.01688774483544486\n",
      "SEED: 1119, FOLD: 4, EPOCH: 13, train_loss: 0.01992326352637315\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 13, valid_loss: 0.016802590819341795\n",
      "SEED: 1119, FOLD: 4, EPOCH: 14, train_loss: 0.019832065409940224\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 14, valid_loss: 0.016841927410236428\n",
      "SEED: 1119, FOLD: 4, EPOCH: 15, train_loss: 0.019571160884115143\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 15, valid_loss: 0.01661477421543428\n",
      "SEED: 1119, FOLD: 4, EPOCH: 16, train_loss: 0.019415428929462814\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 16, valid_loss: 0.016599706773247038\n",
      "SEED: 1119, FOLD: 4, EPOCH: 17, train_loss: 0.019104689919808203\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 17, valid_loss: 0.016565662063658237\n",
      "SEED: 1119, FOLD: 4, EPOCH: 18, train_loss: 0.018735217119472614\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 18, valid_loss: 0.016327608935534955\n",
      "SEED: 1119, FOLD: 4, EPOCH: 19, train_loss: 0.018360996126210775\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 19, valid_loss: 0.016257525022540773\n",
      "SEED: 1119, FOLD: 4, EPOCH: 20, train_loss: 0.017936152642241854\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 20, valid_loss: 0.016134731604584625\n",
      "SEED: 1119, FOLD: 4, EPOCH: 21, train_loss: 0.01741220692064667\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 21, valid_loss: 0.016068573908082077\n",
      "SEED: 1119, FOLD: 4, EPOCH: 22, train_loss: 0.016894541269141264\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 22, valid_loss: 0.016049916962427753\n",
      "SEED: 1119, FOLD: 4, EPOCH: 23, train_loss: 0.016417888971720484\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 23, valid_loss: 0.016040800245744842\n",
      "SEED: 1119, FOLD: 4, EPOCH: 24, train_loss: 0.016261611990900576\n",
      "SEED: 1119 ,FOLD: 4, EPOCH: 24, valid_loss: 0.016069870495370456\n",
      "SEED: 1303, FOLD: 0, EPOCH: 0, train_loss: 0.4718494196894808\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 0, valid_loss: 0.02611260786652565\n",
      "SEED: 1303, FOLD: 0, EPOCH: 1, train_loss: 0.02365400701545287\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 1, valid_loss: 0.021127757483295033\n",
      "SEED: 1303, FOLD: 0, EPOCH: 2, train_loss: 0.02290954096647708\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 2, valid_loss: 0.02013932860323361\n",
      "SEED: 1303, FOLD: 0, EPOCH: 3, train_loss: 0.021301676775666252\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 3, valid_loss: 0.01750053023653371\n",
      "SEED: 1303, FOLD: 0, EPOCH: 4, train_loss: 0.020496038774001427\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 4, valid_loss: 0.017564899075244153\n",
      "SEED: 1303, FOLD: 0, EPOCH: 5, train_loss: 0.020320907869524715\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 5, valid_loss: 0.01777441200933286\n",
      "SEED: 1303, FOLD: 0, EPOCH: 6, train_loss: 0.020325528363278812\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 6, valid_loss: 0.01742417546255248\n",
      "SEED: 1303, FOLD: 0, EPOCH: 7, train_loss: 0.02025052161374386\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 7, valid_loss: 0.01734316631087235\n",
      "SEED: 1303, FOLD: 0, EPOCH: 8, train_loss: 0.020271379919047806\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 8, valid_loss: 0.01744323072156736\n",
      "SEED: 1303, FOLD: 0, EPOCH: 9, train_loss: 0.02021294952356729\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 9, valid_loss: 0.017144766503146718\n",
      "SEED: 1303, FOLD: 0, EPOCH: 10, train_loss: 0.020169080963925175\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 10, valid_loss: 0.017070123845977444\n",
      "SEED: 1303, FOLD: 0, EPOCH: 11, train_loss: 0.020088876476106438\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 11, valid_loss: 0.017136165659342492\n",
      "SEED: 1303, FOLD: 0, EPOCH: 12, train_loss: 0.020054677636295124\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 12, valid_loss: 0.01721916685679129\n",
      "SEED: 1303, FOLD: 0, EPOCH: 13, train_loss: 0.01992479431024496\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 13, valid_loss: 0.01733925568738154\n",
      "SEED: 1303, FOLD: 0, EPOCH: 14, train_loss: 0.01980216357101133\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 14, valid_loss: 0.01673486437648535\n",
      "SEED: 1303, FOLD: 0, EPOCH: 15, train_loss: 0.019696682475615238\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 15, valid_loss: 0.01690051286880459\n",
      "SEED: 1303, FOLD: 0, EPOCH: 16, train_loss: 0.019402021380222362\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 16, valid_loss: 0.016596173282180514\n",
      "SEED: 1303, FOLD: 0, EPOCH: 17, train_loss: 0.01903417050514532\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 17, valid_loss: 0.0163941034248897\n",
      "SEED: 1303, FOLD: 0, EPOCH: 18, train_loss: 0.018757509769520897\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 18, valid_loss: 0.01628792839390891\n",
      "SEED: 1303, FOLD: 0, EPOCH: 19, train_loss: 0.01839914936842262\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 19, valid_loss: 0.016168565223259584\n",
      "SEED: 1303, FOLD: 0, EPOCH: 20, train_loss: 0.01789280880863468\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 20, valid_loss: 0.01608883887529373\n",
      "SEED: 1303, FOLD: 0, EPOCH: 21, train_loss: 0.017345744239139385\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 21, valid_loss: 0.015964315485741412\n",
      "SEED: 1303, FOLD: 0, EPOCH: 22, train_loss: 0.016808938100070194\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 22, valid_loss: 0.01600719251270805\n",
      "SEED: 1303, FOLD: 0, EPOCH: 23, train_loss: 0.016304520613419405\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 23, valid_loss: 0.016040578138615404\n",
      "SEED: 1303, FOLD: 0, EPOCH: 24, train_loss: 0.016083402133992186\n",
      "SEED: 1303 ,FOLD: 0, EPOCH: 24, valid_loss: 0.015976770355233125\n",
      "SEED: 1303, FOLD: 1, EPOCH: 0, train_loss: 0.4705722823307134\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 0, valid_loss: 0.02391692384013108\n",
      "SEED: 1303, FOLD: 1, EPOCH: 1, train_loss: 0.023708835176691628\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 1, valid_loss: 0.01871613955923489\n",
      "SEED: 1303, FOLD: 1, EPOCH: 2, train_loss: 0.021903235357308735\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 2, valid_loss: 0.020335477058376586\n",
      "SEED: 1303, FOLD: 1, EPOCH: 3, train_loss: 0.0211158178989654\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 3, valid_loss: 0.017246578447520734\n",
      "SEED: 1303, FOLD: 1, EPOCH: 4, train_loss: 0.020281049274448036\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 4, valid_loss: 0.017476925386914186\n",
      "SEED: 1303, FOLD: 1, EPOCH: 5, train_loss: 0.020176796553035576\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 5, valid_loss: 0.018317329457827977\n",
      "SEED: 1303, FOLD: 1, EPOCH: 6, train_loss: 0.02020905403069396\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 6, valid_loss: 0.01733841619321278\n",
      "SEED: 1303, FOLD: 1, EPOCH: 7, train_loss: 0.020130361902756966\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 7, valid_loss: 0.01744929775595665\n",
      "SEED: 1303, FOLD: 1, EPOCH: 8, train_loss: 0.020121873126945633\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 8, valid_loss: 0.017273662558623722\n",
      "SEED: 1303, FOLD: 1, EPOCH: 9, train_loss: 0.02016860679925784\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 9, valid_loss: 0.017127508270953384\n",
      "SEED: 1303, FOLD: 1, EPOCH: 10, train_loss: 0.020119821193857468\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 10, valid_loss: 0.017267043728913578\n",
      "SEED: 1303, FOLD: 1, EPOCH: 11, train_loss: 0.02006207509101301\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 11, valid_loss: 0.01722079816141299\n",
      "SEED: 1303, FOLD: 1, EPOCH: 12, train_loss: 0.019954850330300953\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 12, valid_loss: 0.01708168855735234\n",
      "SEED: 1303, FOLD: 1, EPOCH: 13, train_loss: 0.01987423580409824\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 13, valid_loss: 0.016938440714563643\n",
      "SEED: 1303, FOLD: 1, EPOCH: 14, train_loss: 0.01972303233122912\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 14, valid_loss: 0.01693253314920834\n",
      "SEED: 1303, FOLD: 1, EPOCH: 15, train_loss: 0.0195255678255057\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 15, valid_loss: 0.016756311963711468\n",
      "SEED: 1303, FOLD: 1, EPOCH: 16, train_loss: 0.019338935802596203\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 16, valid_loss: 0.01661242741559233\n",
      "SEED: 1303, FOLD: 1, EPOCH: 17, train_loss: 0.019103919095157282\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 17, valid_loss: 0.01662010263119425\n",
      "SEED: 1303, FOLD: 1, EPOCH: 18, train_loss: 0.018770945629617876\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 18, valid_loss: 0.016389012922133718\n",
      "SEED: 1303, FOLD: 1, EPOCH: 19, train_loss: 0.01831025351513771\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 19, valid_loss: 0.016284883953630924\n",
      "SEED: 1303, FOLD: 1, EPOCH: 20, train_loss: 0.01777969932426577\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 20, valid_loss: 0.01616349204310349\n",
      "SEED: 1303, FOLD: 1, EPOCH: 21, train_loss: 0.01726063498147372\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 21, valid_loss: 0.01611636108053582\n",
      "SEED: 1303, FOLD: 1, EPOCH: 22, train_loss: 0.01670689203277014\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 22, valid_loss: 0.016096322664192746\n",
      "SEED: 1303, FOLD: 1, EPOCH: 23, train_loss: 0.01631108817199002\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 23, valid_loss: 0.01610127104712384\n",
      "SEED: 1303, FOLD: 1, EPOCH: 24, train_loss: 0.016065601745377415\n",
      "SEED: 1303 ,FOLD: 1, EPOCH: 24, valid_loss: 0.016133374215236734\n",
      "SEED: 1303, FOLD: 2, EPOCH: 0, train_loss: 0.47142827720043884\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 0, valid_loss: 0.024679188430309296\n",
      "SEED: 1303, FOLD: 2, EPOCH: 1, train_loss: 0.024330817028016285\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 1, valid_loss: 0.020081155960048946\n",
      "SEED: 1303, FOLD: 2, EPOCH: 2, train_loss: 0.022016307190600513\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 2, valid_loss: 0.02060166299343109\n",
      "SEED: 1303, FOLD: 2, EPOCH: 3, train_loss: 0.021345164667329063\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 3, valid_loss: 0.017594250371413572\n",
      "SEED: 1303, FOLD: 2, EPOCH: 4, train_loss: 0.02038769927415727\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 4, valid_loss: 0.017514573942337717\n",
      "SEED: 1303, FOLD: 2, EPOCH: 5, train_loss: 0.020423340773128944\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 5, valid_loss: 0.01738041437097958\n",
      "SEED: 1303, FOLD: 2, EPOCH: 6, train_loss: 0.02026525077720483\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 6, valid_loss: 0.017591912326003822\n",
      "SEED: 1303, FOLD: 2, EPOCH: 7, train_loss: 0.02023626730331908\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 7, valid_loss: 0.017399230200265136\n",
      "SEED: 1303, FOLD: 2, EPOCH: 8, train_loss: 0.020174240126557972\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 8, valid_loss: 0.01759979956384216\n",
      "SEED: 1303, FOLD: 2, EPOCH: 9, train_loss: 0.020307444575904072\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 9, valid_loss: 0.01717204094997474\n",
      "SEED: 1303, FOLD: 2, EPOCH: 10, train_loss: 0.020153641714242058\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 10, valid_loss: 0.016984592777277742\n",
      "SEED: 1303, FOLD: 2, EPOCH: 11, train_loss: 0.02016884158702864\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 11, valid_loss: 0.01688414854662759\n",
      "SEED: 1303, FOLD: 2, EPOCH: 12, train_loss: 0.019996864059805008\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 12, valid_loss: 0.01707459595054388\n",
      "SEED: 1303, FOLD: 2, EPOCH: 13, train_loss: 0.019879486587276493\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 13, valid_loss: 0.01698624449116843\n",
      "SEED: 1303, FOLD: 2, EPOCH: 14, train_loss: 0.019818684936541577\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 14, valid_loss: 0.016773863562515805\n",
      "SEED: 1303, FOLD: 2, EPOCH: 15, train_loss: 0.019642318450454353\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 15, valid_loss: 0.016818421759775708\n",
      "SEED: 1303, FOLD: 2, EPOCH: 16, train_loss: 0.019419706521042877\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 16, valid_loss: 0.01675313628677811\n",
      "SEED: 1303, FOLD: 2, EPOCH: 17, train_loss: 0.019112926354442818\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 17, valid_loss: 0.016386708909911767\n",
      "SEED: 1303, FOLD: 2, EPOCH: 18, train_loss: 0.018827452918217667\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 18, valid_loss: 0.016305308655968735\n",
      "SEED: 1303, FOLD: 2, EPOCH: 19, train_loss: 0.01841101900714895\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 19, valid_loss: 0.016250735042350632\n",
      "SEED: 1303, FOLD: 2, EPOCH: 20, train_loss: 0.017891811200188124\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 20, valid_loss: 0.016120227479508944\n",
      "SEED: 1303, FOLD: 2, EPOCH: 21, train_loss: 0.01738184845695893\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 21, valid_loss: 0.01602214492325272\n",
      "SEED: 1303, FOLD: 2, EPOCH: 22, train_loss: 0.016802576566249994\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 22, valid_loss: 0.01599224696734122\n",
      "SEED: 1303, FOLD: 2, EPOCH: 23, train_loss: 0.01637762811952743\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 23, valid_loss: 0.015983224846422672\n",
      "SEED: 1303, FOLD: 2, EPOCH: 24, train_loss: 0.016130735362083582\n",
      "SEED: 1303 ,FOLD: 2, EPOCH: 24, valid_loss: 0.015989096143415996\n",
      "SEED: 1303, FOLD: 3, EPOCH: 0, train_loss: 0.4719119104354278\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 0, valid_loss: 0.024487373126404626\n",
      "SEED: 1303, FOLD: 3, EPOCH: 1, train_loss: 0.023754606576825398\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 1, valid_loss: 0.01901140016104494\n",
      "SEED: 1303, FOLD: 3, EPOCH: 2, train_loss: 0.021925156261178032\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 2, valid_loss: 0.018054535293153356\n",
      "SEED: 1303, FOLD: 3, EPOCH: 3, train_loss: 0.02075107135148584\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 3, valid_loss: 0.017651699962360518\n",
      "SEED: 1303, FOLD: 3, EPOCH: 4, train_loss: 0.02014718851938412\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 4, valid_loss: 0.01754793306546552\n",
      "SEED: 1303, FOLD: 3, EPOCH: 5, train_loss: 0.020241863431705944\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 5, valid_loss: 0.0178262898432357\n",
      "SEED: 1303, FOLD: 3, EPOCH: 6, train_loss: 0.020281130407491455\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 6, valid_loss: 0.017575859784015588\n",
      "SEED: 1303, FOLD: 3, EPOCH: 7, train_loss: 0.020168406794360584\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 7, valid_loss: 0.017610775945442064\n",
      "SEED: 1303, FOLD: 3, EPOCH: 8, train_loss: 0.02012535679977441\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 8, valid_loss: 0.017884926843856064\n",
      "SEED: 1303, FOLD: 3, EPOCH: 9, train_loss: 0.02025442439522864\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 9, valid_loss: 0.017435679345258644\n",
      "SEED: 1303, FOLD: 3, EPOCH: 10, train_loss: 0.020163262262940407\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 10, valid_loss: 0.017303422998104778\n",
      "SEED: 1303, FOLD: 3, EPOCH: 11, train_loss: 0.020105347104802513\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 11, valid_loss: 0.017216463147529535\n",
      "SEED: 1303, FOLD: 3, EPOCH: 12, train_loss: 0.020045244418408558\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 12, valid_loss: 0.017292528679328307\n",
      "SEED: 1303, FOLD: 3, EPOCH: 13, train_loss: 0.019885939923857433\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 13, valid_loss: 0.017122356061424527\n",
      "SEED: 1303, FOLD: 3, EPOCH: 14, train_loss: 0.01971258656324252\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 14, valid_loss: 0.016971434200448648\n",
      "SEED: 1303, FOLD: 3, EPOCH: 15, train_loss: 0.019591273078559967\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 15, valid_loss: 0.01689938370670591\n",
      "SEED: 1303, FOLD: 3, EPOCH: 16, train_loss: 0.01933727775146996\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 16, valid_loss: 0.016743604253445354\n",
      "SEED: 1303, FOLD: 3, EPOCH: 17, train_loss: 0.019048576222975618\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 17, valid_loss: 0.016543831011014327\n",
      "SEED: 1303, FOLD: 3, EPOCH: 18, train_loss: 0.018768576851141625\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 18, valid_loss: 0.016517964990011284\n",
      "SEED: 1303, FOLD: 3, EPOCH: 19, train_loss: 0.018371005711294172\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 19, valid_loss: 0.016342540617500033\n",
      "SEED: 1303, FOLD: 3, EPOCH: 20, train_loss: 0.01785436127523797\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 20, valid_loss: 0.016325589003307477\n",
      "SEED: 1303, FOLD: 3, EPOCH: 21, train_loss: 0.017369495009652514\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 21, valid_loss: 0.016270924705479826\n",
      "SEED: 1303, FOLD: 3, EPOCH: 22, train_loss: 0.0168015509708852\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 22, valid_loss: 0.01619546559772321\n",
      "SEED: 1303, FOLD: 3, EPOCH: 23, train_loss: 0.016437679468451635\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 23, valid_loss: 0.01617004778236151\n",
      "SEED: 1303, FOLD: 3, EPOCH: 24, train_loss: 0.016260400468456573\n",
      "SEED: 1303 ,FOLD: 3, EPOCH: 24, valid_loss: 0.01616865981902395\n",
      "SEED: 1303, FOLD: 4, EPOCH: 0, train_loss: 0.4728069345642259\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 0, valid_loss: 0.02428517644958837\n",
      "SEED: 1303, FOLD: 4, EPOCH: 1, train_loss: 0.023889950760032818\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 1, valid_loss: 0.01902125073330743\n",
      "SEED: 1303, FOLD: 4, EPOCH: 2, train_loss: 0.022090966408343418\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 2, valid_loss: 0.018100421077438762\n",
      "SEED: 1303, FOLD: 4, EPOCH: 3, train_loss: 0.02068671206201332\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 3, valid_loss: 0.01745030653796026\n",
      "SEED: 1303, FOLD: 4, EPOCH: 4, train_loss: 0.02070647614427667\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 4, valid_loss: 0.029311706072517804\n",
      "SEED: 1303, FOLD: 4, EPOCH: 5, train_loss: 0.021339285943279232\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 5, valid_loss: 0.017698171042970248\n",
      "SEED: 1303, FOLD: 4, EPOCH: 6, train_loss: 0.020546929924276428\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 6, valid_loss: 0.01727378184774092\n",
      "SEED: 1303, FOLD: 4, EPOCH: 7, train_loss: 0.020343489540011986\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 7, valid_loss: 0.017347909935883112\n",
      "SEED: 1303, FOLD: 4, EPOCH: 8, train_loss: 0.020300891155889934\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 8, valid_loss: 0.017199763203305858\n",
      "SEED: 1303, FOLD: 4, EPOCH: 9, train_loss: 0.02018993411321139\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 9, valid_loss: 0.017218328905957085\n",
      "SEED: 1303, FOLD: 4, EPOCH: 10, train_loss: 0.020205257325500683\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 10, valid_loss: 0.017237014855657304\n",
      "SEED: 1303, FOLD: 4, EPOCH: 11, train_loss: 0.020171684874356655\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 11, valid_loss: 0.016960725374519824\n",
      "SEED: 1303, FOLD: 4, EPOCH: 12, train_loss: 0.019969909586876198\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 12, valid_loss: 0.01709196740495307\n",
      "SEED: 1303, FOLD: 4, EPOCH: 13, train_loss: 0.019999748230844303\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 13, valid_loss: 0.016932504517691477\n",
      "SEED: 1303, FOLD: 4, EPOCH: 14, train_loss: 0.0197658179246861\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 14, valid_loss: 0.016858346201479434\n",
      "SEED: 1303, FOLD: 4, EPOCH: 15, train_loss: 0.01964850426800009\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 15, valid_loss: 0.016765473916062288\n",
      "SEED: 1303, FOLD: 4, EPOCH: 16, train_loss: 0.019452258701557697\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 16, valid_loss: 0.016579476371407508\n",
      "SEED: 1303, FOLD: 4, EPOCH: 17, train_loss: 0.01917306738703147\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 17, valid_loss: 0.016494170842426163\n",
      "SEED: 1303, FOLD: 4, EPOCH: 18, train_loss: 0.018866448960118534\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 18, valid_loss: 0.016364999887134347\n",
      "SEED: 1303, FOLD: 4, EPOCH: 19, train_loss: 0.01846854912414067\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 19, valid_loss: 0.016175017905022418\n",
      "SEED: 1303, FOLD: 4, EPOCH: 20, train_loss: 0.017901237082222233\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 20, valid_loss: 0.01614815438432353\n",
      "SEED: 1303, FOLD: 4, EPOCH: 21, train_loss: 0.017372702072927917\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 21, valid_loss: 0.01602057937000479\n",
      "SEED: 1303, FOLD: 4, EPOCH: 22, train_loss: 0.016838049603815096\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 22, valid_loss: 0.015967771730252673\n",
      "SEED: 1303, FOLD: 4, EPOCH: 23, train_loss: 0.016427458290928516\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 23, valid_loss: 0.0159377018521939\n",
      "SEED: 1303, FOLD: 4, EPOCH: 24, train_loss: 0.01620686553634595\n",
      "SEED: 1303 ,FOLD: 4, EPOCH: 24, valid_loss: 0.01594271763627018\n"
     ]
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "SEED = [940, 1513, 1269,1392,1119,1303]  #<-- Update\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "# 多次运行求平均值\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED) # 其实和最后都加起来再除一样（除法结合律）\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:59:19.593024Z",
     "iopub.status.busy": "2020-10-26T08:59:19.591873Z",
     "iopub.status.idle": "2020-10-26T08:59:19.621559Z",
     "shell.execute_reply": "2020-10-26T08:59:19.622078Z"
    },
    "papermill": {
     "duration": 0.406311,
     "end_time": "2020-10-26T08:59:19.622211",
     "exception": false,
     "start_time": "2020-10-26T08:59:19.215900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>id_fffc1c3f4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0      id_000644bb2                            0                       0   \n",
       "1      id_000779bfc                            0                       0   \n",
       "2      id_000a6266a                            0                       0   \n",
       "3      id_0015fd391                            0                       0   \n",
       "4      id_001626bd3                            0                       0   \n",
       "...             ...                          ...                     ...   \n",
       "23809  id_fffb1ceed                            0                       0   \n",
       "23810  id_fffb70c0c                            0                       0   \n",
       "23811  id_fffc1c3f4                            0                       0   \n",
       "23812  id_fffcb9e7c                            0                       0   \n",
       "23813  id_ffffdd77b                            0                       0   \n",
       "\n",
       "       acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0                   0                               0   \n",
       "1                   0                               0   \n",
       "2                   0                               0   \n",
       "3                   0                               0   \n",
       "4                   0                               0   \n",
       "...               ...                             ...   \n",
       "23809               0                               0   \n",
       "23810               0                               0   \n",
       "23811               0                               0   \n",
       "23812               0                               0   \n",
       "23813               0                               0   \n",
       "\n",
       "       acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                                      0                               0   \n",
       "1                                      0                               0   \n",
       "2                                      0                               0   \n",
       "3                                      0                               0   \n",
       "4                                      0                               0   \n",
       "...                                  ...                             ...   \n",
       "23809                                  0                               0   \n",
       "23810                                  0                               0   \n",
       "23811                                  0                               0   \n",
       "23812                                  0                               0   \n",
       "23813                                  0                               0   \n",
       "\n",
       "       adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                               0                              0   \n",
       "1                               0                              0   \n",
       "2                               0                              0   \n",
       "3                               0                              0   \n",
       "4                               0                              0   \n",
       "...                           ...                            ...   \n",
       "23809                           0                              0   \n",
       "23810                           0                              0   \n",
       "23811                           0                              0   \n",
       "23812                           0                              0   \n",
       "23813                           0                              0   \n",
       "\n",
       "       adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                               0  ...                                      0   \n",
       "1                               0  ...                                      0   \n",
       "2                               0  ...                                      0   \n",
       "3                               0  ...                                      0   \n",
       "4                               0  ...                                      0   \n",
       "...                           ...  ...                                    ...   \n",
       "23809                           0  ...                                      0   \n",
       "23810                           0  ...                                      0   \n",
       "23811                           0  ...                                      0   \n",
       "23812                           0  ...                                      0   \n",
       "23813                           0  ...                                      0   \n",
       "\n",
       "       trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0                 0                0                  0   \n",
       "1                 0                0                  0   \n",
       "2                 0                0                  0   \n",
       "3                 0                0                  0   \n",
       "4                 0                0                  0   \n",
       "...             ...              ...                ...   \n",
       "23809             0                0                  0   \n",
       "23810             0                0                  0   \n",
       "23811             0                0                  0   \n",
       "23812             0                0                  0   \n",
       "23813             0                0                  0   \n",
       "\n",
       "       tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                              0                                      0   \n",
       "1                              0                                      0   \n",
       "2                              0                                      0   \n",
       "3                              0                                      0   \n",
       "4                              0                                      0   \n",
       "...                          ...                                    ...   \n",
       "23809                          0                                      0   \n",
       "23810                          0                                      0   \n",
       "23811                          0                                      0   \n",
       "23812                          0                                      0   \n",
       "23813                          0                                      0   \n",
       "\n",
       "       vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0                    0          0                           0              0  \n",
       "1                    0          0                           0              0  \n",
       "2                    0          0                           0              0  \n",
       "3                    0          0                           0              0  \n",
       "4                    0          0                           0              0  \n",
       "...                ...        ...                         ...            ...  \n",
       "23809                0          0                           0              0  \n",
       "23810                0          0                           0              0  \n",
       "23811                0          0                           0              0  \n",
       "23812                0          0                           0              0  \n",
       "23813                0          0                           0              0  \n",
       "\n",
       "[23814 rows x 207 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:59:20.351932Z",
     "iopub.status.busy": "2020-10-26T08:59:20.350936Z",
     "iopub.status.idle": "2020-10-26T08:59:20.354327Z",
     "shell.execute_reply": "2020-10-26T08:59:20.354847Z"
    },
    "papermill": {
     "duration": 0.363639,
     "end_time": "2020-10-26T08:59:20.354996",
     "exception": false,
     "start_time": "2020-10-26T08:59:19.991357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:59:21.089306Z",
     "iopub.status.busy": "2020-10-26T08:59:21.087858Z",
     "iopub.status.idle": "2020-10-26T08:59:22.346985Z",
     "shell.execute_reply": "2020-10-26T08:59:22.347830Z"
    },
    "papermill": {
     "duration": 1.636187,
     "end_time": "2020-10-26T08:59:22.348044",
     "exception": false,
     "start_time": "2020-10-26T08:59:20.711857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV log_loss:  0.01454439906674656\n"
     ]
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "\n",
    "y_true = train_targets_scored[target_cols].values\n",
    "y_pred = valid_results[target_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:59:23.063979Z",
     "iopub.status.busy": "2020-10-26T08:59:23.062400Z",
     "iopub.status.idle": "2020-10-26T08:59:25.532399Z",
     "shell.execute_reply": "2020-10-26T08:59:25.531504Z"
    },
    "papermill": {
     "duration": 2.834068,
     "end_time": "2020-10-26T08:59:25.532537",
     "exception": false,
     "start_time": "2020-10-26T08:59:22.698469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T08:59:26.233852Z",
     "iopub.status.busy": "2020-10-26T08:59:26.232715Z",
     "iopub.status.idle": "2020-10-26T08:59:26.236452Z",
     "shell.execute_reply": "2020-10-26T08:59:26.236950Z"
    },
    "papermill": {
     "duration": 0.356657,
     "end_time": "2020-10-26T08:59:26.237119",
     "exception": false,
     "start_time": "2020-10-26T08:59:25.880462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3982, 207)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.442363,
     "end_time": "2020-10-26T08:59:27.195408",
     "exception": false,
     "start_time": "2020-10-26T08:59:26.753045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1385.904491,
   "end_time": "2020-10-26T08:59:29.057327",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-26T08:36:23.152836",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
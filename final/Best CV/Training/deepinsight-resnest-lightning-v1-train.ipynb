{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.6.0+cu101\n",
      "PyTorch Lightning Version: 1.0.4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[V1]\n",
    "* Add Max./Min. Channels\n",
    "\n",
    "[TODO]\n",
    "* Separate gene expression, cell vaibility and other features\n",
    "* PCGrad (Project Conflicting Gradients)\n",
    "* Tuning resolution and image size\n",
    "\n",
    "ResNeSt:\n",
    "https://github.com/zhanghang1989/ResNeSt\n",
    "\"\"\"\n",
    "\n",
    "kernel_mode = False\n",
    "training_mode = True\n",
    "\n",
    "import sys\n",
    "if kernel_mode:\n",
    "    sys.path.insert(0, \"../input/iterative-stratification\")\n",
    "    sys.path.insert(0, \"../input/pytorch-lightning\")\n",
    "    sys.path.insert(0, \"../input/resnest\")\n",
    "    sys.path.insert(0, \"../input/pytorch-optimizer\")\n",
    "    sys.path.insert(0, \"../input/pytorch-ranger\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "import glob\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, \\\n",
    "    RobustScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch_optimizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.metrics.functional import classification\n",
    "\n",
    "import resnest\n",
    "from resnest.torch import resnest50, resnest101, resnest200, resnest269, \\\n",
    "    resnest50_fast_2s2x40d, resnest50_fast_1s2x40d, resnest50_fast_1s1x64d\n",
    "\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug.augmenters.size import CropToFixedSize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "rand_seed = 1120\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning Version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if kernel_mode:\n",
    "#     !mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "#     !cp ../input/deepinsight-resnest-v1-resnest50/*.pth /root/.cache/torch/hub/checkpoints/\n",
    "#     !ls -la /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"resnest50\"\n",
    "pretrained_model = f\"resnest50_fast_2s2x40d\"\n",
    "experiment_name = f\"deepinsight_ResNeSt_v1_{model_type}\"\n",
    "\n",
    "if kernel_mode:\n",
    "    dataset_folder = \"../input/lish-moa\"\n",
    "    model_output_folder = f\"./{experiment_name}\" if training_mode \\\n",
    "        else f\"../input/deepinsight-resnest-v1-resnest50/{experiment_name}\"\n",
    "else:\n",
    "    dataset_folder = \"/workspace/Kaggle/MoA\"\n",
    "    model_output_folder = f\"{dataset_folder}/{experiment_name}\" if training_mode \\\n",
    "        else f\"/workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v1_resnest50/{experiment_name}\"\n",
    "\n",
    "if training_mode:\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    # Dedicated logger for experiment\n",
    "    exp_logger = TensorBoardLogger(model_output_folder,\n",
    "                                   name=f\"overall_logs\",\n",
    "                                   default_hp_metric=False)\n",
    "\n",
    "# debug_mode = True\n",
    "debug_mode = False\n",
    "\n",
    "num_workers = 2 if kernel_mode else 6\n",
    "# gpus = [0, 1]\n",
    "gpus = [0]\n",
    "# gpus = [1]\n",
    "\n",
    "epochs = 200\n",
    "patience = 16\n",
    "\n",
    "# learning_rate = 1e-3\n",
    "learning_rate = 0.000352  # Suggested Learning Rate from LR finder (V7)\n",
    "learning_rate *= len(gpus)\n",
    "weight_decay = 1e-6\n",
    "# weight_decay = 0\n",
    "\n",
    "# T_max = 10  # epochs\n",
    "T_max = 5  # epochs\n",
    "T_0 = 5  # epochs\n",
    "\n",
    "accumulate_grad_batches = 1\n",
    "gradient_clip_val = 10.0\n",
    "\n",
    "if \"resnest50\" in model_type:\n",
    "    batch_size = 128\n",
    "    infer_batch_size = 256 if not kernel_mode else 256\n",
    "    image_size = 224\n",
    "    resolution = 224\n",
    "elif model_type == \"resnest101\":\n",
    "    batch_size = 48\n",
    "    infer_batch_size = 96\n",
    "    image_size = 256\n",
    "    resolution = 256\n",
    "elif model_type == \"resnest200\":\n",
    "    batch_size = 12\n",
    "    infer_batch_size = 24\n",
    "    image_size = 320\n",
    "    resolution = 320\n",
    "elif model_type == \"resnest269\":\n",
    "    batch_size = 4\n",
    "    infer_batch_size = 8\n",
    "    image_size = 416\n",
    "    resolution = 416\n",
    "\n",
    "# Prediction Clipping Thresholds\n",
    "prob_min = 0.001\n",
    "prob_max = 0.999\n",
    "\n",
    "# Swap Noise\n",
    "swap_prob = 0.1\n",
    "swap_portion = 0.15\n",
    "\n",
    "label_smoothing = 0.001\n",
    "\n",
    "# DeepInsight Transform\n",
    "perplexity = 5\n",
    "\n",
    "fc_size = 512\n",
    "\n",
    "final_drop = 0.0\n",
    "dropblock_prob = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(\n",
    "    f\"{dataset_folder}/train_features.csv\", engine='c')\n",
    "train_labels = pd.read_csv(\n",
    "    f\"{dataset_folder}/train_targets_scored.csv\", engine='c')\n",
    "\n",
    "train_extra_labels = pd.read_csv(\n",
    "    f\"{dataset_folder}/train_targets_nonscored.csv\", engine='c')\n",
    "\n",
    "test_features = pd.read_csv(\n",
    "    f\"{dataset_folder}/test_features.csv\", engine='c')\n",
    "\n",
    "sample_submission = pd.read_csv(\n",
    "    f\"{dataset_folder}/sample_submission.csv\", engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by sig_id to ensure that all row orders match\n",
    "train_features = train_features.sort_values(\n",
    "    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)\n",
    "train_labels = train_labels.sort_values(by=[\"sig_id\"], axis=0,\n",
    "                                        inplace=False).reset_index(drop=True)\n",
    "train_extra_labels = train_extra_labels.sort_values(\n",
    "    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)\n",
    "\n",
    "sample_submission = sample_submission.sort_values(\n",
    "    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23814, 876), (23814, 207), (23814, 403))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, train_labels.shape, train_extra_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3982, 876)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(873, 772, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_features = [\"cp_type\", \"cp_dose\"]\n",
    "numeric_features = [c for c in train_features.columns if c != \"sig_id\" and c not in category_features]\n",
    "all_features = category_features + numeric_features\n",
    "gene_experssion_features = [c for c in numeric_features if c.startswith(\"g-\")]\n",
    "cell_viability_features = [c for c in numeric_features if c.startswith(\"c-\")]\n",
    "len(numeric_features), len(gene_experssion_features), len(cell_viability_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206, 402)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes = [c for c in train_labels.columns if c != \"sig_id\"]\n",
    "train_extra_classes = [c for c in train_extra_labels.columns if c != \"sig_id\"]\n",
    "len(train_classes), len(train_extra_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for df in [train_features, test_features]:\n",
    "    df['cp_type'] = df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n",
    "    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    df['cp_time'] = df['cp_time'].map({24: 0, 48: 0.5, 72: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    21948\n",
       "0     1866\n",
       "Name: cp_type, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"cp_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12147\n",
       "1    11667\n",
       "Name: cp_dose, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"cp_dose\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5    8250\n",
       "1.0    7792\n",
       "0.0    7772\n",
       "Name: cp_time, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"cp_time\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DeepInsight Transform (t-SNE)\n",
    "Based on https://github.com/alok-ai-lab/DeepInsight, but with some minor corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Modified from DeepInsight Transform\n",
    "# https://github.com/alok-ai-lab/DeepInsight/blob/master/pyDeepInsight/image_transformer.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib import pyplot as plt\n",
    "import inspect\n",
    "\n",
    "\n",
    "class DeepInsightTransformer:\n",
    "    \"\"\"Transform features to an image matrix using dimensionality reduction\n",
    "\n",
    "    This class takes in data normalized between 0 and 1 and converts it to a\n",
    "    CNN compatible 'image' matrix\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_extractor='tsne',\n",
    "                 perplexity=30,\n",
    "                 pixels=100,\n",
    "                 random_state=None,\n",
    "                 n_jobs=None):\n",
    "        \"\"\"Generate an ImageTransformer instance\n",
    "\n",
    "        Args:\n",
    "            feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n",
    "                class instance with method `fit_transform` that returns a\n",
    "                2-dimensional array of extracted features.\n",
    "            pixels: int (square matrix) or tuple of ints (height, width) that\n",
    "                defines the size of the image matrix.\n",
    "            random_state: int or RandomState. Determines the random number\n",
    "                generator, if present, of a string defined feature_extractor.\n",
    "            n_jobs: The number of parallel jobs to run for a string defined\n",
    "                feature_extractor.\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "        if isinstance(feature_extractor, str):\n",
    "            fe = feature_extractor.casefold()\n",
    "            if fe == 'tsne_exact'.casefold():\n",
    "                fe = TSNE(n_components=2,\n",
    "                          metric='cosine',\n",
    "                          perplexity=perplexity,\n",
    "                          n_iter=1000,\n",
    "                          method='exact',\n",
    "                          random_state=self.random_state,\n",
    "                          n_jobs=self.n_jobs)\n",
    "            elif fe == 'tsne'.casefold():\n",
    "                fe = TSNE(n_components=2,\n",
    "                          metric='cosine',\n",
    "                          perplexity=perplexity,\n",
    "                          n_iter=1000,\n",
    "                          method='barnes_hut',\n",
    "                          random_state=self.random_state,\n",
    "                          n_jobs=self.n_jobs)\n",
    "            elif fe == 'pca'.casefold():\n",
    "                fe = PCA(n_components=2, random_state=self.random_state)\n",
    "            elif fe == 'kpca'.casefold():\n",
    "                fe = KernelPCA(n_components=2,\n",
    "                               kernel='rbf',\n",
    "                               random_state=self.random_state,\n",
    "                               n_jobs=self.n_jobs)\n",
    "            else:\n",
    "                raise ValueError((\"Feature extraction method '{}' not accepted\"\n",
    "                                  ).format(feature_extractor))\n",
    "            self._fe = fe\n",
    "        elif hasattr(feature_extractor, 'fit_transform') and \\\n",
    "                inspect.ismethod(feature_extractor.fit_transform):\n",
    "            self._fe = feature_extractor\n",
    "        else:\n",
    "            raise TypeError('Parameter feature_extractor is not a '\n",
    "                            'string nor has method \"fit_transform\"')\n",
    "\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "\n",
    "        # The resolution of transformed image\n",
    "        self._pixels = pixels\n",
    "        self._xrot = None\n",
    "\n",
    "    def fit(self, X, y=None, plot=False):\n",
    "        \"\"\"Train the image transformer from the training set (X)\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            y: Ignored. Present for continuity with scikit-learn\n",
    "            plot: boolean of whether to produce a scatter plot showing the\n",
    "                feature reduction, hull points, and minimum bounding rectangle\n",
    "\n",
    "        Returns:\n",
    "            self: object\n",
    "        \"\"\"\n",
    "        # Transpose to get (n_features, n_samples)\n",
    "        X = X.T\n",
    "\n",
    "        # Perform dimensionality reduction\n",
    "        x_new = self._fe.fit_transform(X)\n",
    "\n",
    "        # Get the convex hull for the points\n",
    "        chvertices = ConvexHull(x_new).vertices\n",
    "        hull_points = x_new[chvertices]\n",
    "\n",
    "        # Determine the minimum bounding rectangle\n",
    "        mbr, mbr_rot = self._minimum_bounding_rectangle(hull_points)\n",
    "\n",
    "        # Rotate the matrix\n",
    "        # Save the rotated matrix in case user wants to change the pixel size\n",
    "        self._xrot = np.dot(mbr_rot, x_new.T).T\n",
    "\n",
    "        # Determine feature coordinates based on pixel dimension\n",
    "        self._calculate_coords()\n",
    "\n",
    "        # plot rotation diagram if requested\n",
    "        if plot is True:\n",
    "            # Create subplots\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n",
    "            ax[0, 0].scatter(x_new[:, 0],\n",
    "                             x_new[:, 1],\n",
    "                             cmap=plt.cm.get_cmap(\"jet\", 10),\n",
    "                             marker=\"x\",\n",
    "                             alpha=1.0)\n",
    "            ax[0, 0].fill(x_new[chvertices, 0],\n",
    "                          x_new[chvertices, 1],\n",
    "                          edgecolor='r',\n",
    "                          fill=False)\n",
    "            ax[0, 0].fill(mbr[:, 0], mbr[:, 1], edgecolor='g', fill=False)\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "            plt.show()\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def pixels(self):\n",
    "        \"\"\"The image matrix dimensions\n",
    "\n",
    "        Returns:\n",
    "            tuple: the image matrix dimensions (height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        return self._pixels\n",
    "\n",
    "    @pixels.setter\n",
    "    def pixels(self, pixels):\n",
    "        \"\"\"Set the image matrix dimension\n",
    "\n",
    "        Args:\n",
    "            pixels: int or tuple with the dimensions (height, width)\n",
    "            of the image matrix\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "        self._pixels = pixels\n",
    "        # recalculate coordinates if already fit\n",
    "        if hasattr(self, '_coords'):\n",
    "            self._calculate_coords()\n",
    "\n",
    "    def _calculate_coords(self):\n",
    "        \"\"\"Calculate the matrix coordinates of each feature based on the\n",
    "        pixel dimensions.\n",
    "        \"\"\"\n",
    "        ax0_coord = np.digitize(self._xrot[:, 0],\n",
    "                                bins=np.linspace(min(self._xrot[:, 0]),\n",
    "                                                 max(self._xrot[:, 0]),\n",
    "                                                 self._pixels[0])) - 1\n",
    "        ax1_coord = np.digitize(self._xrot[:, 1],\n",
    "                                bins=np.linspace(min(self._xrot[:, 1]),\n",
    "                                                 max(self._xrot[:, 1]),\n",
    "                                                 self._pixels[1])) - 1\n",
    "        self._coords = np.stack((ax0_coord, ax1_coord))\n",
    "\n",
    "    def transform(self, X, empty_value=0):\n",
    "        \"\"\"Transform the input matrix into image matrices\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "                where n_features matches the training set.\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        img_coords = pd.DataFrame(np.vstack(\n",
    "            (self._coords, X.clip(0, 1))).T).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False).mean()\n",
    "\n",
    "        img_matrices = []\n",
    "        blank_mat = np.zeros(self._pixels)\n",
    "        if empty_value != 0:\n",
    "            blank_mat[:] = empty_value\n",
    "        for z in range(2, img_coords.shape[1]):\n",
    "            img_matrix = blank_mat.copy()\n",
    "            img_matrix[img_coords[0].astype(int),\n",
    "                       img_coords[1].astype(int)] = img_coords[z]\n",
    "            img_matrices.append(img_matrix)\n",
    "\n",
    "        return img_matrices\n",
    "\n",
    "    def transform_3d(self, X, empty_value=0):\n",
    "        \"\"\"Transform the input matrix into image matrices\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "                where n_features matches the training set.\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        img_coords = pd.DataFrame(np.vstack(\n",
    "            (self._coords, X.clip(0, 1))).T).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False)\n",
    "        avg_img_coords = img_coords.mean()\n",
    "        min_img_coords = img_coords.min()\n",
    "        max_img_coords = img_coords.max()\n",
    "\n",
    "        img_matrices = []\n",
    "        blank_mat = np.zeros((3, self._pixels[0], self._pixels[1]))\n",
    "        if empty_value != 0:\n",
    "            blank_mat[:, :, :] = empty_value\n",
    "        for z in range(2, avg_img_coords.shape[1]):\n",
    "            img_matrix = blank_mat.copy()\n",
    "            img_matrix[0, avg_img_coords[0].astype(int),\n",
    "                       avg_img_coords[1].astype(int)] = avg_img_coords[z]\n",
    "            img_matrix[1, min_img_coords[0].astype(int),\n",
    "                       min_img_coords[1].astype(int)] = min_img_coords[z]\n",
    "            img_matrix[2, max_img_coords[0].astype(int),\n",
    "                       max_img_coords[1].astype(int)] = max_img_coords[z]\n",
    "            img_matrices.append(img_matrix)\n",
    "\n",
    "        return img_matrices\n",
    "\n",
    "    def fit_transform(self, X, empty_value=0):\n",
    "        \"\"\"Train the image transformer from the training set (X) and return\n",
    "        the transformed data.\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X, empty_value=empty_value)\n",
    "\n",
    "    def fit_transform_3d(self, X, empty_value=0):\n",
    "        \"\"\"Train the image transformer from the training set (X) and return\n",
    "        the transformed data.\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform_3d(X, empty_value=empty_value)\n",
    "\n",
    "    def feature_density_matrix(self):\n",
    "        \"\"\"Generate image matrix with feature counts per pixel\n",
    "\n",
    "        Returns:\n",
    "            img_matrix (ndarray): matrix with feature counts per pixel\n",
    "        \"\"\"\n",
    "        fdmat = np.zeros(self._pixels)\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        coord_cnt = (\n",
    "            pd.DataFrame(self._coords.T).assign(count=1).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False).count())\n",
    "        fdmat[coord_cnt[0].astype(int),\n",
    "              coord_cnt[1].astype(int)] = coord_cnt['count']\n",
    "        return fdmat\n",
    "\n",
    "    @staticmethod\n",
    "    def _minimum_bounding_rectangle(hull_points):\n",
    "        \"\"\"Find the smallest bounding rectangle for a set of points.\n",
    "\n",
    "        Modified from JesseBuesking at https://stackoverflow.com/a/33619018\n",
    "        Returns a set of points representing the corners of the bounding box.\n",
    "\n",
    "        Args:\n",
    "            hull_points : an nx2 matrix of hull coordinates\n",
    "\n",
    "        Returns:\n",
    "            (tuple): tuple containing\n",
    "                coords (ndarray): coordinates of the corners of the rectangle\n",
    "                rotmat (ndarray): rotation matrix to align edges of rectangle\n",
    "                    to x and y\n",
    "        \"\"\"\n",
    "\n",
    "        pi2 = np.pi / 2.\n",
    "\n",
    "        # Calculate edge angles\n",
    "        edges = hull_points[1:] - hull_points[:-1]\n",
    "        angles = np.arctan2(edges[:, 1], edges[:, 0])\n",
    "        angles = np.abs(np.mod(angles, pi2))\n",
    "        angles = np.unique(angles)\n",
    "\n",
    "        # Find rotation matrices\n",
    "        rotations = np.vstack([\n",
    "            np.cos(angles),\n",
    "            np.cos(angles - pi2),\n",
    "            np.cos(angles + pi2),\n",
    "            np.cos(angles)\n",
    "        ]).T\n",
    "        rotations = rotations.reshape((-1, 2, 2))\n",
    "\n",
    "        # Apply rotations to the hull\n",
    "        rot_points = np.dot(rotations, hull_points.T)\n",
    "\n",
    "        # Find the bounding points\n",
    "        min_x = np.nanmin(rot_points[:, 0], axis=1)\n",
    "        max_x = np.nanmax(rot_points[:, 0], axis=1)\n",
    "        min_y = np.nanmin(rot_points[:, 1], axis=1)\n",
    "        max_y = np.nanmax(rot_points[:, 1], axis=1)\n",
    "\n",
    "        # Find the box with the best area\n",
    "        areas = (max_x - min_x) * (max_y - min_y)\n",
    "        best_idx = np.argmin(areas)\n",
    "\n",
    "        # Return the best box\n",
    "        x1 = max_x[best_idx]\n",
    "        x2 = min_x[best_idx]\n",
    "        y1 = max_y[best_idx]\n",
    "        y2 = min_y[best_idx]\n",
    "        rotmat = rotations[best_idx]\n",
    "\n",
    "        # Generate coordinates\n",
    "        coords = np.zeros((4, 2))\n",
    "        coords[0] = np.dot([x1, y2], rotmat)\n",
    "        coords[1] = np.dot([x2, y2], rotmat)\n",
    "        coords[2] = np.dot([x2, y1], rotmat)\n",
    "        coords[3] = np.dot([x1, y1], rotmat)\n",
    "\n",
    "        return coords, rotmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LogScaler:\n",
    "    \"\"\"Log normalize and scale data\n",
    "\n",
    "    Log normalization and scaling procedure as described as norm-2 in the\n",
    "    DeepInsight paper supplementary information.\n",
    "    \n",
    "    Note: The dimensions of input matrix is (N samples, d features)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._min0 = None\n",
    "        self._max = None\n",
    "\n",
    "    \"\"\"\n",
    "    Use this as a preprocessing step in inference mode.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Min. of training set per feature\n",
    "        self._min0 = X.min(axis=0)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(min=0, max=None)\n",
    "\n",
    "        # Global max. of training set from X_norm\n",
    "        self._max = X_norm.max()\n",
    "\n",
    "    \"\"\"\n",
    "    For training set only.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        # Min. of training set per feature\n",
    "        self._min0 = X.min(axis=0)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(min=0, max=None)\n",
    "\n",
    "        # Global max. of training set from X_norm\n",
    "        self._max = X_norm.max()\n",
    "\n",
    "        # Normalized again by global max. of training set\n",
    "        return (X_norm / self._max).clip(0, 1)\n",
    "\n",
    "    \"\"\"\n",
    "    For validation and test set only.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Adjust min. of each feature of X by _min0\n",
    "        for i in range(X.shape[1]):\n",
    "            X[:, i] = X[:, i].clip(min=self._min0[i], max=None)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(min=0, max=None)\n",
    "\n",
    "        # Normalized again by global max. of training set\n",
    "        return (X_norm / self._max).clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoAImageSwapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 features,\n",
    "                 labels,\n",
    "                 transformer,\n",
    "                 swap_prob=0.15,\n",
    "                 swap_portion=0.1):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transformer = transformer\n",
    "        self.swap_prob = swap_prob\n",
    "        self.swap_portion = swap_portion\n",
    "\n",
    "        # self.crop = CropToFixedSize(width=image_size, height=image_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "\n",
    "        # Swap row featurs randomly\n",
    "        normalized = self.add_swap_noise(index, normalized)\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=0\n",
    "        image = self.transformer.transform_3d(normalized, empty_value=0)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image.transpose((1, 2, 0)),\n",
    "                           (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "\n",
    "        return {\"x\": image, \"y\": self.labels[index, :]}\n",
    "\n",
    "    def add_swap_noise(self, index, X):\n",
    "        if np.random.rand() < self.swap_prob:\n",
    "            swap_index = np.random.randint(self.features.shape[0], size=1)[0]\n",
    "            # Select only gene expression and cell viability features\n",
    "            swap_features = np.random.choice(\n",
    "                np.array(range(3, self.features.shape[1])),\n",
    "                size=int(self.features.shape[1] * self.swap_portion),\n",
    "                replace=False)\n",
    "            X[swap_features] = self.features[swap_index, swap_features]\n",
    "\n",
    "        return X\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoAImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, transformer):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=0\n",
    "        image = self.transformer.transform_3d(normalized, empty_value=0)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image.transpose((1, 2, 0)),\n",
    "                           (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "\n",
    "        return {\"x\": image, \"y\": self.labels[index, :]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, transformer):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=0\n",
    "        image = self.transformer.transform_3d(normalized, empty_value=0)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image.transpose((1, 2, 0)),\n",
    "                           (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "\n",
    "        return {\"x\": image, \"y\": -1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/vbmokin/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual#4.7-Smoothing\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets: torch.Tensor, n_labels: int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "                                           self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight)\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layer):\n",
    "    for m in layer.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1.0)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            fan_out = m.weight.size(0)  # fan-out\n",
    "            fan_in = 0\n",
    "            init_range = 1.0 / math.sqrt(fan_in + fan_out)\n",
    "            m.weight.data.uniform_(-init_range, init_range)\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoAResNeSt(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_model_name,\n",
    "            training_set=(None, None),  # tuple\n",
    "            valid_set=(None, None),  # tuple\n",
    "            test_set=None,\n",
    "            transformer=None,\n",
    "            num_classes=206,\n",
    "            final_drop=0.0,\n",
    "            dropblock_prob=0,\n",
    "            fc_size=512,\n",
    "            learning_rate=1e-3):\n",
    "        super(MoAResNeSt, self).__init__()\n",
    "\n",
    "        self.train_data, self.train_labels = training_set\n",
    "        self.valid_data, self.valid_labels = valid_set\n",
    "        self.test_data = test_set\n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.backbone = getattr(resnest.torch, pretrained_model)(\n",
    "            pretrained=True,\n",
    "            final_drop=final_drop,\n",
    "            dropblock_prob=dropblock_prob)\n",
    "\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(self.backbone.fc.in_features, fc_size, bias=True),\n",
    "            nn.ELU(), nn.Linear(fc_size, num_classes, bias=True))\n",
    "\n",
    "        if self.training:\n",
    "            initialize_weights(self.backbone.fc)\n",
    "\n",
    "        # Save passed hyperparameters\n",
    "        self.save_hyperparameters(\"pretrained_model_name\", \"num_classes\",\n",
    "                                  \"final_drop\", \"dropblock_prob\", \"fc_size\",\n",
    "                                  \"learning_rate\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        x = x.float()\n",
    "        y = y.type_as(x)\n",
    "        logits = self(x)\n",
    "\n",
    "        # loss = F.binary_cross_entropy_with_logits(logits, y, reduction=\"mean\")\n",
    "\n",
    "        # Label smoothing\n",
    "        loss = SmoothBCEwLogits(smoothing=label_smoothing)(logits, y)\n",
    "\n",
    "        self.log('train_loss',\n",
    "                 loss,\n",
    "                 on_step=True,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        x = x.float()\n",
    "        y = y.type_as(x)\n",
    "        logits = self(x)\n",
    "\n",
    "        val_loss = F.binary_cross_entropy_with_logits(logits,\n",
    "                                                      y,\n",
    "                                                      reduction=\"mean\")\n",
    "\n",
    "        self.log('val_loss',\n",
    "                 val_loss,\n",
    "                 on_step=True,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 logger=True)\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        x = x.float()\n",
    "        y = y.type_as(x)\n",
    "        logits = self(x)\n",
    "        return {\"pred_logits\": logits}\n",
    "\n",
    "    def test_epoch_end(self, output_results):\n",
    "        all_outputs = torch.cat([out[\"pred_logits\"] for out in output_results],\n",
    "                                dim=0)\n",
    "        print(\"Logits:\", all_outputs)\n",
    "        pred_probs = F.sigmoid(all_outputs).detach().cpu().numpy()\n",
    "        print(\"Predictions: \", pred_probs)\n",
    "        return {\"pred_probs\": pred_probs}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        #         self.train_dataset = MoAImageDataset(self.train_data,\n",
    "        #                                              self.train_labels,\n",
    "        #                                              self.transformer)\n",
    "        self.train_dataset = MoAImageSwapDataset(self.train_data,\n",
    "                                                 self.train_labels,\n",
    "                                                 self.transformer,\n",
    "                                                 swap_prob=swap_prob,\n",
    "                                                 swap_portion=swap_portion)\n",
    "\n",
    "        self.val_dataset = MoAImageDataset(self.valid_data, self.valid_labels,\n",
    "                                           self.transformer)\n",
    "\n",
    "        self.test_dataset = TestDataset(self.test_data, None, self.transformer)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = DataLoader(self.train_dataset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=num_workers,\n",
    "                                      pin_memory=True,\n",
    "                                      drop_last=False)\n",
    "        print(f\"Train iterations: {len(train_dataloader)}\")\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = DataLoader(self.val_dataset,\n",
    "                                    batch_size=infer_batch_size,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=num_workers,\n",
    "                                    pin_memory=True,\n",
    "                                    drop_last=False)\n",
    "        print(f\"Validate iterations: {len(val_dataloader)}\")\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataloader = DataLoader(self.test_dataset,\n",
    "                                     batch_size=infer_batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=num_workers,\n",
    "                                     pin_memory=True,\n",
    "                                     drop_last=False)\n",
    "        print(f\"Test iterations: {len(test_dataloader)}\")\n",
    "        return test_dataloader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print(f\"Initial Learning Rate: {self.hparams.learning_rate:.6f}\")\n",
    "        #         optimizer = optim.Adam(self.parameters(),\n",
    "        #                                lr=self.hparams.learning_rate,\n",
    "        #                                weight_decay=weight_decay)\n",
    "        #         optimizer = torch.optim.SGD(self.parameters(),\n",
    "        #                                     lr=self.hparams.learning_rate,\n",
    "        #                                     momentum=0.9,\n",
    "        #                                     dampening=0,\n",
    "        #                                     weight_decay=weight_decay,\n",
    "        #                                     nesterov=False)\n",
    "\n",
    "        optimizer = torch_optimizer.RAdam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                         T_max=T_max,\n",
    "                                                         eta_min=0,\n",
    "                                                         last_epoch=-1)\n",
    "\n",
    "        #         scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        #             optimizer,\n",
    "        #             T_0=T_0,\n",
    "        #             T_mult=1,\n",
    "        #             eta_min=0,\n",
    "        #             last_epoch=-1)\n",
    "\n",
    "        #         scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        #             optimizer=optimizer,\n",
    "        #             pct_start=0.1,\n",
    "        #             div_factor=1e3,\n",
    "        #             max_lr=1e-1,\n",
    "        #             # max_lr=1e-2,\n",
    "        #             epochs=epochs,\n",
    "        #             steps_per_epoch=len(self.train_images) // batch_size)\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = MoAResNeSt(\n",
    "#     pretrained_model,\n",
    "#     training_set=(None, None),  # tuple\n",
    "#     valid_set=(None, None),  # tuple\n",
    "#     test_set=None,\n",
    "#     transformer=None,\n",
    "#     num_classes=206,\n",
    "#     final_drop=0.0,\n",
    "#     dropblock_prob=0,\n",
    "#     fc_size=fc_size,\n",
    "#     learning_rate=learning_rate)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = 10\n",
    "skf = MultilabelStratifiedKFold(n_splits=kfolds,\n",
    "                                shuffle=True,\n",
    "                                random_state=rand_seed)\n",
    "\n",
    "label_counts = np.sum(train_labels.drop(\"sig_id\", axis=1), axis=0)\n",
    "y_labels = label_counts.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(training_set, valid_set, test_set, transformer, model_path=None):\n",
    "    if training_mode:\n",
    "        model = MoAResNeSt(\n",
    "            pretrained_model_name=pretrained_model,\n",
    "            training_set=training_set,  # tuple\n",
    "            valid_set=valid_set,  # tuple\n",
    "            test_set=test_set,\n",
    "            transformer=transformer,\n",
    "            num_classes=len(train_classes),\n",
    "            final_drop=final_drop,\n",
    "            dropblock_prob=dropblock_prob,\n",
    "            fc_size=fc_size,\n",
    "            learning_rate=learning_rate)\n",
    "    else:\n",
    "        model = MoAResNeSt.load_from_checkpoint(\n",
    "            model_path,\n",
    "            pretrained_model_name=pretrained_model,\n",
    "            training_set=training_set,  # tuple\n",
    "            valid_set=valid_set,  # tuple\n",
    "            test_set=test_set,\n",
    "            transformer=transformer,\n",
    "            num_classes=len(train_classes),\n",
    "            fc_size=fc_size)\n",
    "        model.freeze()\n",
    "        model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_pickle(obj, model_output_folder, fold_i, name):\n",
    "    dump(obj, open(f\"{model_output_folder}/fold{fold_i}_{name}.pkl\", 'wb'),\n",
    "         pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_pickle(model_output_folder, fold_i, name):\n",
    "    return load(open(f\"{model_output_folder}/fold{fold_i}_{name}.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm2_normalization(train, valid, test):\n",
    "    scaler = LogScaler()\n",
    "    train = scaler.fit_transform(train)\n",
    "    valid = scaler.transform(valid)\n",
    "    test = scaler.transform(test)\n",
    "    return train, valid, test, scaler\n",
    "\n",
    "\n",
    "def quantile_transform(train, valid, test):\n",
    "    q_scaler = QuantileTransformer(n_quantiles=1000,\n",
    "                                   output_distribution='normal',\n",
    "                                   ignore_implicit_zeros=False,\n",
    "                                   subsample=100000,\n",
    "                                   random_state=rand_seed)\n",
    "    train = q_scaler.fit_transform(train)\n",
    "    valid = q_scaler.transform(valid)\n",
    "    test = q_scaler.transform(test)\n",
    "\n",
    "    # Transform to [0, 1]\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train = min_max_scaler.fit_transform(train)\n",
    "    valid = min_max_scaler.transform(valid)\n",
    "    test = min_max_scaler.transform(test)\n",
    "\n",
    "    return train, valid, test, q_scaler, min_max_scaler\n",
    "\n",
    "\n",
    "def extract_feature_map(train,\n",
    "                        feature_extractor='tsne_exact',\n",
    "                        resolution=100,\n",
    "                        perplexity=30):\n",
    "    transformer = DeepInsightTransformer(feature_extractor=feature_extractor,\n",
    "                                         pixels=resolution,\n",
    "                                         perplexity=perplexity,\n",
    "                                         random_state=rand_seed,\n",
    "                                         n_jobs=-1)\n",
    "    transformer.fit(train)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_logloss(y_pred, y_true):\n",
    "    logloss = (1 - y_true) * np.log(1 - y_pred +\n",
    "                                    1e-15) + y_true * np.log(y_pred + 1e-15)\n",
    "    return np.mean(-logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing on Fold 0 ......\n",
      "(21432,) (2382,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v1_resnest50/deepinsight_ResNeSt_v1_resnest50/fold0/epoch25-train_loss_epoch0.016572-val_loss_epoch0.014525-image_size224-resolution224-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a282933fed84f5fb38ccd8151bf0ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ -7.0039,  -7.3164,  -6.5586,  ...,  -6.7266,  -6.0352,  -5.7812],\n",
      "        [-10.5312,  -8.0078,  -8.2500,  ...,  -7.3086, -10.0625,  -5.7656],\n",
      "        [ -7.7461,  -7.5820,  -7.5547,  ...,  -7.6875,  -8.0078,  -7.4961],\n",
      "        ...,\n",
      "        [ -5.9141,  -6.7227,  -7.0273,  ...,  -6.9570,  -7.1562,  -6.5312],\n",
      "        [ -8.0312,  -8.3047,  -7.3945,  ...,  -5.9414,  -6.8555,  -5.4180],\n",
      "        [ -6.7070,  -6.8086,  -6.9805,  ...,  -6.5234,  -6.7930,  -6.4648]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[9.0742e-04 6.6423e-04 1.4162e-03 ... 1.1969e-03 2.3880e-03 3.0746e-03]\n",
      " [2.6703e-05 3.3283e-04 2.6131e-04 ... 6.6948e-04 4.2677e-05 3.1242e-03]\n",
      " [4.3225e-04 5.0926e-04 5.2357e-04 ... 4.5824e-04 3.3283e-04 5.5504e-04]\n",
      " ...\n",
      " [2.6932e-03 1.2016e-03 8.8644e-04 ... 9.5081e-04 7.7915e-04 1.4553e-03]\n",
      " [3.2496e-04 2.4724e-04 6.1417e-04 ... 2.6207e-03 1.0529e-03 4.4174e-03]\n",
      " [1.2207e-03 1.1034e-03 9.2888e-04 ... 1.4668e-03 1.1206e-03 1.5545e-03]]\n",
      "\n",
      "Inferencing on Fold 1 ......\n",
      "(21432,) (2382,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v1_resnest50/deepinsight_ResNeSt_v1_resnest50/fold1/epoch25-train_loss_epoch0.016363-val_loss_epoch0.014738-image_size224-resolution224-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd2c9221eee4696a7b41809ba3a8a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ -7.4414,  -7.2227,  -6.3516,  ...,  -5.8867,  -6.8594,  -6.2539],\n",
      "        [ -8.6172,  -9.7656, -10.9844,  ...,  -9.2188, -10.5703,  -7.8320],\n",
      "        [ -7.8672,  -7.7617,  -7.4219,  ...,  -7.5977,  -8.0391,  -7.3984],\n",
      "        ...,\n",
      "        [ -5.7422,  -5.6289,  -6.1523,  ...,  -6.1641,  -7.7188,  -6.4805],\n",
      "        [ -6.5625,  -8.1875,  -6.9102,  ...,  -5.4922,  -7.3398,  -5.6992],\n",
      "        [ -7.0508,  -7.6367,  -6.3672,  ...,  -6.6523,  -7.2539,  -6.1992]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[5.8603e-04 7.2956e-04 1.7414e-03 ... 2.7676e-03 1.0481e-03 1.9188e-03]\n",
      " [1.8096e-04 5.7399e-05 1.6987e-05 ... 9.9182e-05 2.5690e-05 3.9673e-04]\n",
      " [3.8290e-04 4.2558e-04 5.9748e-04 ... 5.0116e-04 3.2258e-04 6.1178e-04]\n",
      " ...\n",
      " [3.1967e-03 3.5801e-03 2.1248e-03 ... 2.1000e-03 4.4417e-04 1.5306e-03]\n",
      " [1.4105e-03 2.7800e-04 9.9659e-04 ... 4.1008e-03 6.4850e-04 3.3379e-03]\n",
      " [8.6594e-04 4.8208e-04 1.7138e-03 ... 1.2894e-03 7.0667e-04 2.0275e-03]]\n",
      "\n",
      "Inferencing on Fold 2 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v1_resnest50/deepinsight_ResNeSt_v1_resnest50/fold2/epoch35-train_loss_epoch0.015017-val_loss_epoch0.014540-image_size224-resolution224-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540ca1182c5b47d3b9890430ea1f0c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ -7.9492,  -6.3945,  -6.1484,  ...,  -7.1133,  -7.0117,  -7.9375],\n",
      "        [-10.0547, -10.2422,  -7.2891,  ...,  -9.3984,  -7.9531,  -4.6055],\n",
      "        [ -7.9180,  -8.0078,  -7.4844,  ...,  -7.7578,  -7.6328,  -7.0156],\n",
      "        ...,\n",
      "        [ -5.3945,  -6.2188,  -7.2266,  ...,  -6.7734,  -8.2578,  -7.1797],\n",
      "        [ -7.0273,  -7.3398,  -7.4609,  ...,  -6.2852,  -7.6484,  -6.4688],\n",
      "        [ -6.7344,  -8.0391,  -5.3359,  ...,  -5.6719,  -8.3984,  -7.2383]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[3.5286e-04 1.6680e-03 2.1324e-03 ... 8.1348e-04 9.0027e-04 3.5691e-04]\n",
      " [4.2975e-05 3.5644e-05 6.8235e-04 ... 8.2850e-05 3.5143e-04 9.8953e-03]\n",
      " [3.6407e-04 3.3283e-04 5.6171e-04 ... 4.2725e-04 4.8399e-04 8.9693e-04]\n",
      " ...\n",
      " [4.5204e-03 1.9875e-03 7.2670e-04 ... 1.1425e-03 2.5916e-04 7.6151e-04]\n",
      " [8.8644e-04 6.4850e-04 5.7459e-04 ... 1.8606e-03 4.7660e-04 1.5488e-03]\n",
      " [1.1883e-03 3.2258e-04 4.7913e-03 ... 3.4294e-03 2.2519e-04 7.1812e-04]]\n",
      "\n",
      "Inferencing on Fold 3 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v1_resnest50/deepinsight_ResNeSt_v1_resnest50/fold3/epoch25-train_loss_epoch0.016549-val_loss_epoch0.014762-image_size224-resolution224-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f6e4da1d934b26a2bba8d3e78fe6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.1602, -6.9297, -6.9922,  ..., -6.4648, -5.3828, -6.5391],\n",
      "        [-9.7734, -8.8984, -7.3281,  ..., -9.3750, -8.9062, -7.8359],\n",
      "        [-7.5625, -7.8242, -7.1055,  ..., -8.0469, -7.6797, -7.7461],\n",
      "        ...,\n",
      "        [-5.0195, -5.9922, -6.9844,  ..., -6.5859, -7.4648, -6.6016],\n",
      "        [-6.9219, -6.9180, -7.2891,  ..., -6.0078, -7.4648, -5.9922],\n",
      "        [-5.9570, -7.2734, -6.6484,  ..., -5.7852, -7.2266, -6.7773]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[2.1076e-03 9.7752e-04 9.1839e-04 ... 1.5545e-03 4.5738e-03 1.4439e-03]\n",
      " [5.6922e-05 1.3661e-04 6.5660e-04 ... 8.4817e-05 1.3554e-04 3.9506e-04]\n",
      " [5.1928e-04 3.9983e-04 8.2016e-04 ... 3.1996e-04 4.6182e-04 4.3225e-04]\n",
      " ...\n",
      " [6.5651e-03 2.4929e-03 9.2554e-04 ... 1.3781e-03 5.7268e-04 1.3561e-03]\n",
      " [9.8515e-04 9.8896e-04 6.8235e-04 ... 2.4529e-03 5.7268e-04 2.4929e-03]\n",
      " [2.5806e-03 6.9332e-04 1.2941e-03 ... 3.0632e-03 7.2670e-04 1.1377e-03]]\n",
      "\n",
      "Inferencing on Fold 4 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v1_resnest50/deepinsight_ResNeSt_v1_resnest50/fold4/epoch25-train_loss_epoch0.016320-val_loss_epoch0.014554-image_size224-resolution224-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8ec6af6b3943a0a7473fda4e4303f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.9297, -7.2266, -6.6328,  ..., -6.0234, -7.5859, -6.7539],\n",
      "        [-9.2812, -7.1367, -7.2500,  ..., -7.0938, -8.1875, -6.6367],\n",
      "        [-7.4727, -7.5586, -7.4414,  ..., -7.2109, -7.6445, -7.3164],\n",
      "        ...,\n",
      "        [-4.8672, -5.8320, -6.5898,  ..., -6.1484, -7.5391, -6.2266],\n",
      "        [-6.5312, -6.8828, -6.8711,  ..., -5.3789, -8.4219, -6.7070],\n",
      "        [-6.9453, -7.0273, -6.4766,  ..., -6.5938, -6.9219, -6.2930]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[9.775e-04 7.267e-04 1.315e-03 ... 2.415e-03 5.074e-04 1.165e-03]\n",
      " [9.316e-05 7.949e-04 7.095e-04 ... 8.297e-04 2.780e-04 1.309e-03]\n",
      " [5.679e-04 5.212e-04 5.860e-04 ... 7.381e-04 4.785e-04 6.642e-04]\n",
      " ...\n",
      " [7.637e-03 2.924e-03 1.372e-03 ... 2.132e-03 5.317e-04 1.972e-03]\n",
      " [1.455e-03 1.024e-03 1.037e-03 ... 4.593e-03 2.199e-04 1.221e-03]\n",
      " [9.623e-04 8.864e-04 1.536e-03 ... 1.367e-03 9.851e-04 1.846e-03]]\n",
      "\n",
      "Inferencing on Fold 5 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v1_resnest50/deepinsight_ResNeSt_v1_resnest50/fold5/epoch25-train_loss_epoch0.016142-val_loss_epoch0.014599-image_size224-resolution224-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324a8ed9cd23482080d1e835997ce828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-7.5156, -6.6094, -6.1016,  ..., -6.6055, -5.6953, -6.1602],\n",
      "        [-8.2031, -9.0078, -8.7109,  ..., -8.6094, -9.7188, -7.5508],\n",
      "        [-7.5117, -7.5625, -7.2070,  ..., -7.6992, -7.6680, -7.4688],\n",
      "        ...,\n",
      "        [-5.1680, -5.4453, -7.6836,  ..., -7.3086, -8.1719, -6.2969],\n",
      "        [-6.2930, -6.7188, -7.0078,  ..., -6.3281, -6.9883, -6.4062],\n",
      "        [-6.2734, -7.4570, -6.4258,  ..., -6.9883, -8.2344, -6.8125]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[5.4407e-04 1.3456e-03 2.2335e-03 ... 1.3514e-03 3.3512e-03 2.1076e-03]\n",
      " [2.7370e-04 1.2243e-04 1.6475e-04 ... 1.8239e-04 6.0141e-05 5.2547e-04]\n",
      " [5.4646e-04 5.1928e-04 7.4100e-04 ... 4.5300e-04 4.6730e-04 5.7030e-04]\n",
      " ...\n",
      " [5.6648e-03 4.2992e-03 4.6015e-04 ... 6.6948e-04 2.8253e-04 1.8387e-03]\n",
      " [1.8463e-03 1.2064e-03 9.0408e-04 ... 1.7824e-03 9.2173e-04 1.6489e-03]\n",
      " [1.8826e-03 5.7697e-04 1.6165e-03 ... 9.2173e-04 2.6536e-04 1.0986e-03]]\n",
      "\n",
      "Inferencing on Fold 6 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v1_resnest50/deepinsight_ResNeSt_v1_resnest50/fold6/epoch25-train_loss_epoch0.016261-val_loss_epoch0.014526-image_size224-resolution224-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2158749d99804021aa3a1bb2a5f791f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ -5.5781,  -6.2656,  -7.3398,  ...,  -6.5664,  -6.4883,  -6.8594],\n",
      "        [ -8.4453,  -7.6953,  -7.5938,  ...,  -8.1172, -10.2344,  -6.9258],\n",
      "        [ -7.5938,  -7.3008,  -7.1250,  ...,  -7.3516,  -7.0000,  -7.3281],\n",
      "        ...,\n",
      "        [ -5.9219,  -5.8359,  -7.0898,  ...,  -7.2109,  -7.7617,  -6.4219],\n",
      "        [ -5.3906,  -6.5820,  -7.8359,  ...,  -6.9766,  -8.1562,  -6.1953],\n",
      "        [ -5.9492,  -6.7617,  -6.7578,  ...,  -5.8633,  -7.8125,  -6.9570]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[3.765e-03 1.897e-03 6.485e-04 ... 1.405e-03 1.519e-03 1.048e-03]\n",
      " [2.148e-04 4.547e-04 5.035e-04 ... 2.983e-04 3.594e-05 9.813e-04]\n",
      " [5.035e-04 6.747e-04 8.039e-04 ... 6.413e-04 9.112e-04 6.566e-04]\n",
      " ...\n",
      " [2.672e-03 2.913e-03 8.330e-04 ... 7.381e-04 4.256e-04 1.623e-03]\n",
      " [4.539e-03 1.383e-03 3.951e-04 ... 9.327e-04 2.868e-04 2.035e-03]\n",
      " [2.602e-03 1.156e-03 1.161e-03 ... 2.834e-03 4.046e-04 9.508e-04]]\n",
      "\n",
      "Inferencing on Fold 7 ......\n",
      "(21432,) (2382,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v1_resnest50/deepinsight_ResNeSt_v1_resnest50/fold7/epoch25-train_loss_epoch0.016051-val_loss_epoch0.014633-image_size224-resolution224-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e780b0ea59434fbe80e236152db547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.8672, -6.6602, -6.3203,  ..., -5.8281, -8.5000, -7.1680],\n",
      "        [-9.2891, -7.6523, -6.8594,  ..., -7.9570, -9.0469, -6.6172],\n",
      "        [-7.7773, -7.2852, -7.1680,  ..., -7.2070, -7.7227, -7.2109],\n",
      "        ...,\n",
      "        [-5.1484, -6.1055, -7.2109,  ..., -6.7891, -7.3906, -6.3438],\n",
      "        [-5.6172, -6.5117, -7.3750,  ..., -6.0312, -8.0312, -6.7070],\n",
      "        [-6.3047, -6.9609, -6.4336,  ..., -6.1055, -8.7422, -6.5117]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[1.040e-03 1.279e-03 1.796e-03 ... 2.935e-03 2.034e-04 7.701e-04]\n",
      " [9.245e-05 4.747e-04 1.048e-03 ... 3.500e-04 1.177e-04 1.335e-03]\n",
      " [4.189e-04 6.852e-04 7.701e-04 ... 7.410e-04 4.425e-04 7.381e-04]\n",
      " ...\n",
      " [5.775e-03 2.226e-03 7.381e-04 ... 1.124e-03 6.166e-04 1.755e-03]\n",
      " [3.622e-03 1.484e-03 6.266e-04 ... 2.398e-03 3.250e-04 1.221e-03]\n",
      " [1.824e-03 9.475e-04 1.604e-03 ... 2.226e-03 1.596e-04 1.484e-03]]\n",
      "\n",
      "Inferencing on Fold 8 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v1_resnest50/deepinsight_ResNeSt_v1_resnest50/fold8/epoch25-train_loss_epoch0.016327-val_loss_epoch0.014581-image_size224-resolution224-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6877b512cdc241fca0ecbdd50eb64318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.1211, -6.9922, -6.3789,  ..., -6.2188, -7.2656, -5.6289],\n",
      "        [-8.2344, -7.0820, -6.4219,  ..., -7.5430, -9.5391, -7.0742],\n",
      "        [-7.4531, -7.5977, -7.2891,  ..., -7.3477, -8.0781, -7.3125],\n",
      "        ...,\n",
      "        [-4.9023, -5.7070, -7.0391,  ..., -6.7305, -7.9180, -6.3750],\n",
      "        [-7.2969, -8.1406, -6.5898,  ..., -6.4844, -6.6719, -4.8789],\n",
      "        [-5.8555, -6.7188, -7.0547,  ..., -6.6797, -8.1250, -6.5156]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[2.192e-03 9.184e-04 1.694e-03 ... 1.987e-03 6.986e-04 3.580e-03]\n",
      " [2.654e-04 8.392e-04 1.623e-03 ... 5.298e-04 7.200e-05 8.459e-04]\n",
      " [5.794e-04 5.012e-04 6.824e-04 ... 6.437e-04 3.102e-04 6.666e-04]\n",
      " ...\n",
      " [7.374e-03 3.311e-03 8.759e-04 ... 1.192e-03 3.641e-04 1.700e-03]\n",
      " [6.771e-04 2.913e-04 1.372e-03 ... 1.525e-03 1.265e-03 7.549e-03]\n",
      " [2.855e-03 1.206e-03 8.626e-04 ... 1.255e-03 2.959e-04 1.478e-03]]\n",
      "\n",
      "Inferencing on Fold 9 ......\n",
      "(21432,) (2382,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v1_resnest50/deepinsight_ResNeSt_v1_resnest50/fold9/epoch25-train_loss_epoch0.016287-val_loss_epoch0.014737-image_size224-resolution224-perplexity5-fc512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f09af644d734498a4f38397880d2f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-7.0664, -7.1445, -5.7461,  ..., -5.2891, -6.7773, -6.2773],\n",
      "        [-8.1953, -7.6562, -7.0664,  ..., -6.7383, -7.0820, -6.1250],\n",
      "        [-7.5742, -7.5352, -7.0391,  ..., -7.3867, -7.5977, -7.0703],\n",
      "        ...,\n",
      "        [-5.9766, -5.7969, -7.2930,  ..., -6.6562, -8.3438, -6.1680],\n",
      "        [-6.3867, -7.5547, -7.1992,  ..., -6.8477, -7.1641, -6.2344],\n",
      "        [-7.1992, -7.4883, -6.6133,  ..., -6.0469, -6.5742, -5.8750]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[0.0008526 0.0007887 0.003185  ... 0.00502   0.001138  0.001875 ]\n",
      " [0.0002759 0.0004728 0.0008526 ... 0.0011835 0.000839  0.002182 ]\n",
      " [0.000513  0.0005336 0.000876  ... 0.000619  0.000501  0.0008492]\n",
      " ...\n",
      " [0.002531  0.003027  0.00068   ... 0.001285  0.0002378 0.00209  ]\n",
      " [0.001681  0.0005236 0.0007467 ... 0.0010605 0.0007734 0.001957 ]\n",
      " [0.0007467 0.0005593 0.001341  ... 0.00236   0.001394  0.0028   ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure Reproducibility\n",
    "seed_everything(rand_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "best_model = None\n",
    "oof_predictions = np.zeros((train_features.shape[0], len(train_classes)))\n",
    "kfold_submit_preds = np.zeros((test_features.shape[0], len(train_classes)))\n",
    "for i, (train_index, val_index) in enumerate(\n",
    "        skf.split(train_features, train_labels[y_labels])):\n",
    "    if training_mode:\n",
    "        print(f\"Training on Fold {i} ......\")\n",
    "        print(train_index.shape, val_index.shape)\n",
    "\n",
    "        logger = TensorBoardLogger(model_output_folder,\n",
    "                                   name=f\"fold{i}/logs\",\n",
    "                                   default_hp_metric=False)\n",
    "\n",
    "        train = train_features.loc[train_index, all_features].copy().values\n",
    "        fold_train_labels = train_labels.loc[train_index,\n",
    "                                             train_classes].copy().values\n",
    "        valid = train_features.loc[val_index, all_features].copy().values\n",
    "        fold_valid_labels = train_labels.loc[val_index,\n",
    "                                             train_classes].copy().values\n",
    "        test = test_features[all_features].copy().values\n",
    "\n",
    "        # LogScaler (Norm-2 Normalization)\n",
    "        print(\"Running norm-2 normalization ......\")\n",
    "        train, valid, test, scaler = norm2_normalization(train, valid, test)\n",
    "        save_pickle(scaler, model_output_folder, i, \"log-scaler\")\n",
    "\n",
    "        # Extract DeepInsight Feature Map\n",
    "        print(\"Extracting feature map ......\")\n",
    "        transformer = extract_feature_map(train,\n",
    "                                          feature_extractor='tsne_exact',\n",
    "                                          resolution=resolution,\n",
    "                                          perplexity=perplexity)\n",
    "        save_pickle(transformer, model_output_folder, i,\n",
    "                    \"deepinsight-transform\")\n",
    "\n",
    "        model = get_model(training_set=(train, fold_train_labels),\n",
    "                          valid_set=(valid, fold_valid_labels),\n",
    "                          test_set=test,\n",
    "                          transformer=transformer)\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss_epoch',\n",
    "                          min_delta=1e-6,\n",
    "                          patience=patience,\n",
    "                          verbose=True,\n",
    "                          mode='min',\n",
    "                          strict=True),\n",
    "            LearningRateMonitor(logging_interval='step')\n",
    "        ]\n",
    "        # https://pytorch-lightning.readthedocs.io/en/latest/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            filepath=f\"{model_output_folder}/fold{i}\" +\n",
    "            \"/{epoch}-{train_loss_epoch:.6f}-{val_loss_epoch:.6f}\" +\n",
    "            f\"-image_size={image_size}-resolution={resolution}-perplexity={perplexity}-fc={fc_size}\",\n",
    "            save_top_k=1,\n",
    "            save_weights_only=False,\n",
    "            save_last=False,\n",
    "            verbose=True,\n",
    "            monitor='val_loss_epoch',\n",
    "            mode='min',\n",
    "            prefix='')\n",
    "\n",
    "        if debug_mode:\n",
    "            # Find best LR\n",
    "            # https://pytorch-lightning.readthedocs.io/en/latest/lr_finder.html\n",
    "            trainer = Trainer(\n",
    "                gpus=[gpus[0]],\n",
    "                distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n",
    "                auto_lr_find=True,\n",
    "                benchmark=False,\n",
    "                deterministic=True,\n",
    "                logger=logger,\n",
    "                accumulate_grad_batches=accumulate_grad_batches,\n",
    "                gradient_clip_val=gradient_clip_val,\n",
    "                precision=16,\n",
    "                max_epochs=1)\n",
    "\n",
    "            # Run learning rate finder\n",
    "            lr_finder = trainer.tuner.lr_find(\n",
    "                model,\n",
    "                min_lr=1e-7,\n",
    "                max_lr=1e2,\n",
    "                num_training=100,\n",
    "                mode='exponential',\n",
    "                early_stop_threshold=100.0,\n",
    "            )\n",
    "            fig = lr_finder.plot(suggest=True)\n",
    "            fig.show()\n",
    "\n",
    "            # Pick point based on plot, or get suggestion\n",
    "            suggested_lr = lr_finder.suggestion()\n",
    "\n",
    "            # Update hparams of the model\n",
    "            model.hparams.learning_rate = suggested_lr\n",
    "            print(\n",
    "                f\"Suggested Learning Rate: {model.hparams.learning_rate:.6f}\")\n",
    "\n",
    "        else:\n",
    "            trainer = Trainer(\n",
    "                gpus=gpus,\n",
    "                distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n",
    "                max_epochs=epochs,\n",
    "                benchmark=False,\n",
    "                deterministic=True,\n",
    "                # fast_dev_run=True,\n",
    "                checkpoint_callback=checkpoint_callback,\n",
    "                callbacks=callbacks,\n",
    "                accumulate_grad_batches=accumulate_grad_batches,\n",
    "                gradient_clip_val=gradient_clip_val,\n",
    "                precision=16,\n",
    "                logger=logger)\n",
    "            trainer.fit(model)\n",
    "\n",
    "            # Load best model\n",
    "            seed_everything(rand_seed)\n",
    "            best_model = MoAResNeSt.load_from_checkpoint(\n",
    "                checkpoint_callback.best_model_path,\n",
    "                pretrained_model_name=pretrained_model,\n",
    "                training_set=(train, fold_train_labels),  # tuple\n",
    "                valid_Set=(valid, fold_valid_labels),  # tuple\n",
    "                test_set=test,\n",
    "                transformer=transformer,\n",
    "                fc_size=fc_size)\n",
    "            best_model.freeze()\n",
    "\n",
    "            print(\"Predicting on validation set ......\")\n",
    "            output = trainer.test(ckpt_path=\"best\",\n",
    "                                  test_dataloaders=model.val_dataloader(),\n",
    "                                  verbose=False)[0]\n",
    "            fold_preds = output[\"pred_probs\"]\n",
    "            oof_predictions[val_index, :] = fold_preds\n",
    "\n",
    "            print(fold_preds[:5, :])\n",
    "            fold_valid_loss = mean_logloss(fold_preds, fold_valid_labels)\n",
    "            print(f\"Fold {i} Validation Loss: {fold_valid_loss:.6f}\")\n",
    "\n",
    "            # Generate submission predictions\n",
    "            print(\"Predicting on test set ......\")\n",
    "            best_model.setup()\n",
    "            output = trainer.test(best_model, verbose=False)[0]\n",
    "            submit_preds = output[\"pred_probs\"]\n",
    "            print(test_features.shape, submit_preds.shape)\n",
    "\n",
    "            kfold_submit_preds += submit_preds / kfolds\n",
    "\n",
    "        del model, trainer, train, valid, test, scaler, transformer\n",
    "    else:\n",
    "        print(f\"Inferencing on Fold {i} ......\")\n",
    "        print(train_index.shape, val_index.shape)\n",
    "\n",
    "        model_path = glob.glob(f'{model_output_folder}/fold{i}/epoch*.ckpt')[0]\n",
    "\n",
    "        test = test_features[all_features].copy().values\n",
    "\n",
    "        # Load LogScaler (Norm-2 Normalization)\n",
    "        scaler = load_pickle(f'{model_output_folder}', i, \"log-scaler\")\n",
    "        test = scaler.transform(test)\n",
    "\n",
    "        # Load DeepInsight Feature Map\n",
    "        transformer = load_pickle(f'{model_output_folder}', i,\n",
    "                                  \"deepinsight-transform\")\n",
    "\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        model = get_model(training_set=(None, None),\n",
    "                          valid_set=(None, None),\n",
    "                          test_set=test,\n",
    "                          transformer=transformer,\n",
    "                          model_path=model_path)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            logger=False,\n",
    "            gpus=gpus,\n",
    "            distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n",
    "            precision=16,\n",
    "            benchmark=False,\n",
    "            deterministic=True)\n",
    "        output = trainer.test(model, verbose=False)[0]\n",
    "        submit_preds = output[\"pred_probs\"]\n",
    "        kfold_submit_preds += submit_preds / kfolds\n",
    "\n",
    "        del model, trainer, scaler, transformer, test\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if debug_mode:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF Validation Loss: 0.014620\n"
     ]
    }
   ],
   "source": [
    "if training_mode:\n",
    "    print(oof_predictions.shape)\n",
    "else:\n",
    "    oof_predictions = glob.glob(f'{model_output_folder}/../oof_*.npy')[0]\n",
    "    oof_predictions = np.load(oof_predictions)\n",
    "\n",
    "oof_loss = mean_logloss(oof_predictions,\n",
    "                        train_labels[train_classes].values)\n",
    "print(f\"OOF Validation Loss: {oof_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ResNeSt]\n",
    "# OOF Validation Loss: 0.014620\n",
    "# \"dropblock_prob\":        0.0\n",
    "# \"fc_size\":               512\n",
    "# \"final_drop\":            0.0\n",
    "# \"learning_rate\":         0.000352\n",
    "# \"num_classes\":           206\n",
    "# \"pretrained_model_name\": resnest50_fast_2s2x40d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_mode and best_model is not None:\n",
    "    print(best_model.hparams)\n",
    "    extra_params = {\n",
    "        \"gpus\": len(gpus),\n",
    "        # \"pos_weight\": True\n",
    "    }\n",
    "    exp_logger.experiment.add_hparams(hparam_dict={\n",
    "        **dict(best_model.hparams),\n",
    "        **extra_params\n",
    "    },\n",
    "                                      metric_dict={\"oof_loss\": oof_loss})\n",
    "\n",
    "    oof_filename = \"_\".join(\n",
    "        [f\"{k}={v}\" for k, v in dict(best_model.hparams).items()])\n",
    "    with open(f'oof_{experiment_name}_{oof_loss}.npy', 'wb') as f:\n",
    "        np.save(f, oof_predictions)\n",
    "\n",
    "    with open(f'oof_{experiment_name}_{oof_loss}.npy', 'rb') as f:\n",
    "        tmp = np.load(f)\n",
    "        print(tmp.shape)\n",
    "\n",
    "    # Rename model filename to remove `=` for Kaggle Dataset rule\n",
    "    model_files = glob.glob(f'{model_output_folder}/fold*/epoch*.ckpt')\n",
    "    for f in model_files:\n",
    "        new_filename = f.replace(\"=\", \"\")\n",
    "        os.rename(f, new_filename)\n",
    "        print(new_filename)\n",
    "\n",
    "    del best_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3982, 206)\n"
     ]
    }
   ],
   "source": [
    "print(kfold_submit_preds.shape)\n",
    "\n",
    "submission = pd.DataFrame(data=test_features[\"sig_id\"].values,\n",
    "                          columns=[\"sig_id\"])\n",
    "submission = submission.reindex(columns=[\"sig_id\"] + train_classes)\n",
    "submission[train_classes] = kfold_submit_preds\n",
    "# Set control type to 0 as control perturbations have no MoAs\n",
    "submission.loc[test_features['cp_type'] == 0, submission.columns[1:]] = 0\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "submission.to_csv('submission_resnest_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>adrenergic_receptor_antagonist</th>\n",
       "      <th>akt_inhibitor</th>\n",
       "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
       "      <th>alk_inhibitor</th>\n",
       "      <th>ampk_activator</th>\n",
       "      <th>analgesic</th>\n",
       "      <th>androgen_receptor_agonist</th>\n",
       "      <th>androgen_receptor_antagonist</th>\n",
       "      <th>anesthetic_-_local</th>\n",
       "      <th>angiogenesis_inhibitor</th>\n",
       "      <th>angiotensin_receptor_antagonist</th>\n",
       "      <th>anti-inflammatory</th>\n",
       "      <th>antiarrhythmic</th>\n",
       "      <th>antibiotic</th>\n",
       "      <th>anticonvulsant</th>\n",
       "      <th>antifungal</th>\n",
       "      <th>antihistamine</th>\n",
       "      <th>antimalarial</th>\n",
       "      <th>antioxidant</th>\n",
       "      <th>antiprotozoal</th>\n",
       "      <th>antiviral</th>\n",
       "      <th>apoptosis_stimulant</th>\n",
       "      <th>aromatase_inhibitor</th>\n",
       "      <th>atm_kinase_inhibitor</th>\n",
       "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
       "      <th>atp_synthase_inhibitor</th>\n",
       "      <th>atpase_inhibitor</th>\n",
       "      <th>atr_kinase_inhibitor</th>\n",
       "      <th>aurora_kinase_inhibitor</th>\n",
       "      <th>autotaxin_inhibitor</th>\n",
       "      <th>bacterial_30s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_50s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_antifolate</th>\n",
       "      <th>bacterial_cell_wall_synthesis_inhibitor</th>\n",
       "      <th>bacterial_dna_gyrase_inhibitor</th>\n",
       "      <th>bacterial_dna_inhibitor</th>\n",
       "      <th>bacterial_membrane_integrity_inhibitor</th>\n",
       "      <th>bcl_inhibitor</th>\n",
       "      <th>bcr-abl_inhibitor</th>\n",
       "      <th>benzodiazepine_receptor_agonist</th>\n",
       "      <th>beta_amyloid_inhibitor</th>\n",
       "      <th>bromodomain_inhibitor</th>\n",
       "      <th>btk_inhibitor</th>\n",
       "      <th>calcineurin_inhibitor</th>\n",
       "      <th>calcium_channel_blocker</th>\n",
       "      <th>cannabinoid_receptor_agonist</th>\n",
       "      <th>cannabinoid_receptor_antagonist</th>\n",
       "      <th>carbonic_anhydrase_inhibitor</th>\n",
       "      <th>casein_kinase_inhibitor</th>\n",
       "      <th>caspase_activator</th>\n",
       "      <th>catechol_o_methyltransferase_inhibitor</th>\n",
       "      <th>cc_chemokine_receptor_antagonist</th>\n",
       "      <th>cck_receptor_antagonist</th>\n",
       "      <th>cdk_inhibitor</th>\n",
       "      <th>chelating_agent</th>\n",
       "      <th>chk_inhibitor</th>\n",
       "      <th>chloride_channel_blocker</th>\n",
       "      <th>cholesterol_inhibitor</th>\n",
       "      <th>cholinergic_receptor_antagonist</th>\n",
       "      <th>coagulation_factor_inhibitor</th>\n",
       "      <th>corticosteroid_agonist</th>\n",
       "      <th>cyclooxygenase_inhibitor</th>\n",
       "      <th>cytochrome_p450_inhibitor</th>\n",
       "      <th>dihydrofolate_reductase_inhibitor</th>\n",
       "      <th>dipeptidyl_peptidase_inhibitor</th>\n",
       "      <th>diuretic</th>\n",
       "      <th>dna_alkylating_agent</th>\n",
       "      <th>dna_inhibitor</th>\n",
       "      <th>dopamine_receptor_agonist</th>\n",
       "      <th>dopamine_receptor_antagonist</th>\n",
       "      <th>egfr_inhibitor</th>\n",
       "      <th>elastase_inhibitor</th>\n",
       "      <th>erbb2_inhibitor</th>\n",
       "      <th>estrogen_receptor_agonist</th>\n",
       "      <th>estrogen_receptor_antagonist</th>\n",
       "      <th>faah_inhibitor</th>\n",
       "      <th>farnesyltransferase_inhibitor</th>\n",
       "      <th>fatty_acid_receptor_agonist</th>\n",
       "      <th>fgfr_inhibitor</th>\n",
       "      <th>flt3_inhibitor</th>\n",
       "      <th>focal_adhesion_kinase_inhibitor</th>\n",
       "      <th>free_radical_scavenger</th>\n",
       "      <th>fungal_squalene_epoxidase_inhibitor</th>\n",
       "      <th>gaba_receptor_agonist</th>\n",
       "      <th>gaba_receptor_antagonist</th>\n",
       "      <th>gamma_secretase_inhibitor</th>\n",
       "      <th>glucocorticoid_receptor_agonist</th>\n",
       "      <th>glutamate_inhibitor</th>\n",
       "      <th>glutamate_receptor_agonist</th>\n",
       "      <th>glutamate_receptor_antagonist</th>\n",
       "      <th>gonadotropin_receptor_agonist</th>\n",
       "      <th>gsk_inhibitor</th>\n",
       "      <th>hcv_inhibitor</th>\n",
       "      <th>hdac_inhibitor</th>\n",
       "      <th>histamine_receptor_agonist</th>\n",
       "      <th>histamine_receptor_antagonist</th>\n",
       "      <th>histone_lysine_demethylase_inhibitor</th>\n",
       "      <th>histone_lysine_methyltransferase_inhibitor</th>\n",
       "      <th>hiv_inhibitor</th>\n",
       "      <th>hmgcr_inhibitor</th>\n",
       "      <th>hsp_inhibitor</th>\n",
       "      <th>igf-1_inhibitor</th>\n",
       "      <th>ikk_inhibitor</th>\n",
       "      <th>imidazoline_receptor_agonist</th>\n",
       "      <th>immunosuppressant</th>\n",
       "      <th>insulin_secretagogue</th>\n",
       "      <th>insulin_sensitizer</th>\n",
       "      <th>integrin_inhibitor</th>\n",
       "      <th>jak_inhibitor</th>\n",
       "      <th>kit_inhibitor</th>\n",
       "      <th>laxative</th>\n",
       "      <th>leukotriene_inhibitor</th>\n",
       "      <th>leukotriene_receptor_antagonist</th>\n",
       "      <th>lipase_inhibitor</th>\n",
       "      <th>lipoxygenase_inhibitor</th>\n",
       "      <th>lxr_agonist</th>\n",
       "      <th>mdm_inhibitor</th>\n",
       "      <th>mek_inhibitor</th>\n",
       "      <th>membrane_integrity_inhibitor</th>\n",
       "      <th>mineralocorticoid_receptor_antagonist</th>\n",
       "      <th>monoacylglycerol_lipase_inhibitor</th>\n",
       "      <th>monoamine_oxidase_inhibitor</th>\n",
       "      <th>monopolar_spindle_1_kinase_inhibitor</th>\n",
       "      <th>mtor_inhibitor</th>\n",
       "      <th>mucolytic_agent</th>\n",
       "      <th>neuropeptide_receptor_antagonist</th>\n",
       "      <th>nfkb_inhibitor</th>\n",
       "      <th>nicotinic_receptor_agonist</th>\n",
       "      <th>nitric_oxide_donor</th>\n",
       "      <th>nitric_oxide_production_inhibitor</th>\n",
       "      <th>nitric_oxide_synthase_inhibitor</th>\n",
       "      <th>norepinephrine_reuptake_inhibitor</th>\n",
       "      <th>nrf2_activator</th>\n",
       "      <th>opioid_receptor_agonist</th>\n",
       "      <th>opioid_receptor_antagonist</th>\n",
       "      <th>orexin_receptor_antagonist</th>\n",
       "      <th>p38_mapk_inhibitor</th>\n",
       "      <th>p-glycoprotein_inhibitor</th>\n",
       "      <th>parp_inhibitor</th>\n",
       "      <th>pdgfr_inhibitor</th>\n",
       "      <th>pdk_inhibitor</th>\n",
       "      <th>phosphodiesterase_inhibitor</th>\n",
       "      <th>phospholipase_inhibitor</th>\n",
       "      <th>pi3k_inhibitor</th>\n",
       "      <th>pkc_inhibitor</th>\n",
       "      <th>potassium_channel_activator</th>\n",
       "      <th>potassium_channel_antagonist</th>\n",
       "      <th>ppar_receptor_agonist</th>\n",
       "      <th>ppar_receptor_antagonist</th>\n",
       "      <th>progesterone_receptor_agonist</th>\n",
       "      <th>progesterone_receptor_antagonist</th>\n",
       "      <th>prostaglandin_inhibitor</th>\n",
       "      <th>prostanoid_receptor_antagonist</th>\n",
       "      <th>proteasome_inhibitor</th>\n",
       "      <th>protein_kinase_inhibitor</th>\n",
       "      <th>protein_phosphatase_inhibitor</th>\n",
       "      <th>protein_synthesis_inhibitor</th>\n",
       "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
       "      <th>radiopaque_medium</th>\n",
       "      <th>raf_inhibitor</th>\n",
       "      <th>ras_gtpase_inhibitor</th>\n",
       "      <th>retinoid_receptor_agonist</th>\n",
       "      <th>retinoid_receptor_antagonist</th>\n",
       "      <th>rho_associated_kinase_inhibitor</th>\n",
       "      <th>ribonucleoside_reductase_inhibitor</th>\n",
       "      <th>rna_polymerase_inhibitor</th>\n",
       "      <th>serotonin_receptor_agonist</th>\n",
       "      <th>serotonin_receptor_antagonist</th>\n",
       "      <th>serotonin_reuptake_inhibitor</th>\n",
       "      <th>sigma_receptor_agonist</th>\n",
       "      <th>sigma_receptor_antagonist</th>\n",
       "      <th>smoothened_receptor_antagonist</th>\n",
       "      <th>sodium_channel_inhibitor</th>\n",
       "      <th>sphingosine_receptor_agonist</th>\n",
       "      <th>src_inhibitor</th>\n",
       "      <th>steroid</th>\n",
       "      <th>syk_inhibitor</th>\n",
       "      <th>tachykinin_antagonist</th>\n",
       "      <th>tgf-beta_receptor_inhibitor</th>\n",
       "      <th>thrombin_inhibitor</th>\n",
       "      <th>thymidylate_synthase_inhibitor</th>\n",
       "      <th>tlr_agonist</th>\n",
       "      <th>tlr_antagonist</th>\n",
       "      <th>tnf_inhibitor</th>\n",
       "      <th>topoisomerase_inhibitor</th>\n",
       "      <th>transient_receptor_potential_channel_antagonist</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0.017219</td>\n",
       "      <td>0.028460</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.012703</td>\n",
       "      <td>0.016551</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.006487</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.005769</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>0.013880</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.011951</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.012757</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.039086</td>\n",
       "      <td>0.005818</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>0.016305</td>\n",
       "      <td>0.009720</td>\n",
       "      <td>0.021861</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.015661</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.015535</td>\n",
       "      <td>0.012593</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>0.029380</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.003618</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.013662</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.011585</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.005486</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.004259</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.004539</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.017828</td>\n",
       "      <td>0.002565</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.004462</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.012969</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>0.008044</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.004703</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>0.016735</td>\n",
       "      <td>0.012503</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.003576</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.018148</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.001734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.004269</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.037734</td>\n",
       "      <td>0.033125</td>\n",
       "      <td>0.005661</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.004387</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>0.013726</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.012874</td>\n",
       "      <td>0.007193</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.002683</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.004017</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.003615</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.034894</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.006009</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.021654</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.084231</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.039361</td>\n",
       "      <td>0.006647</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.038366</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.002099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.021336</td>\n",
       "      <td>0.017754</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.004256</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.008010</td>\n",
       "      <td>0.026833</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.004639</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.018013</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.042589</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.005371</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.012338</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.005458</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.024003</td>\n",
       "      <td>0.005104</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.008934</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>0.035769</td>\n",
       "      <td>0.010587</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.004475</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.004197</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.035255</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.005862</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.018546</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.004022</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.011118</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.004438</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.003719</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.002436</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.004946</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.008120</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>0.008088</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.004454</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.003224</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.011572</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.019471</td>\n",
       "      <td>0.032757</td>\n",
       "      <td>0.005595</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.006009</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.009872</td>\n",
       "      <td>0.026518</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.003658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.013809</td>\n",
       "      <td>0.022599</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.014430</td>\n",
       "      <td>0.019731</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>0.007158</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.005708</td>\n",
       "      <td>0.006609</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.003379</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.006489</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>0.004275</td>\n",
       "      <td>0.015305</td>\n",
       "      <td>0.015342</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.003716</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.022305</td>\n",
       "      <td>0.006447</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.004585</td>\n",
       "      <td>0.036736</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.011674</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.011396</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.007643</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.007221</td>\n",
       "      <td>0.022928</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.003389</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.003176</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.005157</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.004220</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.002495</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.005010</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>0.020965</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.012937</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>id_ff7004b87</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.003194</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.017026</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>0.203199</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.074082</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.003881</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.021449</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.021078</td>\n",
       "      <td>0.021827</td>\n",
       "      <td>0.047872</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.005504</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.226591</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.009192</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>0.005965</td>\n",
       "      <td>0.005536</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.034569</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.005953</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.021641</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.014277</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.256924</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.005442</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>id_ff925dd0d</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.003684</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.021057</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.004908</td>\n",
       "      <td>0.003286</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.014596</td>\n",
       "      <td>0.031715</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.007192</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.005028</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>0.004363</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.015301</td>\n",
       "      <td>0.008627</td>\n",
       "      <td>0.011167</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.004453</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.012183</td>\n",
       "      <td>0.004750</td>\n",
       "      <td>0.004397</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.003073</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.002329</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.020818</td>\n",
       "      <td>0.007085</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.011191</td>\n",
       "      <td>0.005866</td>\n",
       "      <td>0.018661</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.021159</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.015701</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>0.021636</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.010888</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.007406</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.005586</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>0.005246</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.019052</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>0.007677</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.011975</td>\n",
       "      <td>0.020422</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.020597</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>id_ffb710450</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>0.029902</td>\n",
       "      <td>0.010260</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.011617</td>\n",
       "      <td>0.023003</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>0.004397</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>0.008987</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.003286</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>0.003660</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>0.011985</td>\n",
       "      <td>0.006277</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.006101</td>\n",
       "      <td>0.004512</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.003499</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.026777</td>\n",
       "      <td>0.008446</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.005408</td>\n",
       "      <td>0.019957</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>0.011808</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.005566</td>\n",
       "      <td>0.020623</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.005949</td>\n",
       "      <td>0.009658</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.005448</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.003402</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>0.007402</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.009239</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.004893</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.007947</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>0.033458</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.020745</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.003358</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.003213</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.001608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>id_ffbb869f2</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.029370</td>\n",
       "      <td>0.023803</td>\n",
       "      <td>0.003607</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.020688</td>\n",
       "      <td>0.035770</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>0.003605</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>0.015047</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>0.007336</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.005323</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.014311</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.004638</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.017125</td>\n",
       "      <td>0.003607</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>0.008433</td>\n",
       "      <td>0.020426</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>0.019998</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.003090</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.023035</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.006074</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.007011</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.004798</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.002683</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>0.004490</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.019336</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.004969</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.003660</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.013144</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.006205</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.002743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>id_ffd5800b6</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.014805</td>\n",
       "      <td>0.025146</td>\n",
       "      <td>0.005589</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.031652</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.006210</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.010442</td>\n",
       "      <td>0.002683</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.003522</td>\n",
       "      <td>0.004269</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.016079</td>\n",
       "      <td>0.005085</td>\n",
       "      <td>0.007684</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.003498</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.044277</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.012815</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.025575</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.008558</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.009090</td>\n",
       "      <td>0.015570</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>0.022243</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.016264</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.005261</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.010808</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.006473</td>\n",
       "      <td>0.003607</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.017244</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.004735</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.003224</td>\n",
       "      <td>0.009101</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.004428</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.031294</td>\n",
       "      <td>0.021217</td>\n",
       "      <td>0.003224</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.017870</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.001510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3982 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0     id_0004d9e33                     0.001332                0.001099   \n",
       "1     id_001897cda                     0.000152                0.000372   \n",
       "2     id_002429b5b                     0.000000                0.000000   \n",
       "3     id_00276f245                     0.000501                0.000365   \n",
       "4     id_0027f1083                     0.002931                0.001888   \n",
       "...            ...                          ...                     ...   \n",
       "3977  id_ff7004b87                     0.000212                0.000372   \n",
       "3978  id_ff925dd0d                     0.004130                0.003684   \n",
       "3979  id_ffb710450                     0.004863                0.002796   \n",
       "3980  id_ffbb869f2                     0.001743                0.000807   \n",
       "3981  id_ffd5800b6                     0.001673                0.000793   \n",
       "\n",
       "      acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0           0.001708                        0.017219   \n",
       "1           0.000652                        0.000472   \n",
       "2           0.000000                        0.000000   \n",
       "3           0.001821                        0.021336   \n",
       "4           0.001453                        0.013809   \n",
       "...              ...                             ...   \n",
       "3977        0.000703                        0.002555   \n",
       "3978        0.000883                        0.008432   \n",
       "3979        0.000962                        0.007268   \n",
       "3980        0.000795                        0.029370   \n",
       "3981        0.001685                        0.014805   \n",
       "\n",
       "      acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                              0.028460                        0.004328   \n",
       "1                              0.001160                        0.002215   \n",
       "2                              0.000000                        0.000000   \n",
       "3                              0.017754                        0.002885   \n",
       "4                              0.022599                        0.004728   \n",
       "...                                 ...                             ...   \n",
       "3977                           0.002506                        0.000382   \n",
       "3978                           0.021057                        0.006874   \n",
       "3979                           0.029902                        0.010260   \n",
       "3980                           0.023803                        0.003607   \n",
       "3981                           0.025146                        0.005589   \n",
       "\n",
       "      adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                       0.004370                       0.003497   \n",
       "1                       0.004269                       0.007693   \n",
       "2                       0.000000                       0.000000   \n",
       "3                       0.003742                       0.004256   \n",
       "4                       0.003722                       0.001505   \n",
       "...                          ...                            ...   \n",
       "3977                    0.000538                       0.001372   \n",
       "3978                    0.004908                       0.003286   \n",
       "3979                    0.002816                       0.001889   \n",
       "3980                    0.008500                       0.003132   \n",
       "3981                    0.006527                       0.003629   \n",
       "\n",
       "      adenylyl_cyclase_activator  adrenergic_receptor_agonist  \\\n",
       "0                       0.000295                     0.012703   \n",
       "1                       0.037734                     0.033125   \n",
       "2                       0.000000                     0.000000   \n",
       "3                       0.000390                     0.008010   \n",
       "4                       0.000306                     0.014430   \n",
       "...                          ...                          ...   \n",
       "3977                    0.000246                     0.001442   \n",
       "3978                    0.000257                     0.014596   \n",
       "3979                    0.000269                     0.011617   \n",
       "3980                    0.000660                     0.020688   \n",
       "3981                    0.000405                     0.009956   \n",
       "\n",
       "      adrenergic_receptor_antagonist  akt_inhibitor  \\\n",
       "0                           0.016551       0.000814   \n",
       "1                           0.005661       0.000807   \n",
       "2                           0.000000       0.000000   \n",
       "3                           0.026833       0.000887   \n",
       "4                           0.019731       0.001352   \n",
       "...                              ...            ...   \n",
       "3977                        0.001507       0.000324   \n",
       "3978                        0.031715       0.000902   \n",
       "3979                        0.023003       0.001089   \n",
       "3980                        0.035770       0.000926   \n",
       "3981                        0.031652       0.001010   \n",
       "\n",
       "      aldehyde_dehydrogenase_inhibitor  alk_inhibitor  ampk_activator  \\\n",
       "0                             0.000329       0.000567        0.002274   \n",
       "1                             0.000089       0.033898        0.000176   \n",
       "2                             0.000000       0.000000        0.000000   \n",
       "3                             0.000451       0.002363        0.001515   \n",
       "4                             0.000425       0.000676        0.001681   \n",
       "...                                ...            ...             ...   \n",
       "3977                          0.000244       0.004937        0.000388   \n",
       "3978                          0.000479       0.000369        0.000710   \n",
       "3979                          0.000441       0.000582        0.000715   \n",
       "3980                          0.000338       0.001378        0.001217   \n",
       "3981                          0.000449       0.000934        0.001143   \n",
       "\n",
       "      analgesic  androgen_receptor_agonist  androgen_receptor_antagonist  \\\n",
       "0      0.002009                   0.001966                      0.006454   \n",
       "1      0.000229                   0.000580                      0.002261   \n",
       "2      0.000000                   0.000000                      0.000000   \n",
       "3      0.002624                   0.001117                      0.004639   \n",
       "4      0.001453                   0.002319                      0.007158   \n",
       "...         ...                        ...                           ...   \n",
       "3977   0.000626                   0.000413                      0.001901   \n",
       "3978   0.000767                   0.007463                      0.007192   \n",
       "3979   0.000819                   0.005141                      0.006471   \n",
       "3980   0.002154                   0.001990                      0.006271   \n",
       "3981   0.001518                   0.002275                      0.004032   \n",
       "\n",
       "      anesthetic_-_local  angiogenesis_inhibitor  \\\n",
       "0               0.006487                0.001693   \n",
       "1               0.001385                0.004387   \n",
       "2               0.000000                0.000000   \n",
       "3               0.002938                0.003471   \n",
       "4               0.003257                0.001093   \n",
       "...                  ...                     ...   \n",
       "3977            0.001362                0.000960   \n",
       "3978            0.004741                0.002154   \n",
       "3979            0.004397                0.001118   \n",
       "3980            0.003092                0.002947   \n",
       "3981            0.005055                0.002614   \n",
       "\n",
       "      angiotensin_receptor_antagonist  anti-inflammatory  antiarrhythmic  \\\n",
       "0                            0.006397           0.003832        0.000452   \n",
       "1                            0.000739           0.000394        0.000181   \n",
       "2                            0.000000           0.000000        0.000000   \n",
       "3                            0.002149           0.003276        0.000390   \n",
       "4                            0.005708           0.006609        0.000929   \n",
       "...                               ...                ...             ...   \n",
       "3977                         0.000893           0.000129        0.001326   \n",
       "3978                         0.003599           0.005028        0.001203   \n",
       "3979                         0.003428           0.008987        0.000975   \n",
       "3980                         0.003487           0.004513        0.000580   \n",
       "3981                         0.002660           0.006210        0.000479   \n",
       "\n",
       "      antibiotic  anticonvulsant  antifungal  antihistamine  antimalarial  \\\n",
       "0       0.001945        0.000610    0.000519       0.000860      0.001623   \n",
       "1       0.000239        0.000242    0.000710       0.000392      0.000212   \n",
       "2       0.000000        0.000000    0.000000       0.000000      0.000000   \n",
       "3       0.003869        0.001100    0.000655       0.003433      0.001786   \n",
       "4       0.002399        0.001014    0.000487       0.000665      0.000873   \n",
       "...          ...             ...         ...            ...           ...   \n",
       "3977    0.001220        0.000228    0.003194       0.001104      0.000918   \n",
       "3978    0.001561        0.001510    0.000837       0.000572      0.000992   \n",
       "3979    0.002003        0.001911    0.000549       0.000493      0.000881   \n",
       "3980    0.002902        0.001050    0.000639       0.000987      0.000813   \n",
       "3981    0.001867        0.001014    0.000892       0.001213      0.000866   \n",
       "\n",
       "      antioxidant  antiprotozoal  antiviral  apoptosis_stimulant  \\\n",
       "0        0.005769       0.002499   0.001800             0.003481   \n",
       "1        0.002450       0.001088   0.000366             0.000418   \n",
       "2        0.000000       0.000000   0.000000             0.000000   \n",
       "3        0.005149       0.001300   0.001231             0.002657   \n",
       "4        0.008326       0.003379   0.002364             0.004398   \n",
       "...           ...            ...        ...                  ...   \n",
       "3977     0.001329       0.000179   0.000315             0.003155   \n",
       "3978     0.003811       0.004363   0.003437             0.002294   \n",
       "3979     0.004319       0.003691   0.003286             0.002860   \n",
       "3980     0.006884       0.003605   0.001695             0.002015   \n",
       "3981     0.010442       0.002683   0.002404             0.002071   \n",
       "\n",
       "      aromatase_inhibitor  atm_kinase_inhibitor  \\\n",
       "0                0.003827              0.000317   \n",
       "1                0.000461              0.000403   \n",
       "2                0.000000              0.000000   \n",
       "3                0.003124              0.001273   \n",
       "4                0.002727              0.000459   \n",
       "...                   ...                   ...   \n",
       "3977             0.000293              0.000751   \n",
       "3978             0.003995              0.000255   \n",
       "3979             0.003660              0.000452   \n",
       "3980             0.003048              0.000509   \n",
       "3981             0.002576              0.000412   \n",
       "\n",
       "      atp-sensitive_potassium_channel_antagonist  atp_synthase_inhibitor  \\\n",
       "0                                       0.000570                0.000460   \n",
       "1                                       0.000353                0.000320   \n",
       "2                                       0.000000                0.000000   \n",
       "3                                       0.000913                0.000497   \n",
       "4                                       0.000536                0.000398   \n",
       "...                                          ...                     ...   \n",
       "3977                                    0.000286                0.000136   \n",
       "3978                                    0.000451                0.000426   \n",
       "3979                                    0.000543                0.000402   \n",
       "3980                                    0.000524                0.000353   \n",
       "3981                                    0.000580                0.000487   \n",
       "\n",
       "      atpase_inhibitor  atr_kinase_inhibitor  aurora_kinase_inhibitor  \\\n",
       "0             0.002404              0.000210                 0.000324   \n",
       "1             0.001410              0.003865                 0.013726   \n",
       "2             0.000000              0.000000                 0.000000   \n",
       "3             0.004431              0.000415                 0.001012   \n",
       "4             0.002189              0.000258                 0.000177   \n",
       "...                ...                   ...                      ...   \n",
       "3977          0.017026              0.002841                 0.203199   \n",
       "3978          0.003396              0.000218                 0.000291   \n",
       "3979          0.002475              0.000194                 0.000158   \n",
       "3980          0.002291              0.000375                 0.000165   \n",
       "3981          0.003539              0.000362                 0.000559   \n",
       "\n",
       "      autotaxin_inhibitor  bacterial_30s_ribosomal_subunit_inhibitor  \\\n",
       "0                0.000739                                   0.005764   \n",
       "1                0.000325                                   0.001498   \n",
       "2                0.000000                                   0.000000   \n",
       "3                0.002151                                   0.002004   \n",
       "4                0.000651                                   0.006489   \n",
       "...                   ...                                        ...   \n",
       "3977             0.001214                                   0.000646   \n",
       "3978             0.000474                                   0.005610   \n",
       "3979             0.000583                                   0.005822   \n",
       "3980             0.001363                                   0.003078   \n",
       "3981             0.000672                                   0.003522   \n",
       "\n",
       "      bacterial_50s_ribosomal_subunit_inhibitor  bacterial_antifolate  \\\n",
       "0                                      0.013880              0.001763   \n",
       "1                                      0.000471              0.000176   \n",
       "2                                      0.000000              0.000000   \n",
       "3                                      0.004673              0.001815   \n",
       "4                                      0.005705              0.004275   \n",
       "...                                         ...                   ...   \n",
       "3977                                   0.000241              0.000451   \n",
       "3978                                   0.004065              0.003480   \n",
       "3979                                   0.004329              0.005963   \n",
       "3980                                   0.002459              0.003640   \n",
       "3981                                   0.004269              0.002208   \n",
       "\n",
       "      bacterial_cell_wall_synthesis_inhibitor  bacterial_dna_gyrase_inhibitor  \\\n",
       "0                                    0.011951                        0.008236   \n",
       "1                                    0.002146                        0.000704   \n",
       "2                                    0.000000                        0.000000   \n",
       "3                                    0.018013                        0.003875   \n",
       "4                                    0.015305                        0.015342   \n",
       "...                                       ...                             ...   \n",
       "3977                                 0.000525                        0.000430   \n",
       "3978                                 0.015301                        0.008627   \n",
       "3979                                 0.014242                        0.011985   \n",
       "3980                                 0.015047                        0.007488   \n",
       "3981                                 0.016079                        0.005085   \n",
       "\n",
       "      bacterial_dna_inhibitor  bacterial_membrane_integrity_inhibitor  \\\n",
       "0                    0.007067                                0.000429   \n",
       "1                    0.000823                                0.000216   \n",
       "2                    0.000000                                0.000000   \n",
       "3                    0.003510                                0.000660   \n",
       "4                    0.007562                                0.000391   \n",
       "...                       ...                                     ...   \n",
       "3977                 0.001029                                0.001344   \n",
       "3978                 0.011167                                0.000337   \n",
       "3979                 0.006277                                0.000474   \n",
       "3980                 0.007336                                0.000434   \n",
       "3981                 0.007684                                0.000387   \n",
       "\n",
       "      bcl_inhibitor  bcr-abl_inhibitor  benzodiazepine_receptor_agonist  \\\n",
       "0          0.004064           0.000728                         0.001647   \n",
       "1          0.000503           0.000685                         0.003889   \n",
       "2          0.000000           0.000000                         0.000000   \n",
       "3          0.000926           0.000945                         0.002882   \n",
       "4          0.002748           0.000607                         0.002383   \n",
       "...             ...                ...                              ...   \n",
       "3977       0.000537           0.074082                         0.001480   \n",
       "3978       0.001021           0.000952                         0.004453   \n",
       "3979       0.000939           0.000455                         0.002632   \n",
       "3980       0.000799           0.000575                         0.005323   \n",
       "3981       0.000861           0.000376                         0.003310   \n",
       "\n",
       "      beta_amyloid_inhibitor  bromodomain_inhibitor  btk_inhibitor  \\\n",
       "0                   0.001853               0.001720       0.000442   \n",
       "1                   0.000631               0.003088       0.012102   \n",
       "2                   0.000000               0.000000       0.000000   \n",
       "3                   0.002839               0.001077       0.001468   \n",
       "4                   0.001910               0.001448       0.000359   \n",
       "...                      ...                    ...            ...   \n",
       "3977                0.000695               0.000280       0.003881   \n",
       "3978                0.001473               0.001431       0.000598   \n",
       "3979                0.001746               0.001180       0.000414   \n",
       "3980                0.001946               0.001793       0.000693   \n",
       "3981                0.001887               0.002415       0.000610   \n",
       "\n",
       "      calcineurin_inhibitor  calcium_channel_blocker  \\\n",
       "0                  0.000988                 0.012757   \n",
       "1                  0.000147                 0.012874   \n",
       "2                  0.000000                 0.000000   \n",
       "3                  0.001964                 0.042589   \n",
       "4                  0.000801                 0.005710   \n",
       "...                     ...                      ...   \n",
       "3977               0.000402                 0.021449   \n",
       "3978               0.000825                 0.012183   \n",
       "3979               0.000757                 0.006101   \n",
       "3980               0.000803                 0.014311   \n",
       "3981               0.001061                 0.012895   \n",
       "\n",
       "      cannabinoid_receptor_agonist  cannabinoid_receptor_antagonist  \\\n",
       "0                         0.000894                         0.002958   \n",
       "1                         0.007193                         0.001213   \n",
       "2                         0.000000                         0.000000   \n",
       "3                         0.000588                         0.005371   \n",
       "4                         0.002272                         0.001367   \n",
       "...                            ...                              ...   \n",
       "3977                      0.000472                         0.000220   \n",
       "3978                      0.004750                         0.004397   \n",
       "3979                      0.004512                         0.001990   \n",
       "3980                      0.001751                         0.002691   \n",
       "3981                      0.002020                         0.003232   \n",
       "\n",
       "      carbonic_anhydrase_inhibitor  casein_kinase_inhibitor  \\\n",
       "0                         0.001343                 0.001671   \n",
       "1                         0.001895                 0.001209   \n",
       "2                         0.000000                 0.000000   \n",
       "3                         0.001279                 0.000928   \n",
       "4                         0.002544                 0.003488   \n",
       "...                            ...                      ...   \n",
       "3977                      0.000399                 0.000705   \n",
       "3978                      0.002694                 0.001720   \n",
       "3979                      0.003185                 0.001622   \n",
       "3980                      0.002632                 0.002869   \n",
       "3981                      0.001962                 0.002057   \n",
       "\n",
       "      caspase_activator  catechol_o_methyltransferase_inhibitor  \\\n",
       "0              0.001620                                0.001422   \n",
       "1              0.000586                                0.000433   \n",
       "2              0.000000                                0.000000   \n",
       "3              0.001062                                0.001033   \n",
       "4              0.001706                                0.001596   \n",
       "...                 ...                                     ...   \n",
       "3977           0.000128                                0.000840   \n",
       "3978           0.001913                                0.001714   \n",
       "3979           0.002483                                0.001482   \n",
       "3980           0.001486                                0.000905   \n",
       "3981           0.001504                                0.001719   \n",
       "\n",
       "      cc_chemokine_receptor_antagonist  cck_receptor_antagonist  \\\n",
       "0                             0.005619                 0.002216   \n",
       "1                             0.002022                 0.000656   \n",
       "2                             0.000000                 0.000000   \n",
       "3                             0.012338                 0.000971   \n",
       "4                             0.004300                 0.002013   \n",
       "...                                ...                      ...   \n",
       "3977                          0.000727                 0.000406   \n",
       "3978                          0.004737                 0.003580   \n",
       "3979                          0.004025                 0.003301   \n",
       "3980                          0.004638                 0.001768   \n",
       "3981                          0.007200                 0.001826   \n",
       "\n",
       "      cdk_inhibitor  chelating_agent  chk_inhibitor  chloride_channel_blocker  \\\n",
       "0          0.000697         0.004056       0.000291                  0.003661   \n",
       "1          0.001253         0.001404       0.000169                  0.000557   \n",
       "2          0.000000         0.000000       0.000000                  0.000000   \n",
       "3          0.000812         0.005458       0.000501                  0.003445   \n",
       "4          0.001088         0.004151       0.000363                  0.003716   \n",
       "...             ...              ...            ...                       ...   \n",
       "3977       0.001158         0.001218       0.002369                  0.000665   \n",
       "3978       0.002144         0.003073       0.000262                  0.003140   \n",
       "3979       0.001088         0.002628       0.000331                  0.003499   \n",
       "3980       0.001280         0.003505       0.000335                  0.002395   \n",
       "3981       0.001042         0.003442       0.000398                  0.003498   \n",
       "\n",
       "      cholesterol_inhibitor  cholinergic_receptor_antagonist  \\\n",
       "0                  0.002671                         0.005355   \n",
       "1                  0.004811                         0.000444   \n",
       "2                  0.000000                         0.000000   \n",
       "3                  0.003283                         0.004249   \n",
       "4                  0.001680                         0.003541   \n",
       "...                     ...                              ...   \n",
       "3977               0.000297                         0.000256   \n",
       "3978               0.002204                         0.002329   \n",
       "3979               0.002352                         0.002642   \n",
       "3980               0.001991                         0.004097   \n",
       "3981               0.005600                         0.002038   \n",
       "\n",
       "      coagulation_factor_inhibitor  corticosteroid_agonist  \\\n",
       "0                         0.000651                0.000750   \n",
       "1                         0.000234                0.000222   \n",
       "2                         0.000000                0.000000   \n",
       "3                         0.000483                0.000656   \n",
       "4                         0.000825                0.000790   \n",
       "...                            ...                     ...   \n",
       "3977                      0.000691                0.001086   \n",
       "3978                      0.001023                0.001407   \n",
       "3979                      0.001018                0.001015   \n",
       "3980                      0.000599                0.001131   \n",
       "3981                      0.000678                0.000689   \n",
       "\n",
       "      cyclooxygenase_inhibitor  cytochrome_p450_inhibitor  \\\n",
       "0                     0.039086                   0.005818   \n",
       "1                     0.001952                   0.002256   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.024003                   0.005104   \n",
       "4                     0.022305                   0.006447   \n",
       "...                        ...                        ...   \n",
       "3977                  0.001703                   0.000598   \n",
       "3978                  0.020818                   0.007085   \n",
       "3979                  0.026777                   0.008446   \n",
       "3980                  0.017125                   0.003607   \n",
       "3981                  0.044277                   0.008277   \n",
       "\n",
       "      dihydrofolate_reductase_inhibitor  dipeptidyl_peptidase_inhibitor  \\\n",
       "0                              0.002081                        0.001793   \n",
       "1                              0.000394                        0.000630   \n",
       "2                              0.000000                        0.000000   \n",
       "3                              0.000978                        0.000655   \n",
       "4                              0.002546                        0.001565   \n",
       "...                                 ...                             ...   \n",
       "3977                           0.000343                        0.000741   \n",
       "3978                           0.001610                        0.003563   \n",
       "3979                           0.001965                        0.003147   \n",
       "3980                           0.001585                        0.001004   \n",
       "3981                           0.000947                        0.001946   \n",
       "\n",
       "      diuretic  dna_alkylating_agent  dna_inhibitor  \\\n",
       "0     0.000840              0.002934       0.016305   \n",
       "1     0.000372              0.000821       0.000951   \n",
       "2     0.000000              0.000000       0.000000   \n",
       "3     0.000644              0.001509       0.008934   \n",
       "4     0.001128              0.004585       0.036736   \n",
       "...        ...                   ...            ...   \n",
       "3977  0.000728              0.021078       0.021827   \n",
       "3978  0.001615              0.001184       0.011191   \n",
       "3979  0.001415              0.001301       0.014100   \n",
       "3980  0.000722              0.002012       0.017648   \n",
       "3981  0.000603              0.003027       0.012815   \n",
       "\n",
       "      dopamine_receptor_agonist  dopamine_receptor_antagonist  egfr_inhibitor  \\\n",
       "0                      0.009720                      0.021861        0.000522   \n",
       "1                      0.001214                      0.002683        0.000668   \n",
       "2                      0.000000                      0.000000        0.000000   \n",
       "3                      0.010278                      0.035769        0.010587   \n",
       "4                      0.007457                      0.011674        0.000630   \n",
       "...                         ...                           ...             ...   \n",
       "3977                   0.047872                      0.002698        0.003565   \n",
       "3978                   0.005866                      0.018661        0.000435   \n",
       "3979                   0.005408                      0.019957        0.001141   \n",
       "3980                   0.008433                      0.020426        0.000942   \n",
       "3981                   0.006165                      0.025575        0.000717   \n",
       "\n",
       "      elastase_inhibitor  erbb2_inhibitor  estrogen_receptor_agonist  \\\n",
       "0               0.000894         0.000566                   0.015661   \n",
       "1               0.000344         0.000464                   0.003197   \n",
       "2               0.000000         0.000000                   0.000000   \n",
       "3               0.000582         0.000804                   0.004339   \n",
       "4               0.000780         0.000574                   0.011396   \n",
       "...                  ...              ...                        ...   \n",
       "3977            0.000434         0.000356                   0.000989   \n",
       "3978            0.000682         0.000399                   0.021159   \n",
       "3979            0.000917         0.000488                   0.007692   \n",
       "3980            0.001040         0.000525                   0.004221   \n",
       "3981            0.001530         0.000610                   0.008558   \n",
       "\n",
       "      estrogen_receptor_antagonist  faah_inhibitor  \\\n",
       "0                         0.001257        0.003413   \n",
       "1                         0.001232        0.001221   \n",
       "2                         0.000000        0.000000   \n",
       "3                         0.003231        0.004475   \n",
       "4                         0.000814        0.001124   \n",
       "...                            ...             ...   \n",
       "3977                      0.000264        0.000610   \n",
       "3978                      0.002555        0.000537   \n",
       "3979                      0.001288        0.000364   \n",
       "3980                      0.001376        0.001667   \n",
       "3981                      0.001559        0.001702   \n",
       "\n",
       "      farnesyltransferase_inhibitor  fatty_acid_receptor_agonist  \\\n",
       "0                          0.000284                     0.001384   \n",
       "1                          0.000148                     0.001103   \n",
       "2                          0.000000                     0.000000   \n",
       "3                          0.001226                     0.001217   \n",
       "4                          0.000361                     0.001925   \n",
       "...                             ...                          ...   \n",
       "3977                       0.005504                     0.000444   \n",
       "3978                       0.000252                     0.002044   \n",
       "3979                       0.000262                     0.001468   \n",
       "3980                       0.000325                     0.003367   \n",
       "3981                       0.000226                     0.000849   \n",
       "\n",
       "      fgfr_inhibitor  flt3_inhibitor  focal_adhesion_kinase_inhibitor  \\\n",
       "0           0.000525        0.000474                         0.000227   \n",
       "1           0.002158        0.001866                         0.000570   \n",
       "2           0.000000        0.000000                         0.000000   \n",
       "3           0.004197        0.000668                         0.000622   \n",
       "4           0.000577        0.000559                         0.000339   \n",
       "...              ...             ...                              ...   \n",
       "3977        0.000331        0.226591                         0.000319   \n",
       "3978        0.000561        0.000321                         0.000289   \n",
       "3979        0.000488        0.000220                         0.000347   \n",
       "3980        0.000663        0.000293                         0.000357   \n",
       "3981        0.000604        0.000428                         0.000220   \n",
       "\n",
       "      free_radical_scavenger  fungal_squalene_epoxidase_inhibitor  \\\n",
       "0                   0.001574                             0.000870   \n",
       "1                   0.000527                             0.000725   \n",
       "2                   0.000000                             0.000000   \n",
       "3                   0.000807                             0.001568   \n",
       "4                   0.001723                             0.000401   \n",
       "...                      ...                                  ...   \n",
       "3977                0.000265                             0.001173   \n",
       "3978                0.004009                             0.001071   \n",
       "3979                0.002919                             0.000437   \n",
       "3980                0.001321                             0.000599   \n",
       "3981                0.001922                             0.000358   \n",
       "\n",
       "      gaba_receptor_agonist  gaba_receptor_antagonist  \\\n",
       "0                  0.015535                  0.012593   \n",
       "1                  0.001362                  0.002085   \n",
       "2                  0.000000                  0.000000   \n",
       "3                  0.003921                  0.004964   \n",
       "4                  0.011161                  0.007643   \n",
       "...                     ...                       ...   \n",
       "3977               0.002627                  0.001415   \n",
       "3978               0.004441                  0.015701   \n",
       "3979               0.004203                  0.011808   \n",
       "3980               0.007032                  0.011549   \n",
       "3981               0.009090                  0.015570   \n",
       "\n",
       "      gamma_secretase_inhibitor  glucocorticoid_receptor_agonist  \\\n",
       "0                      0.000638                         0.001502   \n",
       "1                      0.002432                         0.000139   \n",
       "2                      0.000000                         0.000000   \n",
       "3                      0.000893                         0.001228   \n",
       "4                      0.000465                         0.001856   \n",
       "...                         ...                              ...   \n",
       "3977                   0.002626                         0.000719   \n",
       "3978                   0.000651                         0.001779   \n",
       "3979                   0.000376                         0.002042   \n",
       "3980                   0.000426                         0.001591   \n",
       "3981                   0.001537                         0.000872   \n",
       "\n",
       "      glutamate_inhibitor  glutamate_receptor_agonist  \\\n",
       "0                0.001106                    0.005531   \n",
       "1                0.001145                    0.000877   \n",
       "2                0.000000                    0.000000   \n",
       "3                0.001052                    0.002867   \n",
       "4                0.001150                    0.007221   \n",
       "...                   ...                         ...   \n",
       "3977             0.000890                    0.000809   \n",
       "3978             0.001028                    0.007209   \n",
       "3979             0.000473                    0.005566   \n",
       "3980             0.000770                    0.004877   \n",
       "3981             0.000865                    0.005881   \n",
       "\n",
       "      glutamate_receptor_antagonist  gonadotropin_receptor_agonist  \\\n",
       "0                          0.029380                       0.002613   \n",
       "1                          0.005285                       0.000515   \n",
       "2                          0.000000                       0.000000   \n",
       "3                          0.015779                       0.001930   \n",
       "4                          0.022928                       0.003036   \n",
       "...                             ...                            ...   \n",
       "3977                       0.002458                       0.000247   \n",
       "3978                       0.021636                       0.001339   \n",
       "3979                       0.020623                       0.001809   \n",
       "3980                       0.019998                       0.002119   \n",
       "3981                       0.022243                       0.002264   \n",
       "\n",
       "      gsk_inhibitor  hcv_inhibitor  hdac_inhibitor  \\\n",
       "0          0.000777       0.003618        0.001467   \n",
       "1          0.001148       0.004017        0.000205   \n",
       "2          0.000000       0.000000        0.000000   \n",
       "3          0.000718       0.005381        0.001200   \n",
       "4          0.000654       0.002130        0.001437   \n",
       "...             ...            ...             ...   \n",
       "3977       0.000123       0.004108        0.000490   \n",
       "3978       0.000885       0.002172        0.000936   \n",
       "3979       0.000503       0.001613        0.001439   \n",
       "3980       0.000797       0.003090        0.000909   \n",
       "3981       0.001709       0.003322        0.001454   \n",
       "\n",
       "      histamine_receptor_agonist  histamine_receptor_antagonist  \\\n",
       "0                       0.005467                       0.013662   \n",
       "1                       0.000220                       0.004555   \n",
       "2                       0.000000                       0.000000   \n",
       "3                       0.003099                       0.035255   \n",
       "4                       0.003516                       0.013531   \n",
       "...                          ...                            ...   \n",
       "3977                    0.001569                       0.000841   \n",
       "3978                    0.007254                       0.010888   \n",
       "3979                    0.005949                       0.009658   \n",
       "3980                    0.001957                       0.023035   \n",
       "3981                    0.005926                       0.016264   \n",
       "\n",
       "      histone_lysine_demethylase_inhibitor  \\\n",
       "0                                 0.000266   \n",
       "1                                 0.002564   \n",
       "2                                 0.000000   \n",
       "3                                 0.001519   \n",
       "4                                 0.000226   \n",
       "...                                    ...   \n",
       "3977                              0.000338   \n",
       "3978                              0.000653   \n",
       "3979                              0.000480   \n",
       "3980                              0.000529   \n",
       "3981                              0.000666   \n",
       "\n",
       "      histone_lysine_methyltransferase_inhibitor  hiv_inhibitor  \\\n",
       "0                                       0.000728       0.005533   \n",
       "1                                       0.003615       0.001157   \n",
       "2                                       0.000000       0.000000   \n",
       "3                                       0.005862       0.003346   \n",
       "4                                       0.000517       0.003389   \n",
       "...                                          ...            ...   \n",
       "3977                                    0.009192       0.000152   \n",
       "3978                                    0.000788       0.008809   \n",
       "3979                                    0.000712       0.006140   \n",
       "3980                                    0.000918       0.006074   \n",
       "3981                                    0.001403       0.002042   \n",
       "\n",
       "      hmgcr_inhibitor  hsp_inhibitor  igf-1_inhibitor  ikk_inhibitor  \\\n",
       "0            0.001505       0.000839         0.000642       0.000973   \n",
       "1            0.000101       0.000645         0.002519       0.000416   \n",
       "2            0.000000       0.000000         0.000000       0.000000   \n",
       "3            0.018546       0.000493         0.001939       0.000385   \n",
       "4            0.000739       0.001458         0.000216       0.000952   \n",
       "...               ...            ...              ...            ...   \n",
       "3977         0.000167       0.003039         0.005965       0.005536   \n",
       "3978         0.000326       0.001530         0.000332       0.000827   \n",
       "3979         0.000483       0.001267         0.000316       0.000762   \n",
       "3980         0.002158       0.000991         0.000816       0.000707   \n",
       "3981         0.000835       0.000830         0.000764       0.001032   \n",
       "\n",
       "      imidazoline_receptor_agonist  immunosuppressant  insulin_secretagogue  \\\n",
       "0                         0.002922           0.002357              0.001943   \n",
       "1                         0.001273           0.000595              0.001244   \n",
       "2                         0.000000           0.000000              0.000000   \n",
       "3                         0.002254           0.005431              0.003020   \n",
       "4                         0.002274           0.002221              0.003205   \n",
       "...                            ...                ...                   ...   \n",
       "3977                      0.001055           0.002608              0.000191   \n",
       "3978                      0.002171           0.001023              0.001174   \n",
       "3979                      0.002175           0.000964              0.001666   \n",
       "3980                      0.001952           0.002143              0.002274   \n",
       "3981                      0.004052           0.001929              0.001389   \n",
       "\n",
       "      insulin_sensitizer  integrin_inhibitor  jak_inhibitor  kit_inhibitor  \\\n",
       "0               0.001676            0.002907       0.000946       0.000602   \n",
       "1               0.003128            0.008994       0.002648       0.000800   \n",
       "2               0.000000            0.000000       0.000000       0.000000   \n",
       "3               0.001354            0.002936       0.001103       0.000562   \n",
       "4               0.002026            0.001972       0.000288       0.000926   \n",
       "...                  ...                 ...            ...            ...   \n",
       "3977            0.000289            0.000932       0.034569       0.000850   \n",
       "3978            0.001104            0.003070       0.000314       0.000810   \n",
       "3979            0.000964            0.002052       0.000187       0.000516   \n",
       "3980            0.004837            0.003340       0.000384       0.000729   \n",
       "3981            0.001486            0.002705       0.000705       0.000480   \n",
       "\n",
       "      laxative  leukotriene_inhibitor  leukotriene_receptor_antagonist  \\\n",
       "0     0.000655               0.000978                         0.003833   \n",
       "1     0.000308               0.000365                         0.002533   \n",
       "2     0.000000               0.000000                         0.000000   \n",
       "3     0.000650               0.001182                         0.002656   \n",
       "4     0.000712               0.000868                         0.002872   \n",
       "...        ...                    ...                              ...   \n",
       "3977  0.000356               0.001114                         0.002903   \n",
       "3978  0.001140               0.001261                         0.002527   \n",
       "3979  0.000882               0.001000                         0.003220   \n",
       "3980  0.000577               0.000634                         0.005635   \n",
       "3981  0.000624               0.000778                         0.006645   \n",
       "\n",
       "      lipase_inhibitor  lipoxygenase_inhibitor  lxr_agonist  mdm_inhibitor  \\\n",
       "0             0.001591                0.002039     0.000661       0.000314   \n",
       "1             0.000484                0.002825     0.000167       0.000398   \n",
       "2             0.000000                0.000000     0.000000       0.000000   \n",
       "3             0.001719                0.001413     0.004022       0.000542   \n",
       "4             0.000970                0.003176     0.000371       0.000311   \n",
       "...                ...                     ...          ...            ...   \n",
       "3977          0.000427                0.002125     0.000204       0.000568   \n",
       "3978          0.000703                0.001774     0.000361       0.000320   \n",
       "3979          0.000864                0.001545     0.000374       0.000234   \n",
       "3980          0.001357                0.001626     0.000622       0.000364   \n",
       "3981          0.001369                0.001964     0.000647       0.000278   \n",
       "\n",
       "      mek_inhibitor  membrane_integrity_inhibitor  \\\n",
       "0          0.000381                      0.011585   \n",
       "1          0.000273                      0.000857   \n",
       "2          0.000000                      0.000000   \n",
       "3          0.004289                      0.004993   \n",
       "4          0.000308                      0.012181   \n",
       "...             ...                           ...   \n",
       "3977       0.001748                      0.000348   \n",
       "3978       0.000439                      0.003868   \n",
       "3979       0.000310                      0.006887   \n",
       "3980       0.000413                      0.007011   \n",
       "3981       0.000486                      0.005261   \n",
       "\n",
       "      mineralocorticoid_receptor_antagonist  \\\n",
       "0                                  0.001999   \n",
       "1                                  0.000253   \n",
       "2                                  0.000000   \n",
       "3                                  0.011118   \n",
       "4                                  0.000835   \n",
       "...                                     ...   \n",
       "3977                               0.002239   \n",
       "3978                               0.000684   \n",
       "3979                               0.000497   \n",
       "3980                               0.001843   \n",
       "3981                               0.001951   \n",
       "\n",
       "      monoacylglycerol_lipase_inhibitor  monoamine_oxidase_inhibitor  \\\n",
       "0                              0.001048                     0.004834   \n",
       "1                              0.000181                     0.002754   \n",
       "2                              0.000000                     0.000000   \n",
       "3                              0.000730                     0.004438   \n",
       "4                              0.001059                     0.005157   \n",
       "...                                 ...                          ...   \n",
       "3977                           0.000585                     0.000804   \n",
       "3978                           0.001429                     0.007406   \n",
       "3979                           0.001655                     0.005448   \n",
       "3980                           0.000675                     0.004744   \n",
       "3981                           0.001259                     0.010808   \n",
       "\n",
       "      monopolar_spindle_1_kinase_inhibitor  mtor_inhibitor  mucolytic_agent  \\\n",
       "0                                 0.000355        0.001668         0.005486   \n",
       "1                                 0.000658        0.000864         0.000362   \n",
       "2                                 0.000000        0.000000         0.000000   \n",
       "3                                 0.000435        0.001207         0.003719   \n",
       "4                                 0.000300        0.001009         0.003616   \n",
       "...                                    ...             ...              ...   \n",
       "3977                              0.001765        0.000526         0.001116   \n",
       "3978                              0.000761        0.001746         0.005586   \n",
       "3979                              0.000416        0.001423         0.005034   \n",
       "3980                              0.000375        0.002018         0.004798   \n",
       "3981                              0.000381        0.001599         0.005173   \n",
       "\n",
       "      neuropeptide_receptor_antagonist  nfkb_inhibitor  \\\n",
       "0                             0.000956        0.004259   \n",
       "1                             0.000980        0.001659   \n",
       "2                             0.000000        0.000000   \n",
       "3                             0.001402        0.002436   \n",
       "4                             0.001451        0.004220   \n",
       "...                                ...             ...   \n",
       "3977                          0.000876        0.001331   \n",
       "3978                          0.001698        0.005711   \n",
       "3979                          0.002114        0.003402   \n",
       "3980                          0.002559        0.002994   \n",
       "3981                          0.001902        0.006068   \n",
       "\n",
       "      nicotinic_receptor_agonist  nitric_oxide_donor  \\\n",
       "0                       0.000809            0.001320   \n",
       "1                       0.000659            0.000224   \n",
       "2                       0.000000            0.000000   \n",
       "3                       0.000523            0.001966   \n",
       "4                       0.000759            0.003201   \n",
       "...                          ...                 ...   \n",
       "3977                    0.000591            0.000142   \n",
       "3978                    0.001538            0.002641   \n",
       "3979                    0.001032            0.004615   \n",
       "3980                    0.000652            0.003462   \n",
       "3981                    0.000840            0.002285   \n",
       "\n",
       "      nitric_oxide_production_inhibitor  nitric_oxide_synthase_inhibitor  \\\n",
       "0                              0.000803                         0.001308   \n",
       "1                              0.000154                         0.000176   \n",
       "2                              0.000000                         0.000000   \n",
       "3                              0.001765                         0.001008   \n",
       "4                              0.000730                         0.002495   \n",
       "...                                 ...                              ...   \n",
       "3977                           0.000361                         0.000630   \n",
       "3978                           0.000357                         0.002441   \n",
       "3979                           0.000533                         0.004846   \n",
       "3980                           0.001063                         0.002683   \n",
       "3981                           0.000847                         0.001868   \n",
       "\n",
       "      norepinephrine_reuptake_inhibitor  nrf2_activator  \\\n",
       "0                              0.000574        0.000895   \n",
       "1                              0.000437        0.000145   \n",
       "2                              0.000000        0.000000   \n",
       "3                              0.000444        0.000770   \n",
       "4                              0.000689        0.000938   \n",
       "...                                 ...             ...   \n",
       "3977                           0.000259        0.000593   \n",
       "3978                           0.001616        0.001090   \n",
       "3979                           0.001411        0.001350   \n",
       "3980                           0.000699        0.000637   \n",
       "3981                           0.000812        0.001296   \n",
       "\n",
       "      opioid_receptor_agonist  opioid_receptor_antagonist  \\\n",
       "0                    0.002451                    0.004539   \n",
       "1                    0.000335                    0.001331   \n",
       "2                    0.000000                    0.000000   \n",
       "3                    0.004210                    0.004946   \n",
       "4                    0.002661                    0.005333   \n",
       "...                       ...                         ...   \n",
       "3977                 0.005953                    0.000407   \n",
       "3978                 0.004866                    0.005246   \n",
       "3979                 0.003731                    0.008757   \n",
       "3980                 0.004162                    0.004490   \n",
       "3981                 0.002999                    0.006473   \n",
       "\n",
       "      orexin_receptor_antagonist  p38_mapk_inhibitor  \\\n",
       "0                       0.001864            0.000494   \n",
       "1                       0.000988            0.002052   \n",
       "2                       0.000000            0.000000   \n",
       "3                       0.002016            0.000741   \n",
       "4                       0.003897            0.001319   \n",
       "...                          ...                 ...   \n",
       "3977                    0.000560            0.000986   \n",
       "3978                    0.005441            0.000507   \n",
       "3979                    0.007402            0.001023   \n",
       "3980                    0.003578            0.000334   \n",
       "3981                    0.003607            0.000417   \n",
       "\n",
       "      p-glycoprotein_inhibitor  parp_inhibitor  pdgfr_inhibitor  \\\n",
       "0                     0.000886        0.000790         0.000397   \n",
       "1                     0.001141        0.008757         0.002212   \n",
       "2                     0.000000        0.000000         0.000000   \n",
       "3                     0.000704        0.000644         0.000766   \n",
       "4                     0.001028        0.001597         0.000427   \n",
       "...                        ...             ...              ...   \n",
       "3977                  0.000180        0.001377         0.002438   \n",
       "3978                  0.001908        0.002162         0.000728   \n",
       "3979                  0.001435        0.001439         0.000327   \n",
       "3980                  0.001253        0.000655         0.000339   \n",
       "3981                  0.001197        0.000803         0.000309   \n",
       "\n",
       "      pdk_inhibitor  phosphodiesterase_inhibitor  phospholipase_inhibitor  \\\n",
       "0          0.000798                     0.017828                 0.002565   \n",
       "1          0.002115                     0.034894                 0.000223   \n",
       "2          0.000000                     0.000000                 0.000000   \n",
       "3          0.000397                     0.008120                 0.004648   \n",
       "4          0.001371                     0.010781                 0.001788   \n",
       "...             ...                          ...                      ...   \n",
       "3977       0.000959                     0.001174                 0.000261   \n",
       "3978       0.001778                     0.019052                 0.001146   \n",
       "3979       0.001504                     0.009239                 0.001604   \n",
       "3980       0.000752                     0.019336                 0.001705   \n",
       "3981       0.001082                     0.017244                 0.003072   \n",
       "\n",
       "      pi3k_inhibitor  pkc_inhibitor  potassium_channel_activator  \\\n",
       "0           0.002081       0.001376                     0.004462   \n",
       "1           0.006009       0.001137                     0.001087   \n",
       "2           0.000000       0.000000                     0.000000   \n",
       "3           0.008088       0.002169                     0.004454   \n",
       "4           0.000632       0.002004                     0.004581   \n",
       "...              ...            ...                          ...   \n",
       "3977        0.008186       0.000500                     0.000338   \n",
       "3978        0.001487       0.001269                     0.002535   \n",
       "3979        0.000613       0.001846                     0.003592   \n",
       "3980        0.002596       0.001893                     0.006135   \n",
       "3981        0.001864       0.001127                     0.004735   \n",
       "\n",
       "      potassium_channel_antagonist  ppar_receptor_agonist  \\\n",
       "0                         0.007648               0.002273   \n",
       "1                         0.002240               0.001206   \n",
       "2                         0.000000               0.000000   \n",
       "3                         0.008772               0.000896   \n",
       "4                         0.004212               0.003253   \n",
       "...                            ...                    ...   \n",
       "3977                      0.001142               0.000987   \n",
       "3978                      0.004147               0.002223   \n",
       "3979                      0.003983               0.002785   \n",
       "3980                      0.004178               0.003475   \n",
       "3981                      0.011669               0.001663   \n",
       "\n",
       "      ppar_receptor_antagonist  progesterone_receptor_agonist  \\\n",
       "0                     0.002305                       0.012969   \n",
       "1                     0.001911                       0.000207   \n",
       "2                     0.000000                       0.000000   \n",
       "3                     0.003121                       0.002752   \n",
       "4                     0.000988                       0.007749   \n",
       "...                        ...                            ...   \n",
       "3977                  0.000144                       0.000868   \n",
       "3978                  0.001139                       0.005706   \n",
       "3979                  0.001075                       0.003892   \n",
       "3980                  0.001419                       0.003978   \n",
       "3981                  0.002920                       0.001567   \n",
       "\n",
       "      progesterone_receptor_antagonist  prostaglandin_inhibitor  \\\n",
       "0                             0.000971                 0.003666   \n",
       "1                             0.000566                 0.000637   \n",
       "2                             0.000000                 0.000000   \n",
       "3                             0.001202                 0.003258   \n",
       "4                             0.000643                 0.002206   \n",
       "...                                ...                      ...   \n",
       "3977                          0.000584                 0.000194   \n",
       "3978                          0.000749                 0.002077   \n",
       "3979                          0.000368                 0.002036   \n",
       "3980                          0.000741                 0.002026   \n",
       "3981                          0.000804                 0.003224   \n",
       "\n",
       "      prostanoid_receptor_antagonist  proteasome_inhibitor  \\\n",
       "0                           0.008044              0.000392   \n",
       "1                           0.000550              0.000185   \n",
       "2                           0.000000              0.000000   \n",
       "3                           0.003224              0.000400   \n",
       "4                           0.004813              0.000423   \n",
       "...                              ...                   ...   \n",
       "3977                        0.000557              0.000229   \n",
       "3978                        0.007677              0.000338   \n",
       "3979                        0.004893              0.000551   \n",
       "3980                        0.004969              0.000224   \n",
       "3981                        0.009101              0.000575   \n",
       "\n",
       "      protein_kinase_inhibitor  protein_phosphatase_inhibitor  \\\n",
       "0                     0.003382                       0.000326   \n",
       "1                     0.000552                       0.000254   \n",
       "2                     0.000000                       0.000000   \n",
       "3                     0.011572                       0.000281   \n",
       "4                     0.002039                       0.000672   \n",
       "...                        ...                            ...   \n",
       "3977                  0.001681                       0.000177   \n",
       "3978                  0.001406                       0.000556   \n",
       "3979                  0.001371                       0.000620   \n",
       "3980                  0.001891                       0.000413   \n",
       "3981                  0.002804                       0.000487   \n",
       "\n",
       "      protein_synthesis_inhibitor  protein_tyrosine_kinase_inhibitor  \\\n",
       "0                        0.004703                           0.001020   \n",
       "1                        0.000479                           0.000984   \n",
       "2                        0.000000                           0.000000   \n",
       "3                        0.010003                           0.001281   \n",
       "4                        0.005010                           0.001351   \n",
       "...                           ...                                ...   \n",
       "3977                     0.000587                           0.000742   \n",
       "3978                     0.002091                           0.001992   \n",
       "3979                     0.003198                           0.001779   \n",
       "3980                     0.003660                           0.001579   \n",
       "3981                     0.002499                           0.001329   \n",
       "\n",
       "      radiopaque_medium  raf_inhibitor  ras_gtpase_inhibitor  \\\n",
       "0              0.004949       0.000665              0.000606   \n",
       "1              0.000491       0.000267              0.000284   \n",
       "2              0.000000       0.000000              0.000000   \n",
       "3              0.002637       0.000340              0.001384   \n",
       "4              0.010025       0.001161              0.000833   \n",
       "...                 ...            ...                   ...   \n",
       "3977           0.000331       0.000222              0.000235   \n",
       "3978           0.004303       0.000367              0.001176   \n",
       "3979           0.007947       0.000278              0.001319   \n",
       "3980           0.005618       0.000398              0.000833   \n",
       "3981           0.004428       0.000403              0.000972   \n",
       "\n",
       "      retinoid_receptor_agonist  retinoid_receptor_antagonist  \\\n",
       "0                      0.001689                      0.000807   \n",
       "1                      0.021654                      0.000314   \n",
       "2                      0.000000                      0.000000   \n",
       "3                      0.000332                      0.001079   \n",
       "4                      0.001794                      0.000681   \n",
       "...                         ...                           ...   \n",
       "3977                   0.000158                      0.000184   \n",
       "3978                   0.001640                      0.000691   \n",
       "3979                   0.004056                      0.000525   \n",
       "3980                   0.000630                      0.000689   \n",
       "3981                   0.000403                      0.000823   \n",
       "\n",
       "      rho_associated_kinase_inhibitor  ribonucleoside_reductase_inhibitor  \\\n",
       "0                            0.000961                            0.001387   \n",
       "1                            0.084231                            0.000169   \n",
       "2                            0.000000                            0.000000   \n",
       "3                            0.000730                            0.001379   \n",
       "4                            0.001321                            0.002340   \n",
       "...                               ...                                 ...   \n",
       "3977                         0.000509                            0.000236   \n",
       "3978                         0.001085                            0.001423   \n",
       "3979                         0.001456                            0.002407   \n",
       "3980                         0.001089                            0.001707   \n",
       "3981                         0.001078                            0.001962   \n",
       "\n",
       "      rna_polymerase_inhibitor  serotonin_receptor_agonist  \\\n",
       "0                     0.002841                    0.016735   \n",
       "1                     0.000433                    0.039361   \n",
       "2                     0.000000                    0.000000   \n",
       "3                     0.001396                    0.019471   \n",
       "4                     0.002097                    0.008085   \n",
       "...                        ...                         ...   \n",
       "3977                  0.000594                    0.021641   \n",
       "3978                  0.001080                    0.011975   \n",
       "3979                  0.001060                    0.006941   \n",
       "3980                  0.001363                    0.013144   \n",
       "3981                  0.002854                    0.031294   \n",
       "\n",
       "      serotonin_receptor_antagonist  serotonin_reuptake_inhibitor  \\\n",
       "0                          0.012503                      0.002458   \n",
       "1                          0.006647                      0.000538   \n",
       "2                          0.000000                      0.000000   \n",
       "3                          0.032757                      0.005595   \n",
       "4                          0.020965                      0.001928   \n",
       "...                             ...                           ...   \n",
       "3977                       0.002550                      0.000634   \n",
       "3978                       0.020422                      0.003082   \n",
       "3979                       0.033458                      0.003025   \n",
       "3980                       0.019000                      0.003904   \n",
       "3981                       0.021217                      0.003224   \n",
       "\n",
       "      sigma_receptor_agonist  sigma_receptor_antagonist  \\\n",
       "0                   0.003576                   0.001361   \n",
       "1                   0.000577                   0.000749   \n",
       "2                   0.000000                   0.000000   \n",
       "3                   0.002712                   0.002670   \n",
       "4                   0.002865                   0.001249   \n",
       "...                      ...                        ...   \n",
       "3977                0.001332                   0.000680   \n",
       "3978                0.001936                   0.001379   \n",
       "3979                0.001724                   0.001777   \n",
       "3980                0.001825                   0.001496   \n",
       "3981                0.002958                   0.002089   \n",
       "\n",
       "      smoothened_receptor_antagonist  sodium_channel_inhibitor  \\\n",
       "0                           0.001814                  0.018148   \n",
       "1                           0.001635                  0.004923   \n",
       "2                           0.000000                  0.000000   \n",
       "3                           0.001876                  0.006009   \n",
       "4                           0.001029                  0.012937   \n",
       "...                              ...                       ...   \n",
       "3977                        0.000540                  0.003992   \n",
       "3978                        0.001082                  0.020597   \n",
       "3979                        0.000928                  0.020745   \n",
       "3980                        0.002582                  0.014822   \n",
       "3981                        0.001993                  0.017870   \n",
       "\n",
       "      sphingosine_receptor_agonist  src_inhibitor   steroid  syk_inhibitor  \\\n",
       "0                         0.003983       0.001277  0.000770       0.000621   \n",
       "1                         0.000166       0.038366  0.000078       0.001223   \n",
       "2                         0.000000       0.000000  0.000000       0.000000   \n",
       "3                         0.002287       0.001048  0.001258       0.000666   \n",
       "4                         0.003046       0.000569  0.001695       0.000584   \n",
       "...                            ...            ...       ...            ...   \n",
       "3977                      0.000332       0.014277  0.000152       0.000343   \n",
       "3978                      0.000667       0.001399  0.000504       0.000971   \n",
       "3979                      0.001009       0.000683  0.001100       0.000414   \n",
       "3980                      0.001632       0.000648  0.000869       0.000523   \n",
       "3981                      0.001634       0.002656  0.000672       0.000648   \n",
       "\n",
       "      tachykinin_antagonist  tgf-beta_receptor_inhibitor  thrombin_inhibitor  \\\n",
       "0                  0.001998                     0.000239            0.000818   \n",
       "1                  0.000792                     0.000871            0.000299   \n",
       "2                  0.000000                     0.000000            0.000000   \n",
       "3                  0.004997                     0.000365            0.001140   \n",
       "4                  0.003242                     0.000327            0.001664   \n",
       "...                     ...                          ...                 ...   \n",
       "3977               0.000449                     0.000129            0.000362   \n",
       "3978               0.003444                     0.000214            0.001759   \n",
       "3979               0.003358                     0.000378            0.003213   \n",
       "3980               0.006205                     0.000363            0.002718   \n",
       "3981               0.003661                     0.000562            0.001091   \n",
       "\n",
       "      thymidylate_synthase_inhibitor  tlr_agonist  tlr_antagonist  \\\n",
       "0                           0.001038     0.002002        0.000954   \n",
       "1                           0.000137     0.000379        0.000180   \n",
       "2                           0.000000     0.000000        0.000000   \n",
       "3                           0.000928     0.001670        0.001214   \n",
       "4                           0.003587     0.002340        0.001060   \n",
       "...                              ...          ...             ...   \n",
       "3977                        0.000098     0.001060        0.000071   \n",
       "3978                        0.001443     0.003256        0.000599   \n",
       "3979                        0.004392     0.003244        0.000855   \n",
       "3980                        0.001717     0.002133        0.001161   \n",
       "3981                        0.001479     0.002647        0.000991   \n",
       "\n",
       "      tnf_inhibitor  topoisomerase_inhibitor  \\\n",
       "0          0.002103                 0.001105   \n",
       "1          0.002126                 0.000197   \n",
       "2          0.000000                 0.000000   \n",
       "3          0.000932                 0.000878   \n",
       "4          0.001371                 0.003065   \n",
       "...             ...                      ...   \n",
       "3977       0.000863                 0.000183   \n",
       "3978       0.002320                 0.001917   \n",
       "3979       0.001682                 0.003217   \n",
       "3980       0.001800                 0.000957   \n",
       "3981       0.003221                 0.000848   \n",
       "\n",
       "      transient_receptor_potential_channel_antagonist  \\\n",
       "0                                            0.001381   \n",
       "1                                            0.001206   \n",
       "2                                            0.000000   \n",
       "3                                            0.001055   \n",
       "4                                            0.000834   \n",
       "...                                               ...   \n",
       "3977                                         0.000760   \n",
       "3978                                         0.002306   \n",
       "3979                                         0.001452   \n",
       "3980                                         0.000967   \n",
       "3981                                         0.001038   \n",
       "\n",
       "      tropomyosin_receptor_kinase_inhibitor  trpv_agonist  trpv_antagonist  \\\n",
       "0                                  0.001474      0.000620         0.002410   \n",
       "1                                  0.000327      0.000225         0.001751   \n",
       "2                                  0.000000      0.000000         0.000000   \n",
       "3                                  0.000899      0.001418         0.002027   \n",
       "4                                  0.001279      0.000721         0.002299   \n",
       "...                                     ...           ...              ...   \n",
       "3977                               0.000642      0.009237         0.001567   \n",
       "3978                               0.000464      0.000557         0.003298   \n",
       "3979                               0.000647      0.000763         0.002008   \n",
       "3980                               0.001162      0.000547         0.001922   \n",
       "3981                               0.000841      0.000935         0.002037   \n",
       "\n",
       "      tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "0              0.001862                   0.000755   \n",
       "1              0.001050                   0.003084   \n",
       "2              0.000000                   0.000000   \n",
       "3              0.009872                   0.026518   \n",
       "4              0.001105                   0.000995   \n",
       "...                 ...                        ...   \n",
       "3977           0.256924                   0.011050   \n",
       "3978           0.001465                   0.001121   \n",
       "3979           0.001003                   0.001780   \n",
       "3980           0.001095                   0.001724   \n",
       "3981           0.002825                   0.001369   \n",
       "\n",
       "      ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n",
       "0                                  0.000645         0.000846   0.002145   \n",
       "1                                  0.000150         0.003143   0.000431   \n",
       "2                                  0.000000         0.000000   0.000000   \n",
       "3                                  0.000502         0.002424   0.003550   \n",
       "4                                  0.000791         0.001039   0.001377   \n",
       "...                                     ...              ...        ...   \n",
       "3977                               0.000718         0.005442   0.000339   \n",
       "3978                               0.001028         0.001697   0.001782   \n",
       "3979                               0.000892         0.000955   0.001271   \n",
       "3980                               0.000524         0.000754   0.002332   \n",
       "3981                               0.000634         0.001395   0.002021   \n",
       "\n",
       "      vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0                       0.001633       0.001734  \n",
       "1                       0.000196       0.002099  \n",
       "2                       0.000000       0.000000  \n",
       "3                       0.002740       0.003658  \n",
       "4                       0.000459       0.001667  \n",
       "...                          ...            ...  \n",
       "3977                    0.001150       0.000384  \n",
       "3978                    0.000381       0.000885  \n",
       "3979                    0.000451       0.001608  \n",
       "3980                    0.000654       0.002743  \n",
       "3981                    0.000628       0.001510  \n",
       "\n",
       "[3982 rows x 207 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
